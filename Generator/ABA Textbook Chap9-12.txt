9. Multiple Baseline and Changing 
Criterion Designs
 
Key Terms
changing criterion design	multiple baseline across settings	multiple baseline design
delayed multiple baseline design	design	multiple probe design
multiple baseline across multiple baseline across subjects behaviors design design

This chapter describes two additional experimental tactics for analyzing behavior–environment relations—the multiple baseline design and the changing criterion design. In a multiple baseline design, after collecting initial baseline data simultaneously across two or more behaviors, settings, or people, the behavior analyst then applies the treatment variable sequentially across these behaviors, settings, or people and notes the effects. The changing criterion design is used to analyze improvements in behavior as a function of stepwise, incremental criterion changes in the level of responding required for reinforcement. In both designs, experimental control and a functional relation are demonstrated when the behaviors change from a steady state baseline to a new steady state after the introduction of the independent variable is applied, or a new criterion established.

Multiple Baseline Design
The multiple baseline design is the most widely used experimental design for evaluating treatment effects in applied behavior analysis. It is a highly flexible tactic that enables researchers and practitioners to analyze the effects of an independent variable across multiple behaviors, settings, and/or subjects without having to withdraw the treatment variable to verify that the improvements in behavior were a direct result of the application of the treatment. As you recall, the reversal design by its very nature requires that the independent variable be withdrawn to verify the prediction established in baseline. This is not so with the multiple baseline design.
Operation and Logic of the Multiple Baseline Design
Baer,Wolf, and Risley (1968) first described the multiple baseline design in the applied behavior analysis literature. They presented the multiple baseline design as an alternative to the reversal design for two situations: (a) when the target behavior is likely to be irreversible or (b) when it is undesirable, impractical, or unethical to reverse conditions. Figure 1 illustrates Baer and colleagues’explanation of the basic operation of the multiple baseline design.

In the multiple baseline technique, a number of responses are identified and measured over time to provide baselines against which changes can be evaluated. With these baselines established, the experimenter then applies an experimental variable to one of the behaviors, produces a change in it, and perhaps notes little or no change in the other baselines. If so, rather than reversing the just-produced change, he instead applies the experimental variable to one of the other, as yet unchanged, responses. If it changes at that point, evidence is accruing that the experimental variable is indeed effective, and that the prior change was not simply a matter of coincidence. The variable then may be applied to still another response, and so on. The experimenter is attempting to show that he has a reliable experimental variable, in that each behavior changes maximally only when the experimental variable is applied to it. (p. 94)
The multiple baseline design takes three basic forms:
•	The multiple baseline across behaviors design,consisting of two or more different behaviors of the same subject
•	The multiple baseline across settings design, con-sisting of the same behavior of the same subject in two or more different settings, situations, or time periods
•	The multiple baseline across subjects design, con-sisting of the same behavior of two or more different participants (or groups)
Although only one of the multiple baseline design’s basic forms is called an “across behaviors” design, all multiple baseline designs involve the time-lagged application of a treatment variable across technically different (meaning independent) behaviors. That is, in the multiple baseline across settings design, even though the subject’s performance of the same target behavior is measured in two or more settings, each behavior–setting combination is conceptualized and treated as a different behavior for analysis. Similarly, in a multiple baseline across subjects design, each subject–behavior combination functions as a different behavior in the operation of the design

Figure 2 shows the same data set displayed in Figure 1 with the addition of data points representing predicted measures if baseline conditions were not changed and shaded areas illustrating how the three elements of baseline logic—prediction, verification, and replication—are operationalized in the multiple baseline design.  When stable baseline responding has been achieved for Behavior 1, a prediction is made that if the environment were held constant, continued measurement would reveal similar levels of responding. When the researcher’s confidence in such a prediction is justifiably high, the independent variable is applied to Behavior 1. The open data points in the treatment phase for Behavior 1 represent the predicted level of responding. The solid data points show the actual measures obtained for Behavior 1 during the treatment condition. These data show a discrepancy with the predicted level of responding if no changes had been made in the environment, thereby suggesting that the environment were held constant, continued measurement would reveal similar levels of responding. When the researcher’s confidence in such a prediction is justifiably high, the independent variable is applied to Behavior 1. The open data points in the treatment phase for Behavior 1 represent the predicted level of responding. The solid data points show the actual measures obtained for Behavior 1 during the treatment condition. These data show a discrepancy with the predicted level of responding if no changes had been made in the environment, thereby suggesting that the treatment may be responsible for the change in behavior. The data collected for Behavior 1 in a multiple baseline design serve the same functions as the data collected during the first two phases of an A-B-A-B reversal design.
Continued baseline measures of the other behaviors in the experiment offer the possibility of verifying the prediction made for Behavior 1. In a multiple baseline design, verification of a predicted level of responding for one behavior (or tier) is obtained if little or no change is observed in the data paths of the behaviors (tiers) that are still exposed to the conditions under which the prediction was made. In Figure 2 those portions of the baseline condition data paths for Behaviors 2 and 3 within the shaded boxes verify the prediction for Behavior 1. At this point in the experiment, two inferences can be made: (a) The prediction that Behavior 1 would not change in a constant environment is valid because the environment was held constant for Behaviors 2 and 3 and their levels of responding remained unchanged; and (b) the observed changes in Behavior 1 were brought about by the independent variable because only Behavior 1 was exposed to the independent variable and only Behavior 1 changed.
In a multiple baseline design, the independent variable’s function in changing a given behavior is inferred by the lack of change in untreated behaviors. However, verification of function is not demonstrated directly as it is with the reversal design, thereby making the multiple baseline design an inherently weaker tactic (i.e., less convincing from the perspective of experimental control) for revealing a functional relation between the independent
Figure 2	Graphic prototype of a multiple baseline design with shading added to show elements of baseline logic. Open data points represent predicted measures if baseline conditions were unchanged. Baseline data points for Behaviors 2 and 3 within shaded areas verify of the prediction made for Behavior 1. Behavior 3 baseline data within Bracket A verify the prediction made for Behavior 2. Data obtained during the treatment condition for Behaviors 2 and 3 (cross-hatched shading) provide replications of the experimental effect.
variable and a target behavior. However, the multiple baseline design compensates somewhat for this weakness by providing the opportunity to verify or refute a series of similar predictions. Not only is the prediction for Behavior 1 in Figure 2 verified by continued stable baselines for Behavior 2 and 3, but the bracketed portion of the baseline data for Behavior 3 also serves as verification of the prediction made for Behavior 2.
When the level of responding for Behavior 1 under the treatment condition has stabilized or reached a predetermined performance criterion, the independent variable is then applied to Behavior 2. If Behavior 2 changes in a manner similar to the changes observed for Behavior 1, replication of the independent variable’s effect has been achieved (shown by the data path shaded with cross-hatching). After Behavior 2 has stabilized or reached a predetermined performance criterion, the independent variable is applied to Behavior 3 to see whether the effect will be replicated. The independent variable may be applied to additional behaviors in a similar manner until a convincing demonstration of the functional relation has been established (or rejected) and all of the behaviors targeted for improvement have received treatment.
As with verification, replication of the independent variable’s specific effect on each behavior in a multiple baseline design is not manipulated directly. Instead, the generality of the independent variable’s effect across the behaviors comprising the experiment is demonstrated by applying it to a series of behaviors. Assuming accurate measurement and proper experimental control of relevant variables (i.e., the only environmental factor that changes during the course of the experiment should be the presence—or absence—of the independent variable), each time a behavior changes when, and only when, the independent variable is introduced, confidence in the existence of a functional relation increases.
How many different behaviors, settings, or subjects must a multiple baseline design include to provide a believable demonstration of a functional relation? Baer, Wolf, and Risley (1968) suggested that the number of replications needed in any design is ultimately a matter to be decided by the consumers of the research. In this sense, an experiment using a multiple baseline design must contain the minimum number of replications necessary to convince those who will be asked to respond to the experiment and to the researcher’s claims (e.g., teachers, administrators, parents, funding sources, journal editors). A two-tier multiple baseline design is a complete experiment and can provide strong support for the effectiveness of the independent variable (e.g., Lindberg, Iwata, Roscoe, Worsdell, & Hanley, 2003; McCord, Iwata, Galensky, Ellingson, & Thomson, 2001; Newstrom, McLaughlin, & Sweeney, 1999; Test, Spooner, Keul, & Grossi, 1990). McClannahan, McGee, MacDuff, and Krantz (1990) conducted a multiple baseline design study in which the independent variable was sequentially implemented in an eight-tier design across 12 participants. Multiple baseline designs of three to five tiers are most common. When the effects of the independent variable are substantial and reliably replicated, a three- or four-tier multiple baseline design provides a convincing demonstration of experimental effect. Suffice it to say that the more replications one conducts, the more convincing the demonstration will be.
Some of the earliest examples of the multiple baseline design in the applied behavior analysis literature were studies by Risley and Hart (1968); Barrish, Saunders, and Wolf (1969); Barton, Guess, Garcia, and Baer (1970); Panyan, Boozer, and Morris (1970); and Schwarz and Hawkins (1970). Some of the pioneering applications of the multiple baseline technique are not readily apparent with casual examination: The authors may not have identified the experimental design as a multiple baseline design (e.g., Schwarz & Hawkins, 1970), and/or the now-common practice of stacking the tiers of a multiple baseline design one on the other so that all of the data can be displayed graphically in the same figure was not always used (e.g., Maloney & Hopkins, 1973; McAllister, Stachowiak, Baer, & Conderman, 1969; Schwarz & Hawkins, 1970).
In 1970, Vance Hall, Connie Cristler, Sharon Cranston, and Bonnie Tucker published a paper that described three experiments, each an example of one of the three basic forms of the multiple baseline design: across behaviors, across settings, and across subjects. Hall and colleagues’ paper was important not only because it provided excellent illustrations that today still serve as models of the multiple baseline design, but also because the studies were carried out by teachers and parents, indicating that practitioners “can carry out important and significant studies in natural settings using resources available to them” (p. 255).
Multiple Baseline across Behaviors Design
The multiple baseline across behaviors design begins with the concurrent measurement of two or more behaviors of a single participant. After steady state responding has been obtained under baseline conditions, the investigator applies the independent variable to one of the behaviors while maintaining baseline conditions for the other behavior(s). When steady state or criterion-level performance has been reached for the first behavior, the independent variable is applied to the next behavior, and so on (e.g., Bell, Young, Salzberg, & West, 1991; Gena, Krantz, McClannahan, & Poulson, 1996; Higgins, Williams, & McLaughlin, 2001 [see Figure 26.8]).
Ward and Carnes (2002) used a multiple baseline across behaviors design to evaluate the effects of selfset goals and public posting on the execution of three skills by five linebackers on a college football team: (a) reads, in which the linebacker positions himself to cover a specified area on the field on a pass play or from the line of scrimmage on a run; (b) drops, in which the linebacker moves to the correct position depending on the offensive team’s alignment; and (c) tackles. A video camera recorded the players’ movements during all practice sessions and games. Data were collected for the first 10 opportunities each player had with each skill. Reads and drops were recorded as correct if the player moved to the zone identified in the coaches’ playbook; tackles were scored as correct if the offensive ball carrier was stopped.
Following baseline, each player met with one of the researchers, who described the player’s mean baseline performance for a given skill. Players were asked to set a goal for their performances during practice sessions; no goals were set for games. The correct performances during baseline for all five players ranged from 60 to 80%, and all players set goals of 90% correct performance. The players were informed that their performance in each day’s practice would be posted on a chart prior to the next practice session. A Y (yes) or an N (no) was placed next to each player’s name to indicate whether he had met his goal. A player’s performance was posted on the chart only for the skill(s) in intervention. The chart was mounted on a wall in the locker room where all players on the team could see it. The head coach explained the purpose of the chart to other players on the team. Players’ performances during games were not posted on the chart.
 Baseline	Public Posting	Figure 3	A multiple baseline
	5	10	15	20	25	30	35	40
Sessions
The results for one of the players, John, are shown in Figure 3. John met or exceeded his goal of 90% correct performance during all practices for each of the three skills. Additionally, his improved performance generalized to games. The same pattern of results was obtained for each of the other four players in the study, illustrating that the multiple baseline across behaviors design is a single-subject experimental strategy in which each subject serves as his own control. Each player constituted a complete experiment, replicated in this case with four other participants.
Multiple Baseline across Settings Design
In the multiple baseline across settings design, a single behavior of a person (or group) is targeted in two or more different settings or conditions (e.g., locations, times of day). After stable responding has been demonstrated under baseline conditions, the independent variable is introduced in one of the settings while baseline conditions remain in effect in the other settings. When maximum behavior change or criterion-level performance has been achieved in the first setting, the independent variable is applied in the second setting, and so on.
Roane, Kelly, and Fisher (2003) employed a multiple baseline across settings design to evaluate the effects of a treatment designed to reduce the rate at which an 8year-old boy put inedible objects in his mouth. Jason, who had been diagnosed with autism, cerebral palsy, and moderate mental retardation, had a history of putting objects such as toys, cloth, paper, tree bark, plants, and dirt into his mouth.
Figure 4	A multiple baseline across	Baseline	Fanny Pack (Food) settings design showing the number of object mouthing responses per minute during baseline and treatment conditions.
From “The Effects of Noncontingent Access to Food on the Rate of Object Mouthing across Three Settings” by H. S.
Roane, M. L. Kelly, and W.W. Fisher, 2003, Journal of Applied Behavior Analysis, 36,p. 581. Copyright 2003 by the Society for the Experimental Analysis of Behavior, Inc. Reprinted by permission.
Data on Jason’s mouthing were obtained concurrently in a classroom, a playroom, and outdoors—three settings that contained a variety of inedible objects and where caretakers had reported Jason’s mouthing to be problematic. Observers in each setting unobtrusively tallied the number of times Jason inserted an inedible object past the plane of his lips during 10-minute sessions. The researchers reported that Jason’s object mouthing usually consisted of a series of discrete episodes, rather than an extended, continuous event, and that he often placed multiple objects (inedible objects and food) in his mouth simultaneously.
Roane and colleagues (2003) described the baseline and treatment conditions for Jason as follows:
The baseline condition was developed based on the functional analysis results, which showed that mouthing was maintained by automatic reinforcement and occurred independent of social consequences. During baseline, a therapist was present (approximately 1.5 to 3 m from Jason), but all occurrences of mouthing were ignored (i.e., no social consequences were arranged for mouthing, and Jason was allowed to place items in his mouth). No food items were available during baseline.
The treatment condition was identical to baseline except that Jason had continuous access to foods that had been
	5	10	15	20
Sessions
previously identified to compete with the occurrence of object mouthing: chewing gum, marshmallows, and hard candy. Jason wore a fanny pack containing these items around his waist. (pp. 580–581) 
The staggered sequence in which the treatment was implemented in each setting and the results are shown in Figure 4. During baseline, Jason’s mouthed objects at mean rates of 0.9, 1.1, and 1.2 responses per minute in the classroom, a playroom, and outdoor settings, respectively. Introduction of the fanny pack with food in each setting produced an immediate drop to a zero or near zero rate of mouthing. During treatment, Jason put items of food from the fanny pack into his mouth at mean rates of 0.01, 0.01, and 0.07 responses per minute in the classroom, a playroom, and outdoor settings, respectively. The multiple baseline across settings design revealed a clear functional relation between the treatment and the frequency of Jason’s object mouthing. No measures obtained during the treatment condition were as high as the lowest measures in baseline. During 22 of 27 treatment sessions across the three settings, Jason put no inedible objects in his mouth.
As was done in the study by Roane and colleagues (2003), the data paths that comprise the different tiers in a multiple baseline across settings design are typically obtained in different physical environments (e.g., Cushing & Kennedy, 1997; Dalton, Martella, & MarchandMartella, 1999). However, the different “settings” in a multiple baseline across settings design may exist in the same physical location and be differentiated from one another by different contingencies in effect, the presence or absence of certain people, and/or the different times of the day. For example, in a study by Parker and colleagues (1984), the presence or absence of other people in the training room constituted the different settings (environments) in which the effects of the independent variable were evaluated. The attention, demand, and no-attention conditions (i.e., contingencies in effect) defined the different settings in a multiple baseline design study by Kennedy, Meyer, Knowles, and Shukla (2000). The afternoon and the morning portions of the school day functioned as different settings in the multiple baseline across settings design used by Dunlap, Kern-Dunlap, Clarke, and Robbins (1991) to analyze the effects of curricular revisions on a student’s disruptive and off-task behaviors.
In some studies using a multiple baseline across settings design, the participants are varied, changing, and perhaps even unknown to the researchers. For example, Van Houten and Malenfant (2004) used a multiple baseline design across two crosswalks on busy streets to evaluate the effects of an intensive driver enforcement program on the percentage of drivers yielding to pedestrians and the number of motor vehicle–pedestrian conflicts. Watson (1996) used a multiple baseline design across men’s rest rooms on a college campus to assess the effectiveness of posting signs in reducing bathroom graffiti.
Multiple Baseline across Subjects Design
In the multiple baseline across subjects design, one target behavior is selected for two or more subjects (or groups) in the same setting. After steady state responding has been achieved under baseline conditions, the independent variable is applied to one of the subjects while baseline conditions remain in effect for the other subjects. When criterion-level or stable responding has been attained for the first subject, the independent variable is applied to another subject, and so on. The multiple baseline across subjects design is the most widely used of all three forms of the design, in part because teachers, clinicians, and other practitioners are commonly confronted by more than one student or client needing to learn the same skill or eliminate the same problem behavior (e.g., Craft, Alber, & Heward, 1998; Kahng, Iwata, DeLeon, & Wallace, 2000; Killu, Sainato, Davis, Ospelt, & Paul, 1998; Kladopoulos & McComas, 2001). Sometimes a multiple baseline design is conducted across “groups” of participants (e.g., Dixon & Holcomb, 2000; Lewis, Powers, Kelk, & Newcomer, 2002; White & Bailey, 1990).
Krantz and McClannahan (1993) used a multiple baseline across subjects design to investigate the effects of introducing and fading scripts to teach children with autism to interact with their peers. The four participants, ages 9 to 12, had severe communication deficits and minimal or absent academic, social, leisure skills. Prior to the study each of the children had learned to follow first photographic activity schedules (Wacker & Berg, 1983) and later written activity schedules that prompted them through chains of academic, self-care, and leisure activities. Although their teachers modeled social interactions, verbally prompted the children to interact, and provided contingent praise and preferred snacks and activities for doing so, the children consistently failed to initiate interactions without adult prompts.
Each session consisted of a continuous 10-minute interval in which observers recorded the number of times each child initiated and responded to peers while engaged in three art activities—drawing, coloring, and painting— that were rotated across sessions throughout the study. Krantz and McClannahan (1993) described the dependent variables as follows:
Initiation to peers was defined as understandable statements or questions that were unprompted by an adult, that were directed to another child by using his or her name or by facing him or her, and that were separated from the speaker’s previous vocalizations by a change in topic or a change in recipient of interaction. . . . Scripted interactions were those that matched the written script, . . . e.g., “Ross, I like your picture.” Unscripted interactions differed from the script by more than changes in conjunctions, articles, prepositions, pronouns, or changes in verb tense; the question, “Would you like some more paper?” was scored as an unscripted initiation because the noun “paper” did not occur in the script. A response was defined as any contextual utterance (word, phrase, or sentence) that was not prompted by the teacher and that occurred within 5 s of a statement or question directed to the target child. . . . Examples of responses were “what?” “okay,” and “yes, I do.” (p. 124)
During baseline, each child found art materials at his or her place and a sheet of paper with the written instructions, “Do your art” and “Talk a lot.” The teacher prompted each child to read the written instructions, then moved away. During the script condition, the two written instructions in baseline were supplemented by scripts consisting of 10 statements and questions such as, “{Name}, did you like to {swing/rollerskate/ride the bike} outside today?” “{Name}, do you want to use one of my pencils/crayons/brushes}?” (p. 124). Immediately before each session, the teacher completed blank portions of the scripts so that they reflected activities the children had completed or were planning and objects in the classroom environment. Each child’s script included the three other children’s names, and the order of the questions or statements varied across sessions and children.
The script condition was implemented with one child at a time, in staggered fashion (see Figure 5). Initially the teacher manually guided the child through the script, prompting him or her to read the statement to another child and to pencil a check mark next to it after doing so.
Krantz and McClannahan (1993) described the prompting and script-fading procedures as follows:
Standing behind a participant, the teacher manually guided him or her to pick up a pencil, point to an instruction or a scripted statement or question, and move the pencil along below the text. If necessary, the teacher also manually guided the child’s head to face another child to whom a statement or question was addressed. If the child did not verbalize the statement or questions within 5 s, the manual guidance procedure was repeated. If the child read or said a statement or read or asked a question, the teacher used the same type of manual guidance to ensure that the child placed a check mark to the left of that portion of the script.
Figure 5	A multiple baseline across subjects design showing the	Baseline	Script	2 Month
Follow-Up
number of scripted and unscripted initiations to peers and responses by four children with autism during baseline, script, and follow-up sessions. Arrows indicate when fading steps occurred.
From “Teaching Children with Autism to Initiate to Peers: Effects of a Script-Fading Procedure” by P. J. Krantz and L. E. McClannahan, 1993, Journal of Applied Behavior Analysis, 26,p. 129. Copyright 1993 by the Society for the Experimental Analysis of Behavior, Inc. Reprinted by permission.
Manual prompts were faded as quickly as possible; no prompts were delivered to Kate, Mike, Walt, and 

Ross after Sessions 15, 18, 23, and 27, respectively, and the teacher remained at the periphery of the classroom throughout subsequent sessions. After manual guidance had been faded for a target child, fading of the script began. Scripts were faded from end to beginning in five phases. For example, the fading steps for the question “Mike, what do you like to do best on Fun Friday?” were (a) “Mike, what do you like to do best,” (b) “Mike, what do you,” (c) “Mike, what,” (d) “M,” and (e) “.” (p. 125)
Kate and Mike, who never initiated during baseline, had mean initiations per session of 15 and 13, respectively, during the script condition. Walt’s initiations increased from a baseline mean of 0.1 to 17 during the script condition, and Ross averaged 14 initiations per session during script compared to 2 during baseline. As the scripts were faded, each child’s frequency of unscripted initiations increased. After the scripts were faded, the four participants’ frequency of initiations were within the same range as that of a sample of three typically developing children. The researchers implemented the script-fading steps with each participant in response to his or her performance, not according to a predetermined schedule, thereby retaining the flexibility needed to pursue the behavior–environment relations that are the focus of the science of behavior.
However, because each subject did not serve as his or her own control, this study illustrates that the multiple baseline across subjects design is not a true single-subject design. Instead, verification of predictions based on the baseline data for each subject must be inferred from the relatively unchanging measures of the behavior of other subjects who are still in baseline, and replication of effects must be inferred from changes in the behavior of other subjects when they come into contact with the independent variable. This is both a weakness and a potential advantage of the multiple baseline across subjects design (Johnston & Pennypacker, 1993a), discussed later in the chapter.
Variations of the Multiple Baseline Design
Two variations of the multiple baseline design are the multiple probe design and the delayed multiple baseline design. The multiple probe design enables the behavior analyst to extend the operation and logic of the multiple baseline tactic to behaviors or situations in which concurrent measurement of all behaviors comprising the design is unnecessary, potentially reactive, impractical, or too costly. The delayed multiple baseline technique can be used when a planned reversal design is no longer possible or proves ineffective; it can also add additional tiers to an already operational multiple baseline design, as would be the case if new subjects were added to an ongoing study.
Multiple Probe Design
The multiple probe design, first described by Horner and Baer (1978), is a method of analyzing the relation between the independent variable and the acquisition of a successive approximation or task sequence. In contrast to the multiple baseline design—in which data are collected simultaneously throughout the baseline phase for each behavior, setting, or subject in the experiment—in the multiple probe design intermittent measures, or probes, provide the basis for determining whether behavior change has occurred prior to intervention. According to Horner and Baer, when applied to a chain or sequence of related behaviors to be learned, the multiple probe design provides answers to four questions: (a) What is the initial level of performance on each step (behavior) in the sequence? (b) What happens when sequential opportunities to perform each step in the sequence are provided prior to training on that step? (c) What happens to each step as training is applied? and (d) What happens to the performance of untrained steps in the sequence as criterion-level performance is reached on the preceding steps?
Figure 6 shows a graphic prototype of the multiple probe design. Although researchers have developed many variations of the multiple probe technique, the basic design has three key features: (a) An initial probe is taken to determine the subject’s level of performance on each behavior in the sequence; (b) a series of baseline measures is obtained on each step prior to training on that step; and (c) after criterion-level performance is reached on any training step, a probe of each step in the sequence is obtained to determine whether performance changes have occurred in any other steps.
Thompson, Braam, and Fuqua (1982) used a multiple probe design to analyze the effects of an instructional procedure composed of prompts and token reinforcement on the acquisition of a complex chain of laundry skills by three students with developmental disabilities. Observations of people doing laundry resulted in a detailed task analysis of 74 discrete responses that were organized into seven major components (e.g., sorting, loading washer). Each student’s performance was assessed via probe and baseline sessions that preceded training on each component. Probe and baseline sessions began with instructions to the student to do the laundry. When an incorrect response was emitted or when no response occurred within 5 seconds of a prompt to continue, the student was seated away from the laundry area. The trainer then performed the correct response and called the student back to the area so that assessment of the rest of the laundry sequence could continue.
 
Figure 6	Graphic prototype of a multiple probe design. Square data points represent results of probe sessions in which the entire sequence or set of behaviors (1–4) are tested.
Probe sessions differed from baseline sessions in two ways. First, a probe measured each response in the entire chain and occurred immediately prior to baseline and training for every component. Baseline sessions occurred following the probe and measured only previously trained components plus the component about to be trained. Baseline data were gathered on a variable number of consecutive sessions immediately prior to training sessions. Second, no tokens or descriptive praise were delivered during probes. During baseline, tokens were delivered for previously trained responses only. . . . Following baseline, each component was trained using a graduated 3-prompt procedure (Horner & Keilitz, 1975), consisting of verbal instruction, modeling, and graduated guidance. If one prompt level failed to produce a correct response within 5 sec, the next level was introduced. . . . When the student performed a component at 100% accuracy for two consecutive trials, he was required to perform the entire laundry chain from the beginning through the component most recently mastered. The entire chain of previously mastered components was
trained (chain training condition) until it was performed without errors or prompts for two consecutive trials. (Thompson, Braam, & Fuqua, 1982, p. 179)
Figure 7 shows the results for Chester, one of the students. Chester performed a low percentage of correct responses during the probe and baseline sessions, but performed with 100% accuracy after training was applied to each component. During a generalization probe conducted at a community laundromat after training, Chester performed correctly 82% of the 74 total responses in the chain. Five additional training sessions were needed to retrain responses performed incorrectly during the generalization probe and to train “additional responses necessitated by the presence of coin slots and minor differences between the training and laundromat equipment” (p. 179). On two follow-up sessions conducted 10 months after training, Chester performed at 90% accu-
 
Figure 7	A multiple probe design showing the percentage of correct responses for each trial on each component of a laundry task by a young adult male with mental retardation. Heavy vertical lines on the horizontal axis represent successive training sessions; lighter and shorter vertical lines indicate trials within a session.
From “Training and Generalization of Laundry Skills: A Multiple-Probe Evaluation with Handicapped Persons” by T. J. Thompson, S. J. Braam, and R.W. Fuqua, 1982, Journal of Applied Behavior Analysis, 15,p. 180. Copyright 1982 by the Society for the Experimental Analysis of Behavior, Inc. Reprinted by permission.
racy even though he had not performed the laundry task for the past 2 months. Similar results were obtained for the other two students who participated in the study.
Thompson and colleagues (1982) added the chain training condition to their study because they believed that components trained as independent skills were unlikely to be emitted in correct sequence without such practice. It should be noted that the experimenters did not begin training a new component until stable responding had been achieved during baseline observations (see the baseline data for the bottom four tiers in Figure 7). Delaying the training in this manner enabled a clear demonstration of a functional relation between training and skill acquisition.
The multiple probe design is particularly appropriate for evaluating the effects of instruction on skill sequences in which it is highly unlikely that the subject can improve performance on later steps in the sequence without acquiring the prior steps. For example, the repeated measurement of the accuracy in solving division problems of a student who possesses no skills in addition, subtraction, and multiplication would add little to an analysis. Horner and Baer (1978) made this point exceedingly well:
The inevitable zero scores on the division baseline have no real meaning: division could be nothing else than zero (or chance, depending on the test format), and there is no real point in measuring it. Such measures are pro forma: they fill out the picture of a multiple baseline, true, but in an illusory way. They do not so much represent zero behavior as zero opportunity for the behavior to occur, and there is no need to document at the level of well-measured data that behavior does not occur when it cannot. (p. 190)
Thus, the multiple probe design avoids the necessity of collecting ritualistic baseline data when the performance of any component of a chain or sequence is impossible or unlikely before acquisition of its preceding components. In addition to the two uses already mentioned—analysis of the effects of instruction on complex skill sequences and reduction in the amount of baseline measurement for behaviors that have no plausible opportunity to occur—the multiple probe technique is also an effective experimental strategy for situations in which extended baseline measurement may prove reactive, impractical, or costly. The repeated measurement of a skill under nontreatment conditions can prove aversive to some students; and extinction, boredom, or other undesirable responses can occur. In his discussion of multiple baseline designs, Cuvo (1979) suggested that researchers should recognize that “there is a trade-off between repeatedly administering the dependent measure to establish a stable baseline on one hand and risking impaired performance by subjecting participants to a potentially punishing experience on the other hand” (pp. 222–223). Furthermore, complete assessment of all skills in a sequence may require too much time that could otherwise be spent on instruction.
Figure 8	Graphic prototype of a delayed multiple baseline design.
Other examples of the multiple probe design can be found in Arntzen, Halstadtr, and Halstadtr (2003); Coleman-Martin & Wolff Heller (2004); O’Reilly, Green, and Braunling-McMorrow, (1990); and Werts, Caldwell and Wolery.
Delayed Multiple Baseline Design
The delayed multiple baseline design is an experimental tactic in which an initial baseline and intervention are begun, and subsequent baselines are added in a staggered or delayed fashion (Heward, 1978). Figure 8 shows a graphic prototype of the delayed multiple baseline design. The design employs the same experimental reasoning as a full-scale multiple baseline design with the exception that data from baselines begun after the independent variable has been applied to previous behaviors, settings, or subjects cannot be used to verify predictions based on earlier tiers of the design. In Figure 8 baseline measurement of Behaviors 2 and 3 was begun early
Baseline	Treatment 

enough for those data to be used to verify the prediction made for Behavior 1. The final four baseline data points for Behavior 3 also verify the prediction for Behavior 2. However, baseline measurement of Behavior 4 began after the independent variable had been applied to each of the previous behaviors, thus limiting its role in the design to an additional demonstration of replication.
A delayed multiple baseline design may allow the behavior analyst to conduct research in certain environments in which other experimental tactics cannot be implemented. Heward (1978) suggested three such situations.
•	A reversal design is no longer desirable or possible. In applied settings the research environment may shift, negating the use of a previously planned reversal design. Such shifts may involve changes in the subject’s environment that make the target behavior no longer likely to reverse to baseline levels, or changes in the behavior of parents, teachers, administrators, the subject/client, or the behavior analyst that, for any number of reasons, make a previously planned reversal design no longer desirable or possible. . . . If there are other behaviors, settings, or subjects appropriate for application of the independent variable, the behavior analyst could use a delayed multiple baseline technique and still pursue evidence of a functional relation.
•	Limited resources, ethical concerns, or practical difficulties preclude a full-scale multiple baseline design. This situation occurs when the behavior analyst only controls resources sufficient to initially record and intervene with one behavior, setting, or subject, and another research strategy is inappropriate. It may be that as a result of the first intervention, more resources become available for gathering additional baselines. This might occur following the improvement of certain behaviors whose pretreatment topography and/or rate required an inordinate expenditure of staff resources. Or, it could be that a reluctant administrator, after seeing the successful results of the first intervention, provides the resources necessary for additional analysis. Ethical concerns may preclude extended baseline measurement of some behaviors (e.g., Linscheid, Iwata, Ricketts, Williams, & Griffin, 1990). Also under this heading would fall the “practical difficulties” cited by Hobbs and Holt (1976) as a reason for delaying baseline measurement in one of three settings.
•	A “new” behavior, setting, or subject becomes available. A delayed multiple baseline technique might be employed when another research design was originally planned but a multiple baseline analysis becomes the preferred approach due to changes in the environment (e.g., the subject begins to emit another behavior appropriate for intervention with the experimental variable, the subject begins to emit the original target behavior in another setting, or additional subjects displaying the same target behavior become available.)
(adapted from pp. 5–6)
Researchers have used the delayed multiple baseline technique to evaluate the effects of a wide variety of interventions (e.g., Baer, Williams, Osnes, & Stokes, 1984; Copeland, Brown, & Hall, 1974; Hobbs & Holt, 1976; Jones, Fremouw, & Carples, 1977; Linscheid et al., 1990; Risley & Hart, 1968; Schepis, Reid, Behrmann, & Sutton, 1998; White & Bailey, 1990. Poche, Brouwer, and Swearingen (1981) used a delayed multiple baseline design to evaluate the effects of a training program designed to prevent children from being abducted by adults. Three typically developing preschool children were selected as subjects because, during a screening test, each readily agreed to leave with an adult stranger. The dependent variable was the level of appropriateness of self-protective responses emitted by each child when an adult suspect approached the child and attempted to lure her away with a simple lure (“Would you like to go for a walk?”), an authoritative lure (“Your teacher said it was alright for you to come with me.”), or an incentive lure (“I’ve got a nice surprise in my car. Would you like to come with me and see it?”).
Each session began with the child’s teacher bringing the child outdoors, then pretending to have to return to the building for some reason. The adult suspect (a confederate of the experimenters but unknown to the child) then approached the child and offered one of the lures. The confederate also served as observer, scoring the child’s response on a 0 to 6 scale, with a score of 6 representing the desired response (saying, “No, I have to go ask my teacher” and moving at least 20 feet away from the suspect within 3 seconds) and a score of 0 indicating that the child moved some distance away from the school building with the suspect. Training consisted of modeling, behavioral rehearsal, and social reinforcement for correct responses.
Figure 9 shows the results of the training program. During baseline, all three children responded to the lures with safety ratings of 0 or 1. All three children mastered correct responses to the incentive lure in one to three training sessions, with one or two more sessions required for each child to master correct responses to the other two lures. Overall, training took approximately 90 minutes per child distributed over five or six sessions. All three children responded correctly when the lures were administered in generalization probes on sidewalk locations 150 to 400 feet from the school.
Figure 9	A delayed multiple baseline 	Baseline	Training	Generality	Follow-Up design showing the level of appropriateness of self-protective responses during baseline, training, and generality probes in school and community settings. Closed symbols indicate data gathered near the school; open symbols, in a location away from the school.
From “Teaching Self-Protection to Young Children” by C. Poche, R. Brouwer, and M. Swearingen, 1981, Journal of Applied Behavior Analysis, 14,p. 174. Copyright 1981 by the Society for the
Experimental Analysis of Behavior, Inc. Reprinted by permission.
Type of Lure
 
Although each baseline in this study was of equal length (i.e., had an equal number of data points), contradicting the general rule that the baselines in a multiple baseline design should vary significantly in length, there are two good reasons that Poche and colleagues began training when they did with each subject. First, the nearly total stability of the baseline performance of each child provided an ample basis for evaluating the training program (the only exception to complete susceptibility to the adult suspect’s lures occurred when Stan stayed near the suspect instead of actually going away with him on his fourth baseline observation). Second, and more important, the nature of the target behavior required that it be taught to each child as soon as possible. Although continuing baseline measurement for varying lengths across the different tiers of any multiple baseline design is good practice from a purely experimental viewpoint, the ethics of such a practice in this instance would be highly questionable, given the potential danger of exposing the children to adult lures repeatedly while withholding training.
The delayed multiple baseline design presents several limitations (Heward, 1978). First, from an applied standpoint the design is not a good one if it requires the behavior analyst to wait too long to modify important behaviors, although this problem is inherent in all multiple baseline designs. Second, in a delayed multiple baseline design there is a tendency for the delayed baseline phases to contain fewer data points than are found in a standard multiple baseline design, in which all baselines are begun simultaneously, resulting in baseline phases of considerable and varying length. Long baselines, if stable, provide the predictive power that permits convincing demonstrations of experimental control. Behavior analysts using any type of multiple baseline design must be sure that all baselines, regardless of when they are begun, are of sufficient and varied length to provide a believable basis for comparing experimental effects. A third limitation of the delayed multiple baseline design is that it can mask the interdependence of dependent variables.
The strength of any multiple baseline design is that little or no change is noticed in the other, as yet untreated, behaviors until, and only until, the experimenter applies the independent variable. In a delayed multiple baseline design, the “delayed baseline” data gathered for subsequent behaviors may represent changed performance due to the experimental manipulation of other behaviors in the design and, therefore, may not be representative of the true, preexperimental operant level. . . . In such instances, the delayed multiple baseline might result in a “false negative,” and the researcher may erroneously conclude that the intervention was not effective on the subsequent behavior(s), when in reality the lack of simultaneous baseline data did not permit the discovery that the behaviors covaried. This is a major weakness of the delayed multiple baseline design and makes it a research tactic of second choice whenever a full-scale multiple baseline can be employed. However, this limitation can and should be combated whenever possible by beginning subsequent baselines at least several sessions prior to intervention on previous baselines. (Heward,
1978, pp. 8–9)
Both the multiple probe design and the delayed multiple baseline design offer the applied behavior analyst alternative tactics for pursuing a multiple baseline analysis when extended baseline measurement is unnecessary, impractical, too costly, or unavailable. Perhaps the most useful application of the delayed multiple baseline technique is in adding tiers to an already operational multiple baseline design. Whenever a delayed baseline can be supplemented by probes taken earlier in the course of the study, experimental control is strengthened. As a general rule, the more baseline data, the better.
Assumptions and Guidelines for Using Multiple Baseline Designs
Like all experimental tactics, the multiple baseline design requires the researcher to make certain assumptions about how the behavior–environment relations under investigation function, even though discovering the existence and operation of those relations is the very reason for conducting the research. In this sense, the design of behavioral experiments resembles an empirical guessing game—the experimenter guesses; the data answer. The investigator makes assumptions, hypotheses in the informal sense, about behavior and its relation to controlling variables and then constructs experiments designed to produce data capable of verifying or refuting those conjectures. 
Because verification and replication in the multiple baseline design depends on what happens, or does not happen, to other behaviors as a result of the sequential application of the independent variable, the experimenter must be particularly careful to plan and carry out the design in a manner that will afford the greatest degree of confidence in any relations suggested by the data. Although the multiple baseline design appears deceptively simple, its successful application entails much more than selecting two or more behaviors, settings, or subjects, collecting some baseline data, and then introducing a treatment condition to one behavior after the other. We suggest the following guidelines for designing and conducting experiments using multiple baseline designs.
Select Independent, yet Functionally Similar, Baselines
Demonstration of a functional relation in a multiple baseline design depends on two occurrences: (a) the behavior(s) still in baseline showing no change in level, variability, or trend while the behavior(s) in contact with the independent variable changes; and (b) each behavior changes when, and only when, the independent variable has been applied to it. Thus, the experimenter must make two, at times seemingly contradictory, assumptions about the behaviors targeted for analysis in a multiple baseline design. The assumptions are that the behaviors are functionally independent of one another (the behaviors will not covary with one another), and yet the behaviors share enough similarity that each will change when the same independent variable is applied to it (Tawney & Gast, 1984). An error in either assumption can result in a failure to demonstrate a functional relation.
For example, let us suppose that the independent variable is introduced with the first behavior, and changes in level and/or trend are noted, but the other behaviors still in baseline also change. Do the changes in the stillin-baseline behaviors mean that an uncontrolled variable is responsible for the changes in all of the behaviors and that the independent variable is an effective treatment? Or do the simultaneous changes in the untreated behaviors mean that the changes in the first behavior were affected by the independent variable and have generalized to the other behaviors? Or, let us suppose instead that the first behavior changes when the independent variable is introduced, but subsequent behaviors do not change when the independent variable is applied. Does this failure to replicate mean that a factor other than the independent variable was responsible for the change observed in the first behavior? Or does it mean only that the subsequent behaviors do not operate as a function of the experimental variable, leaving open the possibility that the change noted in the first behavior was affected by the indepen-
dent variable?
Answers to these questions can be pursued only by further experimental manipulations. In both kinds of failure to demonstrate experimental control, the multiple baseline design does not rule out the possibility of a functional relation between the independent variable and the behavior(s) that did change when the variable was applied. In the first instance, the failure to demonstrate experimental control with the originally planned design is offset by the opportunity to investigate and possibly isolate the variable robust enough to change multiple behaviors simultaneously. Discovery of variables that reliably produce generalized changes across behaviors, settings, and/or subjects is a major goal of applied behavior analysis; and if the experimenter is confident that all other relevant variables were held constant before, during, and after the observed behavior changes, the original independent variable is the first candidate for further investigation.
In the second situation, with its failure to replicate changes from one behavior to another, the experimenter can pursue the possibility of a functional relation between the independent variable and the first behavior, perhaps using a reversal technique, and seek to discover later an effective intervention for the behavior(s) that did not change. Another possibility is to drop the original independent variable altogether and search for another treatment that might be effective with all of the targeted behaviors.
Select Concurrent and Plausibly Related Multiple Baselines
In an effort to ensure the functional independence of behaviors in a multiple baseline design, experimenters should not select response classes or settings so unrelated to one another as to offer no plausible means of comparison. For the ongoing baseline measurement of one behavior to provide the strongest basis for verifying the prediction of another behavior that has been exposed to an independent variable, two conditions must be met: (a) The two behaviors must be measured concurrently, and (b) all of the relevant variables that influence one behavior must have an opportunity to influence the other behavior. Studies that employ a multiple baseline approach across subjects and settings often stretch the logic of the design beyond its capabilities. For example, using the stable baseline measures of one child’s compliance with parental requests as the basis for verifying the effect of intervention on the compliance behavior of another child living with another family is questionable practice. The sets of variables influencing the two children are surely differentiated by more than the presence or absence of the experimental variable.
There are some important limits to designating multiple behavior/setting combinations that are intended to function as part of the same experiment. In order for the use of multiple behaviors and settings to be part of the same design and thus augment experimental reasoning, the general experimental conditions under which the two responses (whether two from one subject or one from each of two subjects) are emitted and measured must be ongoing concurrently. . . . Exposure [to the independent variable] does not have to be simultaneous for the different behavior/setting combinations, [but] it must be the identical treatment conditions along with the associated extraneous variables that impinge on the two responses and/or settings. This is because the conditions imposed on one behavior/setting combination must have the opportunity of influencing the other behavior/setting combination at the same time, regardless of the condition that actually prevails for the second. . . . It follows that using responses of two subjects each responding in different settings would not meet the requirement that there be a coincident opportunity for detecting the treatment effect. A treatment condition [as well as the myriad other variables possibly responsible for changes in the behavior of one subject] could not then come into contact with the responding of the other subject, because the second subject’s responding would be occurring in an entirely different location. . . . Generally, the greater the plausibility that the two responses would be affected by the single treatment [and all other relevant variables], the more powerful is the demonstration of experimental control evidenced by data showing a change in only one behavior. (Johnston and Pennypacker, 1980, pp. 276–278)
The requirements of concurrency and plausible influence must be met for the verification element of baseline logic to operate in a multiple baseline design. However, replication of effect is demonstrated each time a baseline steady state is changed by the introduction of the independent variable, more or less regardless of where or when the variable is applied. Such nonconcurrent and/or unrelated baselines can provide valuable data on the generality of a treatment’s effectiveness. 
This discussion should not be interpreted to mean that a valid (i.e., logically complete) multiple baseline design cannot be conducted across different subjects, each responding in different settings. Numerous studies using mixed multiple baselines across subjects, responses classes, and/or settings have contributed to the development of an effective technology of behavior change (e.g., Dixon et al., 1998; Durand, 1999; Ryan, Ormond, Imwold, & Rotunda, 2002).
Let us consider an experiment designed to analyze the effects of a particular teacher training intervention, perhaps a workshop on using tactics to increase each student’s opportunity to respond during group instruction. Concurrent measurement is begun on the frequency of student response opportunities in the classrooms of the teachers who are participating in the study. After stable baselines have been established, the workshop is presented first to one teacher (or group of teachers) and eventually, in staggered multiple baseline fashion, to all of the teachers.
In this example, even though the different subjects (teachers) are all behaving in different environments (different classrooms), comparison of their baseline conditions is experimentally sound because the variables likely to influence their teaching styles operate in the larger, shared environment in which they all behave (the school and teaching community). Nevertheless, whenever experiments are proposed or published that involve different subjects responding in different settings, researchers and consumers should view the baseline comparisons with a critical eye toward their logical relation to one other.
Do Not Apply the Independent Variable to the Next BehaviorToo Soon
To reiterate, for verification to occur in a multiple baseline design, it must be established clearly that as the independent variable is applied to one behavior and change is noted, little or no change is observed in the other, asyet-untreated behaviors. The potential for a powerful demonstration of experimental control has been destroyed in many studies because the independent variable was applied to subsequent behaviors too soon. Although the operational requirement of sequential application in the multiple baseline tactic is met by introduction of the independent variable even in adjacent time intervals, the experimental reasoning afforded by such closely spaced manipulations is minimal.
The influence of unknown, concomitant, extraneous variables that might be present could still be substantial, even a day or two later. This problem can be avoided by demonstrating continued stability in responding for the second behavior/setting combination during and after the introduction of the treatment for the first combination until a sufficient period of time has elapsed to detect any effect on the second combination that might appear.
(Johnston & Pennypacker, 1980, p. 283)
Vary Significantly the Lengths of Multiple Baselines
Generally, the more the baseline phases in a multiple baseline design differ in length from one another, the stronger the design will be. Baselines of significantly different lengths allow the unambiguous conclusion (assuming an effective treatment variable) that each behavior not only changes when the independent variable is applied, but also that each behavior does not change until the independent variable has been applied. If the different baselines are of the same or similar length, the possibility exists that changes noted when the independent variable is introduced are the result of a confounding variable, such as practice or reactivity to observation and measurement, and not a function of the experimental variable.
Those effects . . . called practice, adaptation, warm-up, self-analysis, etc.; whatever they may be and whatever they may be called, the multiple baseline design controls for them by systematically varying the length of time (sessions, days, weeks) in which they occur prior to the introduction of the training package. . . . Such control is essential, and when the design consists of only two baselines, then the number of data points in each prior to experimental intervention should differ as radically as possible, at least by a factor of 2. I cannot see not systematically varying lengths of baselines prior to intervention, and varying them as much as possible/practical. Failure to do that . . . weakens the design too much for credibility. (D. M. Baer, personal communication, June 2, 1978)
Intervene on the Most Stable Baseline First
In the ideal multiple baseline design, the independent variable is not applied to any of the behaviors until steady state responding has been achieved for each. However, the applied behavior analyst is sometimes denied the option of delaying treatment just to increase the strength of an experimental analysis. When intervention must begin before stability is evident across each tier of the design, the independent variable should be applied to the behavior, setting, or subject that shows the most stable level of baseline responding. For example, if a study is designed to evaluate the effects of a teaching procedure on the rate of math computation of four students and there is no a priori reason to teach the students in any particular sequence, instruction should begin with the student showing the most stable baseline. However, this recommendation should be followed only when the majority of the baselines in the design show reasonable stability.
Sequential application of the independent variable should be made in the order of greatest stability at the time of each subsequent application. Again, however, the realities of the applied world must be heeded. The social significance of changing a particular behavior must sometimes take precedence over the desire to meet the requirements of experimental design.
Considering the Appropriateness of Multiple Baseline Designs
The multiple baseline design offers significant advantages, which no doubt have accounted for its widespread use by researchers and practitioners. Those advantages, however, must be weighed against the limitations and weaknesses of the design to determine its appropriateness in any given situation.
Advantages of the Multiple Baseline Design
Probably the most important advantage of the multiple baseline design is that it does not require withdrawing a seemingly effective treatment to demonstrate experimental control. This is a critical consideration for target behaviors that are self-injurious or dangerous to others. This feature of the multiple baseline design also makes it an appropriate method for evaluating the effects of independent variables that cannot, by their nature, be withdrawn and for investigating target behaviors that are likely or that prove to be irreversible (e.g., Duker & van Lent, 1991). Additionally, because the multiple baseline design does not necessitate a reversal of treatment gains to baseline levels, parents, teachers, or administrators may accept it more readily as a method of demonstrating the effects of an intervention.
The requirement of the multiple baseline design to sequentially apply the independent variable across multiple behaviors, settings, or subjects complements the usual practice of many practitioners whose goal is to develop multiple behavior changes. Teachers are charged with helping multiple students learn multiple skills to be used in multiple settings. Likewise, clinicians typically need to help their clients improve more than one response class and emit more adaptive behavior in several settings. The multiple baseline design is ideally suited to the evaluation of the progressive, multiple behavior changes sought by many practitioners in applied settings.
Because the multiple baseline design entails concurrent measurement of two or more behaviors, settings, or subjects, it is useful in assessing the occurrence of generalization of behavior change. The simultaneous monitoring of several behaviors gives the behavior analyst the opportunity to determine their covariation as a result of manipulations of the independent variable (Hersen & Barlow, 1976). Although changes in behaviors still under baseline conditions eliminate the ability of the multiple baseline design to demonstrate experimental control, such changes reveal the possibility that the independent variable is capable of producing behavioral improvements with desirable generality, thereby suggesting an additional set of research questions and analytic tactics (e.g., Odom, Hoyson, Jamieson, & Strain, 1985).
Finally, the multiple baseline design has the advantage of being relatively easy to conceptualize, thereby offering an effective experimental tactic for teachers and parents who are not trained formally in research methodology (Hall et al., 1970).
Limitations of the Multiple Baseline Design
The multiple baseline design presents at least three scientific limitations or considerations. First, a multiple baseline design may not allow a demonstration of experimental control even though a functional relation exists between the independent variable and the behaviors to which it is applied. Changes in behaviors still under baseline conditions and similar to concurrent changes in a behavior in the treatment condition preclude the demonstration of a functional relation within the original design. Second, from one perspective, the multiple baseline design is a weaker method for showing experimental control than the reversal design. This is because verification of the baseline prediction made for each behavior within a multiple baseline design is not directly demonstrated with that behavior, but must be inferred from the lack of change in other behaviors. This weakness of the multiple baseline design, however, should be weighed against the design’s advantage of providing multiple replications across different behaviors, settings, or subjects. Third, the multiple baseline design provides more information about the effectiveness of the treatment variable than it does about the function of any particular target behavior.
Consistently [the] multiple baseline is less an experimental analysis of the response than of the technique used to alter the response. In the reversal design, the response is made to work again and again; in the multiplebaseline designs, it is primarily the technique that works again and again, and the responses either work once each [if different responses are used] or else a single response works once each per setting or once each per subject. Repetitive working of the same response in the same subject or the same setting is not displayed. But, while repetitive working of the response is foregone, repetitive and diverse working of the experimental technique is maximized, as it would not be in the reversal design. (Baer, 1975, p. 22)
Two important applied considerations that must be evaluated in determining the appropriateness of the multiple baseline design are the time and resources required for its implementation. Because the treatment variable cannot be applied to subsequent behaviors, settings, or subjects until its effects have been observed on previous behaviors, settings, or subjects, the multiple baseline design requires that intervention be withheld for some behaviors, settings, or subjects, perhaps for a long time. This delay raises practical and ethical concerns. Treatment cannot be delayed for some behaviors; their importance makes delaying treatment impractical. And as Stolz (1978) pointed out, “If the intervention is generally acknowledged to be effective, denying it simply to achieve a multiple-baseline design might be unethical” (p. 33). Second, the resources needed for the concurrent measurement of multiple behaviors must be considered. Use of a multiple baseline design can be particularly costly when behavior must be observed and measured in several settings. However, when the use of intermittent probes during baseline can be justified in lieu of continuous measurement (Horner & Baer, 1978), the cost of concurrently measuring multiple behaviors can be reduced.
Changing Criterion Design
The changing criterion design can be used to evaluate the effects of a treatment that is applied in a graduated or stepwise fashion to a single target behavior. The changing criterion design was first described in the applied behavior analysis literature in two papers coauthored by Vance Hall (Hall & Fox, 1977; Hartmann & Hall, 1976).
Operation and Logic of the Changing Criterion Design
The reader can refer to Figure 10 before and after reading Hartmann and Hall’s (1976) description of the changing criterion design.
The design requires initial baseline observations on a single target behavior. This baseline phase is followed by implementation of a treatment program in each of a series of treatment phases. Each treatment phase is associated with a step-wise change in criterion rate for the target behavior. Thus, each phase of the design provides a baseline for the following phase. When the rate of the target behavior changes with each stepwise change in the criterion, therapeutic change is replicated and experimental control is demonstrated. (p. 527)
The operation of two elements of baseline logic— prediction and replication—is clear in the changing criterion design. When stable responding is attained within each phase of the design, a prediction of future responding is made. Replication occurs each time the level of behavior changes in a systematic way when the criterion is changed. Verification of the predictions based on each phase is not so obvious in this design but can be approached in two ways. First, varying the lengths of phases systematically enables a form of self-evident verification. The prediction is made that the level of responding will not change if the criterion is not changed. When the criterion is not changed and stable responding continues, the prediction is verified. When it can be shown within the design that levels of responding do not change unless the criterion is changed, regardless of the varied lengths of phases, experimental control is evident. Hall and Fox (1977) suggested another possibility for verification: “The experimenter may return to a former criterion and if the behavior conforms to this criterion level there is also a cogent argument for a high degree of behavioral control” (p. 154). Such a reversed criterion is shown in the nextto-last phase of Figure 10. Although returning to an earlier criterion level requires a brief interruption of the steady improvement in behavior, the reversal tactic strengthens the analysis considerably and should be included in the changing criterion design unless other factors indicate its inappropriateness.
	Baseline	Treatment	Figure 10	Graphic
 
One way to conceptualize the changing criterion design is as a variation of the multiple baseline design. Both Hartmann and Hall (1976, p. 530) and Hall and Fox (1977, p. 164) replotted data from changing criterion design experiments in a multiple baseline format with each tier of the multiple baseline showing the occurrence or nonoccurrence of the target behavior at one of the criterion levels used in the experiment. A vertical condition change line doglegs through the tiers indicating when the criterion for reinforcement was raised to the level represented by each tier. By graphing whether the target behavior was emitted during each session at or above the level represented on each tier both before and after the change in criterion to that level, a kind of multiple baseline analysis is revealed. However, the strength of the multiple baseline argument is not quite so convincing because the “different” behaviors represented by each tier are not independent of one another. For example, if a target behavior is emitted 10 times in a given session, all of the tiers representing criteria below 10 responses would have to show that the behavior occurred, and all of the tiers representing criteria of 11 or more would have to show no occurrence of the behavior, or zero responding. The majority of the tiers that would appear to show verification and replication of effect, in fact, could only show these results because of the events plotted on another tier. A multiple baseline design provides its convincing demonstration of experimental control because the measures obtained for each behavior in the design are a function of the controlling variables for that behavior, not artifacts of the measurement of another behavior. Thus, recasting the data from a changing criterion design into a many-tiered multiple baseline format will often result in a biased picture in favor of experimental control.
Even though the multiple baseline design is not completely analogous, the changing criterion design can be conceptualized as a method of analyzing the development of new behaviors. As Sidman (1960) pointed out, “It is possible to make reinforcement contingent upon a specified value of some aspect of behavior, and to treat that value as a response class in its own right” (p. 391). The changing criterion design can be an effective tactic for showing the repeated production of new rates of behavior as a function of manipulations of the independent variable (i.e., criterion changes).
Other than the experiments included in the Hartmann and Hall (1976) and Hall and Fox (1977) papers, there have been relatively few examples of pure changing criterion designs published in the applied behavior analysis literature (e.g., DeLuca & Holborn, 1992; Foxx & Rubinoff, 1979; Johnston & McLaughlin, 1982). Some researchers have employed a changing criterion tactic as an analytic element within a larger design (e.g., Martella, Leonard, Marchand-Martella, & Agran, 1993; Schleien, Wehman, & Kiernan, 1981).
Allen and Evans (2001) used a changing criterion design to evaluate the effects of an intervention to reduce the excessive checking of blood sugar levels by Amy, a 15-year-old girl diagnosed with insulin-dependent diabetes about 2 years prior to the study. Persons with this form of diabetes must guard against hypoglycemia (i.e., low blood sugar), a condition that produces a cluster of symptoms such as headaches, dizziness, shaking, impaired vision, and increased heart rate, and can lead to seizures and loss of consciousness. Because hypoglycemic episodes are physically unpleasant and can be a source of social embarrassment, some patients become hypervigilent in avoiding them, checking for low blood sugar more often than is necessary and deliberately maintaining high blood glucose levels. This leads to poor metabolic control and increased risk of complications such as blindness, renal failure, and heart disease.
At home Amy’s parents helped her monitor her blood sugar levels and insulin injections; at school Amy checked her blood glucose levels independently. Her physician recommended that Amy keep her blood sugar levels between 75 and 150 mg/dl, which required her to check her blood sugar 6 to 12 times per day. Soon after she had been diagnosed with diabetes, Amy experienced a single hypoglycemic episode in which her blood sugar fell to 40 mg/dl, and she experienced physical symptoms but no loss of consciousness. After that episode Amy began checking her glucose levels more and more often, until at the time of her referral she was conducting 80 to 90 checks per day, which cost her parent approximately $600 per week in reagent test strips. Amy was also maintaining her blood sugar level between 275 to 300 mg/dl, far above the recommended levels for good metabolic control.
Following a 5-day baseline condition, a treatment was begun in which Amy and her parents were exposed to a gradually decreasing amount of information about her blood glucose level. Over a 9-month period Amy’s parents gradually reduced the number of test strips she was given each day, beginning with 60 strips during the first phase of the treatment. Allen and Evans (2001) explained the treatment condition and method for changing criteria as follows:
The parents expressed fears, however, that regardless of the criterion level, Amy might encounter a situation in which additional checking would be necessary. Concerns about adherence to the exposure protocol by the parents resulted in a graduated protocol in which Amy could earn a small number of additional test strips above and beyond the limit set by the parents. One additional test strip could be earned for each half hour of engagement in household chores. Amy was allowed to earn a maximum of five additional tests above the criterion when the criterion was set at 20 test strips or higher. Amy was allowed two additional test strips when the criterion was set below 20. Access to test strips was reduced in graduated increments, with the parents setting criteria to levels at which they were willing to adhere. Criteria changes were contingent upon Amy successfully reducing total test strip use to below the criterion on 3 successive days. (p. 498)
Figure 11 shows the criterion changes and the number of times Amy monitored her blood glucose level during the last 10 days of each criterion level. The results
 
Figure 11	A changing criterion design showing the number of blood glucose monitoring checks conducted during the last 10 days of each criterion level. Dashed lines and corresponding numbers indicate the maximum number of test strips allotted at each level. Checks above the criterion levels were conducted with additional test strips earned by Amy.
From “Exposure-Based Treatment to Control Excessive Blood Glucose Monitoring” by K. D. Allen and J. H. Evans,
2001, Journal of Applied Behavior Analysis, 12,p. 499. Copyright 2001 by the Society for the Experimental Analysis
of Behavior, Inc. Reprinted by permission.
clearly show that Amy responded well to the treatment and rarely exceeded the criterion. Over the course of the 9-month treatment program,Amy reduced the number of times she monitored her blood sugar from 80 to 95 times per day during baseline to fewer than 12 tests per day, a level that she maintained at a 3-month follow-up. Amy’s parents indicated that they did not plan to decrease the criterion any further. A concern was that Amy might maintain high blood sugar levels during treatment. The authors reported that her blood sugar levels increased initially during treatment, but gradually decreased over the treatment program to a range of 125 to 175 mg/dl, within or near the recommended level.
Although the figure shows data only for the final 10 days of each criterion level, it is likely that the phases varied in length.  The study consisted of seven criterion changes of two magnitudes, 20 and 2. Although greater variation in the magnitude of criterion changes and a return to a previously attained higher criterion level may have provided a more convincing demonstration of experimental control, the practical and ethical considerations of doing so would be questionable. As always, the applied behavior analyst must balance experimental concerns with the need to improve behavior in the most effective, efficient, ethical manner.
This study illustrates very well the changing criterion design’s flexibility and is a good example of behavior analysts and clients working together. “Because the parents were permitted to regulate the extent of each criterion change, the intervention was quite lengthy. However, by allowing the parents to adjust their own exposure to acceptable levels, adherence to the overall procedure may have been improved.” (Allen & Evans, 2001, p. 500)
Guidelines for Using the Changing Criterion Design
Proper implementation of the changing criterion design requires the careful manipulation of three design factors: length of phases, magnitude of criterion changes, and number of criterion changes.
Length of Phases
Because each phase in the changing criterion design serves as a baseline for comparing changes in responding measured in the next phase, each phase must be long enough to achieve stable responding. “Each treatment phase must be long enough to allow the rate of the target behavior to restabilize at a new and changed rate; it is stability after change has been achieved, and before introduction of the next change in criterion, that is crucial to producing a convincing demonstration of control” (Hartmann & Hall, 1976, p. 531). Target behaviors that are slower to change therefore require longer phases.
The length of phases in a changing criterion design should vary considerably to increase the design’s validity. For experimental control to be evident in a changing criterion design, the target behavior not only must change to the level required by each new criterion in a predictable (preferably immediate) fashion, but also must conform to the new criterion for as long as it is in effect. When the target behavior closely follows successively more demanding criteria that are held in place for varied periods of time, the likelihood is reduced that the observed changes in behavior are a function of factors other than the independent variable (e.g., maturation, practice effects). In most situations, the investigator should not set a predetermined number of sessions for which each criterion level will remain in effect. It is best to let the data guide ongoing decisions whether to extend the length of a current criterion phase or introduce a new criterion.
Magnitude of Criterion Changes
Varying the size of the criterion changes enables a more convincing demonstration of experimental control. When changes in the target behavior occur not only at the time a new criterion is implemented but also to the level specified by the new criterion, the probability of a functional relation is strengthened. In general, a target behavior’s immediate change to meet a large criterion change is more impressive than a behavior change in response to a small criterion change. However, two problems arise if criterion changes are too large. First, setting aside practical considerations, and speaking from a design standpoint only, large criterion changes may not permit inclusion of a sufficient number of changes in the design (the third design factor) because the terminal level of performance is reached sooner. The second problem is from an applied view: Criterion changes cannot be so large that they conflict with good instructional practice. Criterion changes must be large enough to be detectable, but not so large as to be unachievable. Therefore, the variability of the data in each phase must be considered in determining the size of criterion changes. Smaller criterion changes can be employed with very stable levels of responding, whereas larger criterion changes are required to demonstrate behavior change in the presence of variability (Hartmann & Hall, 1976).
When using a changing criterion design, behavior analysts must guard against imposing artificial ceilings (or floors) on the levels of responding that are possible in each phase. An obvious mistake of this sort would be to give a student only five math problems to complete when the criterion for reinforcement is five. Although the student could complete fewer than five problems, the possibility of exceeding the criterion has been eliminated, resulting perhaps in an impressive-looking graph, but one that is badly affected by poor experimental procedure.
Number of Criterion Changes
In general, the more times the target behavior changes to meet new criteria, the more convincing the demonstration of experimental control is. For example, eight criterion changes, one of which was a reversal to a previous level, were implemented in the changing design illustrated in Figure 10, and Allen and Evans (2001) conducted seven criterion changes (Figure 11). In both of these cases, a sufficient number of criterion changes occurred to demonstrate experimental control. The experimenter cannot, however, simply add any desired number of criterion changes to the design. The number of criterion changes that are possible within a changing criterion design is interrelated with the length of phases and the magnitude of criterion changes. Longer phases mean that the time necessary to complete the analysis increases; with a limited time to complete the study, the greater the number of phases, the shorter each phase can be.
Considering the Appropriateness of the Changing Criterion Design
The changing criterion design is a useful addition to the behavior analyst’s set of tactics for evaluating systematic behavior change. Like the multiple baseline design, the changing criterion design does not require that improvement in behavior be reversed. However, partial reversals to earlier levels of performance enhance the design’s capability to demonstrate experimental control. Unlike the multiple baseline design, only one target behavior is required.
Several characteristics of the changing criterion design limit its effective range of applications. The design can be used only with target behaviors that are already in the subject’s repertoire and that lend themselves to stepwise modification. However, this is not as severe a limitation as it might seem. For example, students perform many academic skills to some degree, but not at a useful rate. Many of these skills (e.g., solving math problems, reading) are appropriate for analysis with a changing criterion design. Allowing students to progress as efficiently as possible while meeting the design requirements of changing criterion analysis can be especially difficult. Tawney and Gast (1984) noted that “the challenge of identifying criterion levels that will permit the demonstration of experimental control without impeding optimal learning rates” is problematic with all changing criterion designs (p. 298).
Although the changing criterion design is sometimes suggested as an experimental tactic for analyzing the effects of shaping programs, it is not appropriate for this purpose. In shaping, a new behavior that initially is not in the person’s repertoire is developed by reinforcing responses that meet a gradually changing criterion, called successive approximations, toward the terminal behavior. However, the changing response criteria employed in shaping are topographical in nature, requiring different forms of behavior at each new level. The multiple probe design (Horner & Baer, 1978), however, is an appropriate design for analyzing a shaping program because each new response criterion (successive approximation) represents a different response class whose frequency of occurrence is not wholly dependent on the frequency of behaviors meeting other criteria in the shaping program. Conversely, the changing criterion design is best suited for evaluating the effects of instructional techniques on stepwise changes in the rate, frequency, accuracy, duration, or latency of a single target behavior.
 
Summary
Multiple Baseline Design
1.	In a multiple baseline design, simultaneous baseline measurement is begun on two or more behaviors. After stable baseline responding has been achieved, the independent variable is applied to one of the behaviors while baseline conditions remain in effect for the other behavior(s). After maximum change has been noted in the first behavior, the independent variable is then applied in sequential fashion to the other behaviors in the design.
2.	Experimental control is demonstrated in a multiple baseline design by each behavior changing when, and only when, the independent variable is applied.
3.	The multiple baseline design takes three basic forms: (a) a multiple baseline across behaviors design consisting of two or more different behaviors of the same subject; (b) a multiple baseline across settings design consisting of the same behavior of the same subject in two or more different settings; and (c) a multiple baseline across subjects design consisting of the same behavior of two or more different participants.
Variations of the Multiple Baseline Design
4.	The multiple probe design is effective for evaluating the effects of instruction on skill sequences in which it is highly unlikely that the subject’s performance on later steps in the sequence can improve without instruction or mastery of the earlier steps in the chain. The multiple probe design is also appropriate for situations in which prolonged baseline measurement may prove reactive, impractical, or too costly.
5.	In a multiple probe design, intermittent measurements, or probes, are taken on all of the behaviors in the design at the outset of the experiment. Thereafter, probes are taken each time the subject has achieved mastery of one of the behaviors or skills in the sequence. Just prior to instruction on each behavior, a series of true baseline measures are taken until stability is achieved.
6.	The delayed multiple baseline design provides an analytic tactic in situations in which (a) a planned reversal design is no longer desirable or possible; (b) limited resources preclude a full-scale multiple baseline design; or (c) a new behavior, setting, or subject appropriate for a multiple baseline analysis becomes available.
7.	In a delayed multiple baseline design, baseline measurement of subsequent behaviors is begun sometime after baseline measurement was begun on earlier behaviors in the design. Only baselines begun while earlier behaviors in the design are still under baseline conditions can be used to verify predictions made for the earlier behaviors.
8.	Limitations of the delayed multiple baseline design include (a) having to wait too long to modify certain behaviors, (b) a tendency for baseline phases to contain too few data points, and (c) the fact that baselines begun after the independent variable has been applied to earlier behaviors in the design can mask the interdependence (covariation) of behaviors.
Assumptions and Guidelines for Using Multiple Baseline Designs
9.	Behaviors comprising multiple baseline designs should be functionally independent of one another (i.e., they do not covary) and should share a reasonable likelihood that each will change when the independent variable is applied to it.
10.	Behaviors selected for a multiple baseline design must be measured concurrently and must have an equal opportunity of being influenced by the same set of relevant variables.
11.	In a multiple baseline design, the independent variable should not be applied to the next behavior until the previous behavior has changed maximally and a sufficient period of time has elapsed to detect any effects on behaviors still in baseline conditions.
12.	The length of the baseline phases for the different behaviors comprising a multiple baseline design should vary significantly.
13.	All other things being equal, the independent variable should be applied first to the behavior showing the most stable level of baseline responding.
14.	Conducting a reversal phase in one or more tiers of a multiple baseline design can strengthen the demonstration of a functional relation.
Considering the Appropriateness of Multiple Baseline Designs
15.	Advantages of the multiple baseline design include the fact that (a) it does not require withdrawing a seemingly effective treatment, (b) sequential implementation of the independent variable parallels the practice of many teachers and clinicians whose task is to change multiple behaviors in different settings and/or subjects, (c) the concurrent measurement of multiple behaviors allows direct monitoring of generalization of behavior change, and (d) the design is relatively easy to conceptualize and implement.
16.	Limitations of the multiple baseline design include the fact that (a) if two or more behaviors in the design covary, the multiple baseline design may not demonstrate a functional relation even though one exists; (b) because verification must be inferred from the lack of change in other behaviors, the multiple baseline design is inherently weaker than the reversal design in showing experimental control between the independent variable and a given behavior; (c) the multiple baseline design is more an evaluation of the independent variable’s general effectiveness than an analysis of the behaviors involved in the design; and (d) conducting a multiple baseline design experiment requires considerable time and resources.
Changing Criterion Design
17.	The changing criterion design can be used to evaluate the effects of a treatment on the gradual or stepwise improvement of a behavior already in the subject’s repertoire.
18.	After stable baseline responding has been achieved, the first treatment phase is begun, in which reinforcement (or punishment) is usually contingent on the subject’s performing at a specified level (criterion). The design entails a series of treatment phases, each requiring an improved level of performance over the previous phase. Experimental control is demonstrated in the changing criterion design when the subject’s behavior closely conforms to the gradually changing criteria.
19.	Three features combine to determine the potential of a changing criterion design to demonstrate experimental control: (a) the length of phases, (b) the magnitude of criterion changes, and (c) the number of criterion changes. The believability of the changing criterion design is enhanced if a previous criterion is reinstated and the subject’s behavior reverses to the level previously observed under that criterion.
Considering the Appropriateness of the Changing Criterion Design
20.	The primary advantages of the changing criterion design are that (a) it does not require a withdrawal or reversal of a seemingly effective treatment, and (b) it enables an experimental analysis within the context of a gradually improving behavior, thus complementing the practice of many teachers.
21.	Limitations of the changing criterion design are that the target behavior must already be in the subject’s repertoire, and that incorporating the necessary features of the design may impede optimal learning rates

Chapter 10 Planning and Evaluating Applied Behavior Analysis Research 

component analysis direct replication double-blind control placebo control Key Terms procedural fidelity replication systematic replication treatment drift treatment integrity Type I error Type II error Behavior Analyst Certification Board® BCBA® & BCABA® Behavior Analyst Task List © , Third Edition Content Area 1: Ethical Considerations 1-12 Give preference to assessment and intervention methods that have been scientifically validated, and use scientific methods to evaluate those that have not yet been scientifically validated. Content Area 5: Experimental Evaluation of Interventions 5-1 5-3 Systematically manipulate independent variables to analyze their effects on treatment. 10-3 Conduct a component analysis (i.e., determining effective component(s) of an intervention package). Content Area 10: Systems Support Design and use systems for monitoring procedural integrity. © 2006 The Behavior Analyst Certification Board, Inc., ® (BACB ® ) all rights reserved. A current version of this document may be found at www.bacb.com. Requests to reprint, copy, or distribute this document and questions about this document must be submitted directly to the BACB.
Cooper, John, et al. Applied Behavior Analysis: Pearson New International Edition PDF EBook, Pearson Education, Limited, 2013. ProQuest Ebook Central, 

Previously outlined were considerations and procedures for selecting target behaviors, detailed strategies for designing measurement systems, presented guidelines for displaying and interpreting behavioral data, and described experimental tactics for revealing whether observed changes in a target behavior can be attributed to an intervention. This chapter supplements the information described thus far by examining questions and considerations that should be addressed when designing, replicating, and evaluating behavioral research. We begin by reviewing the central role of the individual subject in behavioral research, and follow with a discussion of the value of flexibility in experimental design. Importance of the Individual Subject in Behavioral Research To achieve maximum effectiveness, the research methods of any science must respect the defining characteristics of that science’s subject matter. Behavior analysis— a science devoted to discovering and understanding the controlling variables of behavior —defines its subject matter as the activity of living organisms, a dynamic phenomenon that occurs at the level of the individual organism. It follows that the research methods most often used by behavior analysts feature repeated measures of behavior of individual organisms (the only place, by definition, where behavior can be found). This focus on the behavior of individual subjects has enabled applied behavior analysts to discover and refine effective interventions for a wide range of socially significant behavior. To further explain the importance that this focus on the individual subject or client holds for applied behavior analysis, we will now contrast it with a research model that revolves around comparisons of data representing the aggregate measures of different groups of subjects. This groups-comparison approach to designing and evaluating experiments has predominated “behavioral research” in psychology, education, and other social sciences for decades. Brief Outline of a Groups-Comparison Experiment The basic format for a groups-comparison experiment can be described as follows. 1 A pool of subjects (e.g., 60 first-grade nonreaders) is selected randomly from the pop1 ulation (e.g., all first-grade nonreaders in a school district) relevant to the research question (e.g., Will the XYZ intensive phonics program improve first-grade nonreaders’ ability to decode unpredictable text?). The subjects are divided randomly into two groups: the experimental group and the control group. An initial measure (pretest) of the dependent variable (e.g., score on a test of decoding skills) is obtained for all subjects in the study, the individual pretest scores for the subjects in each group are combined, and the mean and standard deviation are calculated for each group’s performance on the pretest. Subjects in the experimental group are then exposed to the independent variable (e.g., 6 weeks of the XYZ program), which is not provided to subjects in the control group. After the treatment program has been completed, a posttest measure of the dependent variable is obtained for all subjects, and the mean and standard deviation posttest scores for each group are computed. 2 The researcher then compares any changes in each group’s scores from pretest to posttest, applying various statistical tests to the data that enable inferences regarding the likelihood that any differences between the two groups’ performances can be attributed to the independent variable. For example, assuming that the mean pretest scores for the experimental and control groups were similar, and the posttest measure revealed an improved mean score for the experimental group but not for the control group, statistical analyses would indicate the mathematical probability that the difference was due to chance. When a statistical test rules out chance as a likely factor, the researcher infers that that independent variable was responsible for effects on the dependent variable (e.g., the experimental group’s improvement from pretest to posttest). Researchers who combine and compare measures of groups of subjects in this way do so for two primary reasons. First, advocates of group designs assume that averaging the measures of many subjects’ performance controls for intersubject variability; thus, they assume that any changes in performance are the work of the independent variable. The second rationale for using large groups of subjects is the assumption that increasing the number of subjects in a study increases the external validity of the findings. That is, a treatment variable found effective with the subjects in the experimental group will also be effective with other subjects in the population from which the sample subjects were selected. The assumption of increased generality of findings is discussed later in this chapter in the section on replication. In the next section we comment on


the first reason for the use of groups of subjects— that doing so controls for intersubject variability. Our discussion identifies three fundamental concerns with typical groups-comparison designs that bear heavily on experimental reasoning. 3 Group Data May Not Represent the Performance of Individual Subjects By definition, applied behavior analysis is concerned with improving the behavior of the individual. Knowing that the average performance of a group of subjects changed may not reveal anything about the performance of individual subjects. It is quite possible for the average performance of subjects in the experimental group to have improved, while the performance of some subjects stayed the same, and the performance of others even deteriorated. It is even possible for the majority of subjects to show no improvement, for some subjects to get worse, and for a few subjects to improve sufficiently to yield an overall average improvement of statistical significance. In defense of the groups-comparison approach, it might be said that it can show that a treatment is generally effective, that no treatment works with everyone, that people respond differently, and so on. But the fact that a group’s average performance improves with a treatment is insufficient reason to adopt it, particularly for people in dire need of help with academic, social, or other behavioral challenges. General effectiveness is insufficient; the factors responsible for one subject’s improvement with the treatment and another’s lack of improvement must be discovered. To be most useful, a treatment must be understood at the level at which people come into contact with it and are affected by it: the individual level. The two graphs in Figure 10.1 suggest some of the many faulty conclusions that are possible when an in3 vestigator’s interpretation of a study is based on group mean scores. Each graph presents hypothetical data for the individual and average performances of two groups, each group consisting of two subjects. The data show no change in the mean response measure from pretest to posttest for either group. The preand posttest group data in both graphs in Figure1 would suggest that the independent variable had no effect on the subjects’ behavior. However, the left-hand graph in Figure 1 shows that Subject A’s performance improved from pretest to posttest, and Subject B’s behavior deteriorated over the same period of time. 4 The right-hand graph shows that although the preand posttest measures for Subjects C and D were identical, if repeated measures of Subject C’s and Subject D’s behavior between the pretest and posttest had been conducted, significant variability within and between the two subjects would have been revealed. Group Data Masks Variability in the Data A second problem associated with the mean performance of a group of subjects is that it hides variability in the data. Even if repeated measures of Subject C’s and Subject D’s behavior between the preand posttest had been conducted as shown in Figure 1, a researcher who relied on the group’s mean performance as the primary indicator of behavior change would be ignorant of the variability that occurred within and between subjects. When repeated measurement reveals significant levels of variability, an experimental search with the goal of identifying and controlling the factors responsible for the variability is in order. The widespread belief that the effects of uncontrolled variables in a study can be somehow controlled by statistical manipulations of the dependent variable is faulty.

Statistical control is never a substitute for experimental control. . . . The only way to determine whether or not uncontrolled variables are influencing the data is to inspect the data at the finest available level of decomposition, usually point-by-point for each individual subject. No purpose is served by combining the data statistically to obscure such effects. (Johnston & Pennypacker, 1980, p. 371) Instead of controlling its sources before the fact, the between groups approach emphasizes controlling variability statistically after the fact. These two tactics do not have the same effects on the database. Whereas efforts to control actual variability lead to improved control over responding and, thus, a clearer picture of the effects of each condition, statistical manipulation of variable data cannot remove the influences already represented in the data. (Johnston & Pennypacker, 1993b, p. 184) Attempting to “cancel out” variability through statistical manipulation neither eliminates it from the data nor controls the variables responsible for it. And the researcher who attributes the effects of unknown or uncontrolled variables to chance removes himself or herself even further from the identification and analysis of important variables. In his monumental work, Tactics of Scientific Research, Sidman (1960) dealt repeatedly and forcefully with this critical issue. To some experimenters, chance is simply a name for the combined effects of uncontrolled variables. If such variables are, in fact, controllable, then chance in this sense is simply an excuse for sloppy experimentation, and no further comment is required. If the uncontrolled variables are actually unknown, then chance is, as Boring (1941) has pointed out, a synonym for ignorance. . . . One of the most discouraging and at the same time challenging aspects of behavioral science is the sensitivity of behavior to a tremendous array of variables. . . . But variables are not canceled statistically. They are simply buried so that their effects cannot be seen. The rationale for statistical immobilization of unwanted variables is based on the assumed random nature of such variables. . . . Not only is the assumption of randomness with respect to the uncontrolled variables an untested one but it is also highly improbable. There are few, if any, random phenomena in the behavioral world. (pp. 45, 162– 163) Sidman (1960) also commented on an experimenter’s use of statistics in an attempt to deal with troublesome sequence effects. He has a neat trick up his sleeve. By averaging together the data for both subjects under Condition A, and again under Condition B, he “cancels out” the order effect, and completely bypasses the problem of irreversibility. By a simple arithmetical operation, two subjects have become one, and a variable has been eliminated. It has not, in fact, gone anywhere. Numbers may be made to disappear by adding and subtracting them from each other. Five apples minus three apples are two apples. The numbers are easily changed by a few strokes of the pen, but some eating has to be done before the apples themselves will vanish. (p. 250) The “eating” that must be done to control the effects of any variable can be accomplished in only two ways: (a) holding the variable constant throughout the experiment, or (b) isolating the suspected factor as an independent variable and manipulating its presence, absence, and/or value during the experiment. Intrasubject Replication Is Absent from Group Designs A third weakness of the groups-comparison statistical inference research model is that the power of replicating effects with individual subjects is lost. One of the great strengths of within-subject experimental designs is the convincing demonstration of a functional relation made possible by replication within the design itself. Even though multiple subjects are typically involved in applied behavior analysis research, each subject is always treated as a separate experiment. Although behavior analysts often display and describe the data for all subjects as a group, data from individual subjects are used as the basis for determining and interpreting experimental effects. Applied behavior analysts are wise to heed Johnston and Pennypacker’s (1980) admonition, “An effect that emerges only after individual data have been combined is probably artifactual and not representative of any real behavioral processes” (p. 257). This discussion should not be interpreted to mean that the overall performance of groups of subjects cannot, or should not, be studied with the strategies and tactics of applied behavior analysis. There are many applied situations in which the overall performance of a group is socially significant. For example, Brothers, Krantz, and McClannahan (1994) evaluated an intervention to increase the number of pounds of recyclable office paper recycled by 25 staff members at a school. Still, it is important to remember that group data may not represent the performance of individual participants, and vice versa. For example, Lloyd, Eberhardt, and Drake (1996) compared the effects of group versus individual reinforcement contingencies within the context of collaborative group study conditions on quiz scores by students in a Spanish language class. The results showed that the group contingencies resulted in higher mean quiz scores for the class as a whole compared to the individual contingencies condition. However, overall benefits at the class level were mitigated by differential results for individual students. When group results do not represent individual performances, researchers should supplement group data with individual results, ideally in the form of  graphic displays (e.g., Lloyd et al., 1996; Ryan & Hemmes, 2005).
In some instances, however, the behavior analyst may not be able to control the access of subjects to the experimental setting and contingencies or even be able to identify who the subjects are (e.g.,Van Houten & Malenfant, 2004; Watson, 1996). The dependent variable must then consist of all of the responses made by individuals who enter the experimental setting. This approach is used frequently in community-based behavior analysis research. For example, group data have been collected and analyzed on such dependent variables as litter control on a university campus (Bacon-Prue, Blount, Pickering, & Drabman, 1980), car pooling by university students (Jacobs, Fairbanks, Poche, & Bailey, 1982), drivers’ compliance and caution at stop signs (Van Houten & Malenfant, 2004), the use of child safety belts in shopping carts (Barker, Bailey, & Lee, 2004), and reducing graffiti on restroom walls (Watson, 1996).
Importance of Flexibility in Experimental Design
On one level, an effective experimental design is any arrangement of type and sequence of independent variable manipulations that produces data that are interesting and convincing to the researcher and the audience. In this context the word design is particularly appropriate as a verb as well as a noun; the effective behavioral researcher must actively design each experiment so that each achieves its own unique design. There are no ready-made experimental designs awaiting selection. The prototype designs are examples of analytic tactics that afford a form of experimental reasoning and control that has proven effective in advancing our understanding of a wide range of phenomena of interest to applied behavior analysts. Johnston and Pennypacker (1980, 1993a) have been clear and consistent in stating that the “suspicion some may hold that generic categories of design types exist and should be botanized” (1980, p. 293) is counterproductive to the practice of the science of behavior.
In order to explain how to design and interpret within subject comparisons, it is tempting to develop categories of similar arrangements or designs. This almost requires giving each category a label, and the labeled categories then imply that there is something importantly different that distinguishes each from the others.
(1993a, p. 267)
The requirements for creating useful comparisons cannot be reduced to a cookbook of simple rules or formulas. . . . It misleads students by suggesting that particular types of arrangements have specific functions and by failing to encourage them to understand the underlying considerations that open up unlimited experimental options. (1993a, p. 285)
Sidman (1960) was even more adamant in his warning regarding the undesirable effects of researchers’ believing in the existence of a given set of rules for experimental design.
The examples may be accepted as constituting a set of rules that must be followed in the design of experiments. I cannot emphasize too strongly that this would be disastrous. I could make the trite statement that every rule has its exception, but this is not strong enough. Nor is the more relaxed statement that the rules of experimental design are flexible, to be employed only where appropriate. The fact is that there are no rules of experimental design. (p. 214)
We agree with Sidman. The student of applied behavior analysis should not be led to believe that any of the analytic tactics constitute experimental designs per se. Still, we believe that it is useful to present the most commonly used analytic tactics in design form for two reasons. First, the vast majority of studies that have advanced the field of applied behavior analysis have used experimental designs that incorporated one or more of the analytic tactics. Second, we believe that the beginning student of behavior analysis benefits from an examination of specific examples of isolated experimental tactics and their application; it is one step in learning the assumptions and strategic principles that guide the selection and arrangement of analytic tactics into an experimental design that effectively and convincingly addresses the research question(s) at hand.
Experimental Designs That Combine Analytic Tactics
Combining multiple baseline and reversal tactics may allow a more convincing demonstration of experimental control than either tactic alone. For example, by withdrawing the treatment variable (a return to baseline) and then reapplying it within one or more tiers in a multiple baseline design, researchers are able to determine the existence of a functional relation between the independent variable and each behavior, setting, or subject of the multiple baseline element and also to analyze the effectiveness of the independent variable across the tiers (e.g.,Alexander, 1985; Ahearn, 2003; Barker, Bailey, & Lee, 2004; Blew, Schwartz, & Luce, 1985; Bowers, Woods, Carlyon, & Friman, 2000; Heward & Eachus, 1979; Miller & Kelley, 1994; Zhou, Goff, & Iwata, 2000).
To investigate the research questions of interest, investigators often build experimental designs that entail a combination of analytic tactics. For example, it is not uncommon for experimenters to evaluate multiple treatments by sequentially applying each in a multiple baseline fashion (e.g., Bay-Hinitz, Peterson, & Quilitch, 1994; Iwata, Pace, Cowdery, & Miltenberger, 1994; Van Houten, Malenfant, & Rolider, 1985; Wahler & Fox, 1980; Yeaton & Bailey, 1983). Experimental designs that combine multiple baseline, reversal, and/or alternating treatments tactics can also provide the basis for comparing the effects of two or more independent variables or conducting a component analysis of elements of a treatment package. For example, the experimental designs used by L. J. Cooper and colleagues’ (1995) used alternating treatments comparisons within a sequence of multiple treatment reversals to identify the active variables in treatment packages for children with feeding disorders.
Haring and Kennedy (1990) used multiple baseline across settings and reversal tactics in their experimental design that compared the effectiveness of time-out and differential reinforcement of other behavior (DRO) on the frequency of problem behaviors by two secondary students with severe disabilities (see Figure 2).  Sandra and Raff each frequently engaged in repetitive, stereotypic problem behaviors (e.g., body rocking, loud vocalizations, hand flapping, spitting) that interfered with classroom and community activities. In addition to assessing the effects of the time-out and DRO interventions against a no-treatment baseline condition, the design also enabled the researchers to conduct two comparisons of the relative effects of each treatment during an instructional task and leisure context. The design enabled Haring and Kennedy to discover that the time-out and DRO interventions produced different outcomes depending on the activity context in which they were applied. For both students, DRO was more effective than time-out in suppressing problem behavior in the task context; the opposite results were obtained in the leisure context, where time-out suppressed problem behavior and DRO proved ineffective.
Experimenters have also incorporated alternating treatments into experimental designs containing multiple baseline elements. For example, Ahearn, Kerwin, Eicher, Shantz, and Swearingin (1996) evaluated the relative effects of two treatments for food refusal in an alternating treatments design implemented in a multiple baseline across subjects format. Likewise, McGee, Krantz, and McClannahan (1985) evaluated the effects of several procedures for teaching language to autistic children with an experimental design that incorporated alternating treatments within a multiple baseline across behaviors component that was, in turn, nested within an overall multiple baseline across subjects format. Zanolli and Daggett (1998) investigated the effects of reinforcement rate on the spontaneous social initiations of socially withdrawn preschoolers with an experimental design consisting of multiple baseline, alternating treatments, and reversal tactics.
Figure 3 shows how Sisson and Barrett (1984) incorporated a multiple probe across behaviors component, an alternating treatments analysis, and a multiple baseline across behaviors element in a design comparing the effects of two language-training procedures. The design enabled the investigators to discover the superiority of the total communication method for these two children, as well as the fact that direct application of the treatment was required for learning to occur on specific sentences. Results for a third subject revealed a functional relation of the same form and direction as that found for the two children whose results are shown in Figure 3, but one not so strongly in favor of the total communication procedure.
Our intent in describing several experiments that combined analytic tactics is not to offer any of these examples as model designs. They are presented instead as illustrations of the infinite number of experimental designs that are possible by arranging different combinations and sequences of independent variable manipulations. In every instance the most effective (i.e., convincing) experimental designs are those that use an ongoing evaluation of data from individual subjects as the basis for employing the three elements of baseline logic—prediction, verification, and replication.
Internal Validity:
Controlling Potential Sources of Confounding in Experimental Design
An experiment is interesting and convincing, and yields the most useful information for application, when it provides an unambiguous demonstration that the independent variable was solely responsible for the observed behavior change. Experiments that demonstrate a clear functional relation are said to have a high degree of internal validity. The strength of an experimental design is
Figure 2	Experimental design employing multiple baselines across settings and reversal tactics counterbalanced across two subjects to analyze the effects of time-out (TO) and differential reinforcement of other behavior (DRO) treatment conditions.
From “Contextual Control of Problem Behavior” by T. G. Haring and C. H. Kennedy, 1990, Journal of Applied Behavior Analysis, 23,pp. 239–240. Copyright 1990 by the Society for the Experimental Analysis of Behavior, Inc. Reprinted by permission.
	10	20
Sessions
determined by the extent to which it (a) demonstrates a reliable effect (i.e., repeated manipulation of the independent variable produces a consistent pattern of behavior change) and (b) eliminates or reduces the possibility that factors other than the independent variable produced the behavior change (i.e., controls for confounding variables).
Implicit in the term experimental control, which is often used to signify a researcher’s ability to reliably produce a specified behavior change by manipulating an independent variable, is the idea that the researcher controls the subject’s behavior. However, “control of behavior” is inaccurate because the experimenter can control only some aspect of the subject’s environment. Therefore, the level of experimental control obtained by a researcher refers to the extent to which she controls all relevant variables in a given experiment. The researcher exerts this control within the context of an experimental design that, even though carefully planned at the outset, takes its ultimate form from the researcher’s ongoing examination and response to the data.
Figure 3	Experimental design employing	Baseline	Alternating Treatments	Total Communication an alternating treatments tactic, a multiple probe, and a multiple baseline across behaviors analysis.
From “Alternating Treatments Comparison of Oral and Total
Communication Training with Minimally Verbal Retarded
Children” by L. A. Sisson and R. P. Barrett, 1984, Journal of Applied Behavior Analysis, 17,p. 562. Copyright 1984 by the Society for the Experimental Analysis of Behavior, Inc.
Reprinted by permission.
 
An effective experimental design simultaneously reveals a reliable functional relation between independent and dependent variables (if one exists) and minimizes the likelihood that the observed behavior changes are the result of unknown or uncontrolled variables. An experiment has high internal validity when changes in the dependent variable are demonstrated to be a function only of the independent variable. When planning an experiment and later when examining the actual data from an ongoing study, the investigator must always be on the lookout for threats to internal validity. Uncontrolled factors known or suspected to have exerted influence on the dependent variable are called confounding variables. Much of a researcher’s efforts during the course of a study are aimed at eliminating or controlling confounding variables.
The attainment of steady state responding is the primary means by which applied behavior analysts assess the degree of experimental control. Separating the effects of the independent variable from the effects of a potentially confounding variable requires clear, empirical evidence that the potentially confounding variable is no longer present, has been held constant across experimental conditions, or has been isolated for manipulation as an independent variable. Any experiment can be af-
Sessions
fected by a virtually unlimited number of potential confounds; and as with every aspect of experimental design, there are no set rules for identifying and controlling confounding variables to which researchers can turn. However, some common and likely sources of confounding can be identified as well as tactics that can be considered to control them. Confounding variables can be viewed as related primarily to one of four elements of an experiment: subject(s), setting, measurement of the dependent variable, and independent variable.
Subject Confounds
A variety of subject variables can confound the results of a study. Maturation, which refers to changes that take place in a subject over the course of an experiment, is a potential confounding variable. For example, a subject’s improved performance during the later phases of a study may be the result of physical growth or the acquisition of academic, social, or other behaviors and be unrelated to manipulations of the independent variable. Experimental designs that incorporate rapidly changing conditions or multiple introductions and withdrawals of the independent variable over time usually control for maturation effectively.
In most applied behavior analysis research, a subject is in the experimental setting and contacting the contingencies implemented by the investigator for only a portion of the day. As it is in any study, the assumption is made that each subject’s behavior during each session will be primarily a function of the experimental conditions in effect. In reality, however, each subject’s behavior may also be influenced by events that have occurred outside of the experiment. For example, suppose that the frequency of contributions to a class discussion is the dependent variable in a study. Now suppose that just prior to a session a student who has been contributing to discussions at a high rate was involved in a fight in the lunchroom and emits substantially fewer contributions compared to his level of responding in previous sessions. This change in the student’s behavior may, or may not, be a result of the lunchroom fight. If the lunchroom fight coincided with a change in the independent variable, it would be especially difficult to detect or separate any effects of the experimental conditions from those of the extra-experimental event.
Although the researcher may be aware of some events that are likely causes of variability during a study, many other potential confounds go undetected. Repeated measurement is both the control for and the means to detect the presence and effects of such variables. The uncontrolled variables responsible for a subject’s having a “bad day” or an unusually “good day” are particularly troublesome in research designs with few and/or widely spaced measurements of the dependent variable. This is one of the major weaknesses of using pretest-posttest comparisons to evaluate the effects of a treatment program.
Because groups-comparison experiments are predicated on subjects’ similarity in relevant characteristics (e.g., gender, age, ethnicity, cultural and linguistic background, current skills), they are vulnerable to confounding by differences among subjects. Concern that the characteristics of one or more subjects may confound an experiment’s results is generally not an issue in the singlesubject experiments of applied behavior analysis. First, a person should participate in a study because she will benefit if the target behavior is changed successfully. Second, a subject’s idiosyncratic characteristics cannot confound a study using a true within-single experimental design. With the exception of a multiple baseline across subjects analysis, each participant in a behavioral study serves as her own control, which guarantees identically matched subjects in all experimental conditions because those subjects are the same person. Third, the external validity of results from a single-subject analysis is not dependent on the extent to which the subject(s) shares certain characteristics with others. The extent to which a functional relation applies to other subjects is established by replicating the experiment with different subjects.
Setting Confounds
Most applied behavior analysts conduct studies in natural settings where a host of variables are beyond their control. Studies in natural settings are more prone to confounding by uncontrolled events than are studies conducted in laboratories where extraneous variables can be more tightly managed. Even so, the applied experimenter is not without resources to mitigate the detrimental effects of setting confounds. For instance, when the applied researcher observes that an uncontrolled event has coincided with changes in the data, he should hold all possible aspects of the experiment constant until repeated measurement again reveals stable responding. If the unplanned event appears to have a robust effect on the target behavior, or is otherwise of interest to the investigator, and is amenable to experimental manipulation, the investigator should treat it as an independent variable and explore its possible effects experimentally.
Applied researchers concerned about setting confounds must also be on the lookout for the availability of “bootleg” reinforcement within and outside the experimental situation. A good example of how a setting confound operates occurs when, unbeknownst to the experimenter, subjects have ready access to potential reinforcers. In such a case, the effectiveness of those consequences as reinforcers diminishes.
Measurement Confounds
Many factors should be considered in designing an accurate and nonreactive measurement system. Still, numerous sources of confounding may exist within a well-planned measurement system. For instance, data might be confounded by observer drift, the influence of the experimenter’s behavior on observers, and/or observer bias. Although admittedly difficult to accomplish in applied settings where observers often see the independent variable being implemented, keeping observers naive to the conditions and expected outcomes of an experiment reduces the potential of confounding by observer bias. On a related note, when observers score permanent products, the products should not contain identifying marks that indicate who produced each product and under what experimental conditions it was produced. Having observers score papers from baseline and treatment conditions in randomized order reduces the likelihood that observer drift or bias will confound the data within one treatment condition phase. (This procedure is more suitable to controlling for drift or bias by observers conducting postexperiment accuracy or IOA assessments.)
Unless a completely unobtrusive measurement system is devised (e.g., a covert system using one-way mirrors, or observations conducted at some distance from the subject), reactivity to the measurement procedure must always be considered as a possible confound. To offset this possible confound, the experimenter must maintain baseline conditions long enough for any reactive effects to run their course and for stable responding to be obtained. If reactivity to measurement produces undesirable effects (e.g., aggressive behavior, cessation of productivity) and a more unobtrusive measurement procedure cannot be devised, intermittent probes should be considered. Measures can also be confounded by practice, adaptation, and warm-up effects, especially during the initial stages of baseline. Again, the proper procedure is to continue baseline conditions until stable responding is obtained or variability is reduced to minimal levels. Intermittent probes should not be used for baseline measurement of behaviors for which practice effects would be expected. This is because, if the target behavior is susceptible to practice effects, those effects will occur during the intervention condition when more frequent measures are conducted, thereby confounding any effects of the independent variable.
Independent Variable Confounds
Most independent variables are multifaceted; that is, there is usually more to a treatment condition than the specific variable of interest to the investigator. For example, the effects of a token economy on students’ academic productivity may be confounded by variables such as the personal relationship between the students and the teacher who delivers the tokens, social interactions associated with delivering and exchanging tokens, the expectation of teacher and students that performance will improve when the token system is implemented, and so on. If the intent is to analyze the effects of token reinforcement per se, these potentially confounding variables must be controlled.
Schwarz and Hawkins (1970) provided a good example of a control procedure for ruling out an aspect associated with a treatment as responsible for behavior change. The researchers evaluated the effects of token reinforcement on three maladaptive behaviors of an elementary student who was described as severely withdrawn. During treatment, the therapist and the girl met each day after school and viewed a videotape that had been made earlier that day of the student’s classroom behavior. The therapist also administered tokens contingent on the girl’s videotaped behavior displaying progressively fewer occurrences of maladaptive behaviors.
Schwarz and Hawkins recognized an independent confound potentially lurking in the design of their study. They reasoned that if improvement occurred as a function of the treatment, the question would remain whether the student’s behavior had improved because the therapist-delivered positive attention and rewards improved her self-concept, which in turn changed her maladaptive behaviors in the classroom, which were symptomatic of her poor self-concept. In that case, Schwarz and Hawkins could not be certain that the contingent tokens played an important role in changing the behavior. Schwarz and Hawkins, anticipating this possible confound, controlled for it in a simple and direct way. Following baseline, they implemented a condition in which the therapist met with the girl each day after school and provided her with social attention and token reinforcement contingent on improvements in handwriting. During this control phase, the three target behaviors—face touching, slouching, and low voice volume—showed no change, thereby increasing their confidence in a conclusion that the girl’s ultimate behavioral improvements during the subsequent intervention phases were due to the token reinforcement.
When medical researchers design experiments to test the effects of a drug, they use a technique called a placebo control to separate effects that may be produced by a subject’s perceived expectations of improvement because of taking the drug apart from the effects actually produced by the drug. In the typical groups-comparison design, the subjects in the experimental group receive the real drug, and subjects in the control group receive placebo pills. Placebo pills contain an inert substance, but they look, feel, and taste exactly like the pills containing the real drug being tested.
Applied behavior analysts have also employed placebo controls in single-subject experiments. For example, in their study evaluating a pharmacological treatment of the impulsivity by students with attentiondeficit/hyperactivity disorder (ADHD), Neef, Bicard, Endo, Coury, and Aman (2005) had a pharmacist prepare placebos and medications in identical gelatin capsules in 1-week supplies for each child. Neither the students nor the observers knew if a child had taken the medication or had taken the placebos. When neither the subject(s) nor the observers know whether the independent variable is present or absent from session to session, this type of control procedure is called a double-blind control. A double-blind control procedure eliminates confounding by subject expectations, parent and teacher expectations, differential treatment by others, and observer bias.
Treatment Integrity
The results of many experiments have been confounded by the inconsistent application of the independent variable. The researcher must make a concerted effort to ensure that the independent variable is applied exactly as planned and that no other unplanned variables are administered inadvertently along with the planned treatment. The terms treatment integrity and procedural fidelity refer to the extent to which the independent variable is implemented or carried out as planned.
Low treatment integrity invites a major source of confounding into an experiment, making it difficult, if not impossible, to interpret the results with confidence. Data from an experiment in which the independent variable was administered improperly, applied inconsistently, conducted piecemeal, and/or delivered in overdose or underdose form often lead to conclusions that—depending on the results obtained—represent either a false positive (claiming a functional relation when no such relation exists) or a false negative (failing to detect a functional relation when one actually does exist). If a functional relation is apparent from the analysis of the data, one cannot be sure whether the treatment variable as described by the experimenter was responsible or whether the effects were a function of extraneous, uncontrolled elements of the intervention as it was actually applied. On the other hand, it may be equally erroneous to interpret the failure to produce significant behavior change as evidence that the independent variable is ineffective. In other words, had the independent variable been implemented as planned, it might have been effective.
Numerous threats to treatment integrity exist in applied settings (Billingsley, White, & Munson, 1980; Gresham, Gansle, & Noell, 1993; Peterson, Homer, & Wonderlich, 1982). Experimenter bias can cause the researcher to administer the independent variable in such a way that it enjoys an unfair advantage over the baseline or comparative conditions. Treatment drift occurs when the application of the independent variable during later phases of an experiment differs from the way it was applied at the outset of the study. Treatment drift can result from the complexity of the independent variable, which can make it difficult for practitioners to implement all of the elements consistently over the course of an experiment. Contingencies influencing the behavior of those responsible for implementing the independent variable can also result in treatment drift. For example, a teacher might implement only those aspects of a procedure that she favors and might implement the full intervention only when the experimenter is present.
Precise Operational Definition. Achieving a high level of treatment integrity begins with developing a complete and precise operational definition of the treatment procedures. Besides providing the basis for training the persons who will implement an intervention and judging the level of treatment integrity attained, operational definitions of treatment conditions are a requisite for meeting the technological dimension of applied behavior analysis (Baer et al., 1968). An investigator’s failure to provide explicit operational definitions of the treatment variable hampers the dissemination and proper use of the intervention by practitioners, and makes it difficult for other researchers to replicate and ultimately validate the findings.
Gresham and colleagues (1993) recommended that descriptions of the independent variable be judged by the same standards of explicitness that are used in assessing the quality of definitions of dependent variable. That is, they should be clear, concise, unambiguous, and objective. More specifically, Gresham and colleagues also suggested that treatments be operationally defined in each of four dimensions: verbal, physical, spatial, and temporal. They used Mace, Page, Ivancic, and O’Brien’s (1986) definition of a time-out procedure as an example of an operational definition of an independent variable.
(a) Immediately following the occurrence of a target behavior (temporal dimension), (b) the therapist said “No, go to time-out” (verbal dimension), (c) led the child by the arm to a prepositioned time-out chair (physical dimension), (d) seated the child facing the corner (spatial dimension). If the child’s buttocks were raised from the time-out chair or if the child’s head was turned more than 45° (spatial dimension), the therapist used the least amount of force necessary to guide compliance with the time-out procedure (physical dimension). (f) At the end of 2 min (temporal dimension), the therapist turned the time-out chair 45° from the corner
(physical and spatial dimensions) and walked away (physical dimension). (pp. 261–262)
Simplify, Standardize, and Automate. When planning an experiment, placing a high priority on simplifying and standardizing the independent variable and providing criterion-based training and practice for the people who will be responsible for implementing it enhances treatment integrity. Treatments that are simple, precise, and brief, and that require relatively little effort, are more likely to be delivered with consistency than those that are not. Simple, easy-to-implement techniques also have a higher probability of being accepted and used by practitioners than those that are not, and thus possess a certain degree of self-evident social validity. Simplicity is, of course, a relative concern, not a mandate; effecting change in some socially important behaviors may require the application of intense, complex interventions over a long period of time and may involve many people. Baer (1987) made this point succinctly when he stated:
Long problems are simply those in which the task analysis requires a series of many behavior changes, perhaps in many people, and although each of them is relatively easy and quick, the series of them requires not so much effort as time, and so it is not arduous but merely tedious. (pp. 336–337)
Practitioners need not be thwarted or dismayed by complex interventions; they simply need to realize the treatment integrity implications. All things being equal, however, a simple and brief treatment will probably be applied more accurately and consistently than will a complex and extended one.
To ensure the consistent implementation of the independent variable, experimenters should standardize as many of its aspects as cost and practicality allow. Standardization of treatment can be accomplished in a variety of ways. When a treatment requires a complex and/or extended sequence of behaviors, a script for the person administering it may improve the accuracy and consistency with which the independent variable is applied. For example, Heron, Heward, Cooke, and Hill (1983) used a scripted lesson plan with overhead transparencies to ensure that a classwide peer tutor training program was implemented consistently across groups of children.
If automating the intervention will not compromise it in any way, researchers might consider “canning” the independent variable so that an automated device can be used for its delivery. Although a videotaped tutor training presentation in Heron and colleagues’ (1983) study would have eliminated any potential confounding caused by the teacher’s slightly different presentations of the lesson from group to group and across sets of tutoring skills, using a canned presentation would also have eliminated the desired interactive and personal aspects of the training program. Some treatment variables are well suited to automated presentation in that automation neither limits the desirability of the treatment nor seriously reduces its social validity in terms of acceptability or practicability (e.g., use of videotaped programs to model residential energy conservation).
Training and Practice. Training and practice in properly implementing the independent variable provides the person(s) who will be responsible for conducting the treatment or experimental sessions with the necessary skills and knowledge to carry out the treatment. It would be a mistake for the researcher to assume that a person’s general competence and experience in the experimental setting (e.g., a classroom) guarantees correct and consistent application of an independent variable in that setting (e.g., implementing a peer-mediated tutoring program).
As stated earlier, scripts detailing treatment procedures and cue cards or other devices that remind and prompt people through steps of an intervention can be helpful. Researchers should not, however, assume that merely providing the intervention agent with a detailed script will ensure a high degree of treatment integrity. Mueller and colleagues (2003) found that a combination of verbal instructions, modeling, and/or rehearsal was required for parents to implement pediatric feeding protocols with a high level of treatment integrity. Performance feedback has also been shown to improve the integrity with which parents and practitioners implement behavior support plans and explicit teaching techniques (e.g., Codding, Feinberg, Dunn, & Pace, 2005; Sarakoff & Strumey, 2004; Witt, Noell, LaFleur, & Mortenson, 1997).
Assessing Treatment Integrity. Although simplification, standardization, and training help increase the degree of treatment integrity, they do not guarantee it. If there is any doubt about the correct and consistent application of the independent variable, investigators should provide data on the accuracy and reliability of the independent variable (Peterson et al., 1982; Wolery, 1994). Treatment integrity (or procedural fidelity) data reveal the extent to which the actual implementation of all of the experimental conditions over the course of a study matches their descriptions in the method section
of a research report. 
Even though the effective control of the presence and absence of the independent variable is a requisite to an internally valid experiment, applied behavior analysts have not always made sufficient efforts to assure the integrity of the independent variable. Two reviews of articles published in the Journal of Applied Behavior Analysis from 1968 to 1990 found that the majority of authors did not report data assessing the degree to which the independent variable was properly and consistently applied (Gresham et al., 1993; Peterson et al., 1982). Peterson and colleagues noted that a “curious double standard” had developed in applied behavior analysis in which data on the interobserver agreement of dependent variable measures were required for publication, but such data were seldom provided or required for the independent variable.
Peterson and colleagues (1982) suggested that the technology developed for assessing and increasing the accuracy and believability of measures of the dependent variable is fully applicable to the collection of procedural fidelity data. Importantly, observation and recording of the independent variable provides the experimenter with data indicating whether calibration of the treatment agent is necessary (i.e., bringing the intervention agent’s behavior into agreement with the true value of the independent variable). Observation and calibration give the researcher an ongoing ability to use retraining and practice to ensure a high level of treatment integrity over the course of an experiment.
Figure 4 shows the data collection form used by trained observers to collect treatment integrity data in a study evaluating the effects of different qualities and durations of reinforcement for problem behavior, compliance, and communication within a treatment package for escape-maintained problem behavior (Van Norman, 2005). The observers viewed videotapes of randomly selected sessions representing approximately one third to one half of all sessions in each condition and phase of the study. The percentage of treatment integrity for each condition was calculated by dividing the number of steps the experimenter completed correctly during a session by the total number of steps completed.
This overview of sources of potential confounding variables is, of necessity, incomplete. A complete inventory of all possible threats to the internal validity of experimental research would be well beyond the scope of this text. And presenting such a list might suggest that a researcher need only control for the variables listed and not worry about anything else. In truth, the list of potential confounds is unique to every experiment. The effective researcher is the one who questions and probes the influence of as many relevant variables as possible. No experimental design can control all potential confounds; the challenge is to reduce, eliminate, or identify the influence of as many potentially confounding variables as possible.
Social Validity:
Assessing the Applied Value of Behavior Changes and the Treatments That Accomplish Them
	Video Clip #	1-1AL	Rater Initials:	E. B.	Date:	7/6/05
Phase 1/A (SD)
Procedural Steps	Opportunities	Correct	% Correct	Yes	No		N/A
1. The instructor delivers a task prompt at the beginning of the session, e.g., "it's time to work" or similar.				 
N		N/A
2. If the participant does not respond, the instructor represents the choice by replacing materials or restating the contingencies.	 
 
 
Y	N		N/A
3. The break card (or similar) is presented simultaneously (within 3 seconds) with work task materials.	 
 
16/16	 
N		N/A
4. Following work choice (touching materials associated with work)
a.	Removes the task materials
b.	Presents a timer with green colored cue card
c.	Provides access to high preference items
d.	Engages in play with the participant for 1 minute	 
 
7/7	 
N		N/A
5. Following break requests
a.	Removes the task materials
b.	Presents timer with yellow cue card
c.	Provides access to moderately preferred tangible items and neutral     commenting for 30 seconds	 
 
8/8	 
N		N/A
6. Following problem behavior within 10 s the instructor
a.	Removes the task/play materials
b.	Presents a timer with red cue card
c.	Provides no attentiion or tangible items for 10 seconds				Y	N		N/A
7. Which side was the break card (or similar) presented (the participant's R = right or L = left) for each choice presentation.	R	R	L	L	L	R	L	R	R	L	R	L	R	L	L	R				
																				
	Figure 4	Example of a form used to record treatment integrity data.
Adapted from “The Effects of Functional Communication Training, Choice Making, and an Adjusting Work Schedule on Problem Behavior Maintained by Negative Reinforcement” by R. K. Van Norman, 2005, p. 204. Unpublished doctoral dissertation. Columbus, OH: The Ohio State University. Used by permission.
In his landmark article, “Social Validity: The Case for Subjective Measurement or How Applied Behavior Analysis Is Finding Its Heart,” Montrose Wolf (1978) proposed the then “radical concept that clients (including parents and guardians of dependent people, and even those whose taxes support social programs) must understand and admire the goals, outcomes, and methods of an intervention” (Risley, 2005, p. 284). Wolf recommended that the social validity of a study in applied behavior analysis
should be assessed in three ways: the social significance of the target behavior, the appropriateness of the procedures, and the social importance of the results.
Although social validity assessments may increase the likelihood that a study will be published and can be helpful in the marketing and public relations of behavioral programs (Hawkins, 1991; Winett, Moore, & Anderson, 1991), the ultimate purpose of social validity assessments is “to help choose and guide [behavior change] program developments and applications” (Baer & Schwartz, 1991, p. 231). Social validity assessments are most often accomplished by asking the direct consumers of a behavior change program (the learners, clients, research subjects) and/or a group of indirect consumers (e.g., family members, teachers, therapists, community people) questions about how satisfied they are with the relevance and importance of the goals of the program, the acceptability of the procedures, and the value of the behavior change outcomes achieved. 
Verbal statements by practitioners and consumers that they find a treatment or program acceptable and effective should not be viewed as either evidence that the program was effective, or if it was, that consumers will continue to use the methods. Noting Baer, Wolf, and Risley’s (1968) admonition that “a subject’s verbal description of his own nonverbal behavior usually would not be accepted as a measure of his actual behavior” (p. 93), Hawkins (1991) recommended that the term consumer satisfaction be used instead of social validity because it acknowledges that what is typically obtained in social validity assessments “is essentially a collection of consumer opinions” (p. 205), the validity of which has not been determined.
In measuring consumers’ verbal judgments, we are only hoping that these verbal behaviors are substantially controlled by variables directly relevant to the habilitation task at hand, and thus that they predict habilitative outcomes to some degree. The validity of such consumer judgments has yet to be established; they should not be viewed as a validity criterion but rather as a second opinion from a lay person who may or may not be better informed and less biased than the professional is. (p. 212)
Validating the Social Importance of Behavior Change Goals
The social validity of behavior change goals begins with a clear description of those goals.
To assess the social importance of goals, the researcher must be precise about the goals of the behavior change effort at the levels of (a) the broad social goal (e.g., improved parenting, enhanced social skills, improved cardiovascular health, increased independence), (b) the categories of behavior hypothesized to be related to the broad goal (e.g., parenting—providing instructional feedback, using time-out, etc.), and/or (c) the responses that comprise the behavioral category of interest (e.g., using time-out—directing the child to a location away from other people, instructing the child to “sit out” for a specified duration, etc.). Social validation may be conducted for any of these levels of goals. (Fawcett, 1991, pp. 235–236)
Van Houten (1979) suggested two basic approaches to determining socially valid goals: (a) Assess the performance of persons considered competent, and (b) experimentally manipulate different levels of performance to determine empirically which produces optimal results. Observations of the performance of typical performers can be used to identify and validate behavior change goals and target levels of performance. To arrive at a socially valid performance criterion for a social skills training program for two adults with disabilities who worked in a restaurant, Grossi, Kimball, and Heward (1994) observed four restaurant employees without disabilities over a period of 2 weeks to determine the frequency with which they acknowledged verbal initiations from coworkers. Results from these observations revealed that the employees without disabilities acknowledged an average of 90% of initiations directed toward them. This level of performance was selected as the goal for the two target employees in the study.
A study by Warren, Rogers-Warren, and Baer (1976) provided a good example of testing the effects of different levels of performance to determine socially valid outcomes. The researchers assessed the effect of different frequencies of children’s offers to share play materials with their peers on the peers’ reactions to those offers. They found that peers accepted offers to share most consistently when those offers were made at a middle frequency; that is, not too frequently, not too seldom.
Validating the Social Acceptance of Interventions
Several scales and questionnaires for obtaining consumers’ opinions of the acceptability of behavioral interventions have been developed. For example, the Intervention Rating Profile is a 15-item Likert-type scale for assessing the acceptability of classroom interventions (Martens, Witt, Elliott, & Darveaux, 1985). The Treatment Acceptability Rating Form (TARF) consists of 20 questions with which parents rate the acceptability of behavioral treatments used in outpatient clinic (Reimers & Wacker, 1988). Figure 5 shows the experimenter-modified version of the TARF used by Van Norman (2005) to obtain treatment acceptability information from each participant’s parents, teachers, therapists, and behavior support staff. Although some of the people whose opinions were being sought had witnessed, or had watched a video of, the intervention being used with the student, the following description of the intervention was read to each consumer before he or she was asked to answer each of the questions:
First we conducted an assessment to find out what motivated Zachary to engage in challenging behavior(s) such as throwing materials, hitting people, and dropping to the floor. We found that Zachary engaged in challenging behavior(s), at least in part, in order to escape or avoid task demands.
Next, we taught Zachary to ask for a break as a replacement behavior for challenging behavior by using physical prompting and attaching the response of asking for a break to access to a highly preferred item, lots of attention, and a long duration break (3 min).
Then we gave Zachary the choice to ask for work by simply touching the work materials (essentially engaging in the first step of the task) and getting access to highly preferred items, attention, and long duration break (1 min) or asking for a break and getting access to moderately preferred items for a shorter duration break (30 sec). At any time during this procedure if Zachary engaged in problem behavior he was given a 10 s break with no attention and no activities/items.
Finally, we continued to give Zachary the choice to ask for work, a break or engage in problem behavior, however now we required Zachary to comply with a greater number of task-related instructions before he was given access to the highly preferred activities, attention, and a 1 min break. Each session we increased the number of task-related instructions that were given and needed to be complied with before access to the highly preferred break.
Physical prompting was only used during the initial phase to teach Zachary new responses, specifically how to ask for a break and how to ask for work. Otherwise Zachary was making all independent choices as they were presented. (p. 247)
Validating the Social Importance of Behavior Changes
Methods for assessing the social validity of outcomes include (a) comparing participants’ performance to the performance of a normative sample, (b) asking consumers to rate the social validity of participants’ performance, (c) asking experts to evaluate participants’ performance, (d) using a standardized assessment instrument, and (e) testing participants’ newly learned level of performance in the natural environment.
Normative Sample
Van den Pol and colleagues (1981) used the performance of a normative sample of typical fast-food restaurant customers to assess the social validity of the posttraining performance of the young adults with disabilities whom they had taught to order or pay for a meal without assistance. The researchers simply observed 10 randomly selected, typical customers who ordered and ate a meal in fast-food restaurants. They recorded the accuracy with which these customers performed each step of a 22-step task analysis. The students’ performance at the followup probe equaled or exceeded that of the customers in the normative sample in all but 4 of 22 specific skills.
Using normative samples to assess the social validity of behavior change is not limited to posttreatment comparisons. Comparing subjects’ behavior to ongoing probes of the behavior of a normative sample provides a formative assessment measure of how much improvement has been made and how much is still needed. An excellent sample of ongoing social validity assessment is a study by Rhode, Morgan, and Young (1983), in which token reinforcement and self-evaluation procedures were used to improve the classroom behavior of six students with behavior disorders. The overall goal of the study was to help the six students improve their appropriate classroom behavior (e.g., following classroom rules, completing teacher-assigned tasks, volunteering relevant responses) and decrease inappropriate behavior (e.g., talking out, noncompliance, aggression) so that they would be accepted and successful in regular (general education) classrooms. At least once per day throughout the course of the 17-week study, the researchers randomly selected classmates in the regular classrooms for observation. The same observation codes and procedures that were used to measure the six target students’ behavior were used to obtain the normative sample data.
Figure 6 shows the mean and range of the six students’ appropriate behavior during each condition and phase of the study compared to the normative sample. (Individual graphs showing the percentage of appropriate behavior in the resource and regular classroom of all six subjects in each of nearly 90 sessions were also included in Rhode and colleagues’ article.) During baseline, the six boys’ levels of appropriate behavior were well below those of their nondisabled peers. During Phase I of the study, in which the subjects learned to self-evaluate, their behavior in the resource room improved to a level matching that of their regular classroom peers. However, when the subjects were in the regular classroom during Phase I, their behavior compared poorly with that of the other 
 
Figure 5	Examples of questions adapted from the Treatment Acceptability Rating Form—Revised (Reimers and Wacker, 1988) to obtain consumers’ opinions of the acceptability of intervention procedures used to treat challenging behaviors of secondary students with severe disabilities.
Treatment Acceptability Rating Form—Revised (TARF-R)
1.	How clear is your understanding of the suggested procedures?
	_____	_____	_____	_____	_____	_____	_____
Not at Neutral Very all clear	clear
2.	How acceptable do you find the strategies to be regarding your concerns about theidentified learner?
	_____	_____	_____	_____	_____	_____	_____
Not at all Neutral Very acceptable	acceptable
3.	How willing are you to implement the suggested procedures as you heard themdescribed?
	_____	_____	_____	_____	_____	_____	_____
Not at Neutral Very all willing	willing
4.	Given the learner’s behavior issues, how reasonable do you find the suggestedprocedures?
	_____	_____	_____	_____	_____	_____	_____
Not at all Neutral Very reasonable	reasonable
5.	How costly will it be to implement these strategies?
	_____	_____	_____	_____	_____	_____	_____
Not at Neutral Very all costly	costly
11. How disruptive will it be to your classroom to implement the suggested procedures?
	_____	_____	_____	_____	_____	_____	_____
Not at all Neutral Very disruptive	disruptive
13.	How affordable are these procedures?
	_____	_____	_____	_____	_____	_____	_____
Not at all Neutral Very affordable	affordable
14.	How much do you like the proposed procedures?
	_____	_____	_____	_____	_____	_____	_____
Do not like 	Neutral	Like them them at all	very much
17. How much discomfortis your learner likely to experience as a result of these procedures?
	_____	_____	_____	_____	_____	_____	_____
No discomfort Neutral Very much at all	discomfort
19.	How willing would you be to change your classroom routine to implement theseprocedures?
	_____	_____	_____	_____	_____	_____	_____
Not at Neutral Very all willing	willing
20.	How well will carrying out these procedures fit into your classroom routine?
	_____	_____	_____	_____	_____	_____	_____
	Not at 	Neutral	Very 
	all well	well
From “The Effects of Functional Communication Training, Choice Making, and an Adjusting Work Schedule on Problem Behavior Maintained by Negative Reinforcement” by R. K. Van Norman, 2005, pp. 248–256. Unpublished doctoral dissertation. Columbus, OH: The Ohio State University. Used by permission.
	= Resource room (subjects)	Figure 6	Example of using 
	= Regular classroom (subjects)	measures of the behavior of a
= Regular classroom (peers)
normative sample standard for
= Range/regular classroom (Subjects)
assessing the social validity of outcomes of a behavior change program.
From “Generalization and Maintenance of Treatment
Gains of Behaviorally Handicapped Students from
Resource Rooms to Regular Classrooms Using SelfEvaluation Procedures” by G. Rhode, D. P. Morgan, and
K. R. Young, 1983, Journal of Applied Behavior Analysis,
16,p. 184. Copyright 1984 by the Society for the Experimental Analysis of Behavior, Inc. Reprinted by permission.
 
Experimental Conditions
students in the normative sample. As Phase II progressed, which involved various strategies for generalization and maintenance of the treatment gains, the mean level of appropriate behavior by the six students matched that of their nondisabled peers, and variability among the six students decreased (except for one subject who exhibited no appropriate behavior on one session in the next-tolast condition).
Consumer Opinion
The most frequently used method for assessing social validity is to ask consumers, including subjects or clients whenever possible, if they thought behavior changes occurred during the study or program, and if so, if they thought those behavior changes were important and valuable. Figure 7 shows the questionnaire Van Norman (2005) used to obtain opinions of consumers (i.e., the subjects’ parents, teachers, and instructional aides; school administrators; behavior support staff; an occupational therapist; a school psychologist; and a psychology aide) on the social validity of results of an intervention designed to reduce challenging behavior maintained by escape. Van Norman created a series of 5-minute video clips from randomly selected before-intervention and afterintervention sessions and placed the clips in random order on a CD. The social validity evaluators did not know whether each clip represented a before- or afterintervention session. After viewing each clip, the consumer completed the questionnaire shown in Figure 7.
Expert Evaluation
Experts can be called on to judge the social validity of some behavior changes. For example, as one measure of the social validity of changes in the unaided notetaking skills of high school students with learning disabilities in social studies lectures as a result of having been exposed to teacher-prepared guided notes, White (1991) asked 16 secondary social studies teachers to rate the students’ baseline and post-intervention lecture notes on three dimensions: (1) accuracy and completeness compared to lecture content; (2) usefulness for study for tests over the lecture content; and (3) how the notes compared to those taken by typical general education students. (The teachers did not know whether each set of notes they were rating was from baseline or post-intervention condition.)


Fawcett (1991) observed that, “If expert ratings are not sufficiently high, [the researcher should] consider what else might be done to program for social validity. Despite the best efforts, assessments may show that research goals are regarded by consumer judges as insignificant, interventions procedures as unacceptable, or results as unimportant” (p. 238). Standardized Tests Standardized tests can be used to assess the social validity of some behavior change program outcomes. Iwata, Pace, Kissel, Nau, and Farber (1990) developed the SelfInjury Trauma Scale (SITS) to enable researchers and therapists to measure the number, type, severity, and location of injuries produced by self-injurious behavior. The SITS yields a Number Index and Severity Index with scores of 0 to 5, and an estimate of current risk. Although the data collected in a treatment program may show significant decreases in the behaviors that produce self-injury (e.g., eye poking, face slapping, head banging), the social significance of the treatment must be validated by evidence of reduced injury. Iwata and colleagues wrote: . . . the social relevance of the behavior lies in its traumatic outcome. The measurement of physical injuries prior to treatment can establish the fact that a client or subject actually displays behavior warranting serious attention. . . . Conversely, injury measurement following treatment can corroborate observed changes in behavior because reduction of an injury-producing response below a certain level should be reflected in the eventual disappearance of observable trauma. In both of these in- stances, data on injuries provide a means of assessing social validity. (Wolf, 1978, pp. 99– 100) Twohig and Woods (2001) used the SITS to validate the outcomes of a habit-reversal treatment for the chronic skin picking of two typically developing adult males. Both men reported that they had engaged in skin picking since childhood, digging the fingernails into the ends of a finger and pulling or scrapping the skin, which sometimes caused bleeding, scarring, and infections. Two observers independently rated pretreatment, posttreatment, and follow-up photographs of the two men’s hands with the SITS. The Number Index (NI) and Severity Index (SI) SITS scores on the pretreatment photographs for both men were 1 and 2, respectively, indicating one to four injuries on either hand and distinct but superficial breaks in the skin. NI and SI scores of 0 on the posttreatment photos for both men indicated no apparent injuries. On the follow-up photos taken 4 months after treatment had ended, both men had SITS NI and SI scores of 1, indicating red or irritated skin. Real-World Test Perhaps the most socially valid way to assess the social validity of a learner’s newly acquired behavior is to put it to an authentic test in the natural environment. For instance, the validity of what three adolescents with learning disabilities had learned about road signs and traffic laws was validated when they passed the Ohio Department of Motor Vehicles test and earned their temporary driver’s permits (Test & Heward, 1983). In a similar way, the social validity of the cooking skills being learned by three secondary students with developmental disabilities and visual impairments was tested frequently when their friends arrived at the end of probe sessions to share the food they had just prepared (Trask-Tyler, Grossi, & Heward, 1994). In addition to providing a direct and authentic assessment of social validity, real-world tests put the learner’s repertoire in contact with naturally occurring contingencies of reinforcement, which may promote maintenance and generalization of the newly acquired behaviors. External Validity: Replicating Experiments to Determine the Generality of Research Findings External validity refers to the degree to which a functional relation found reliable and socially valid in a given experiment holds under different conditions. An intervention that works only within a circumscribed set of conditions and proves ineffective when any aspect in the original experiment is altered makes a limited contribution to the development of a reliable and useful technology of behavior change. When a carefully controlled experiment has shown that a particular treatment produces consistent and socially significant improvements in the target behavior of a given subject, a series of important questions should then be asked: Will this same treatment be as effective if it is applied to other behaviors? Will the procedure continue to work if it is changed in some way (e.g., if implemented at a different time of the day, by another person, on a different schedule)? Will it work in a setting different from the original experiment? Will it work with participants of different ages, backgrounds, and repertoires? Questions about external validity are not abstract or rhetorical; they are empirical questions that can be addressed only by empirical methods. A functional relation with external validity, or generality, will continue to operate under a variety of conditions. External validity is a matter of degree, not an all-or-nothing property. A functional relation that cannot be reproduced under any conditions other than the exact set of original variables (including the original subject) possesses no external validity. At the other end of the continuum, a procedure that is effective at any time, under any conditions, in any setting, with any behavior, and for any subject has complete generality (an improbable situation). Most functional relations fall somewhere between the two ends of this continuum, and those found to have higher degrees of generality make the greater contribution to applied behavior analysis. Investigators who use groups-comparison research methods approach the issue of external validity quite differently than do investigators who use within-subjects research methods. External Validity and Groups Design Research As stated previously, practitioners of group-comparison experimental designs claim two advantages for the use of large groups of subjects. In addition to the assumption that aggregating the data of a group of subjects will control for intersubject variability, researchers who employ group designs assume that including many subjects in an experiment increases the external validity of a study’s results. On the surface this assumption is perfectly logical, and when viewed at the proper level of extrapolation, it is also true. The more subjects with which a functional relation has been demonstrated, the more likely it is that that functional relation will also be effective with other subjects who share similar characteristics. And in fact, demonstrating a functional relation with various subjects in different settings is exactly how applied behavior analysts document external validity.

However, the investigator who claims that the findings of a groups-comparison study possess generality to other individuals in the population from which the experimental subjects were chosen violates a fundamental premise of the groups-comparison method and ignores a defining characteristic of behavior. The proper inferences about the results of a groups-design study are from the sample to the population, not from the sample to the individual (Fisher, 1956). The careful methods of random sampling used in groups-design research are followed to ensure that the participants in the study represent a heterogeneous sample of all the relevant characteristics found in the population from which they were selected. Indeed, the better the sample represents the population from which it is drawn, the less meaningful are the results for any individual subject. “The only statement that can be made concerns the average response of a group with that particular makeup which, unfortunately, is unlikely to be duplicated” (Hersen & Barlow, 1976, p. 56). A second problem inherent in attempting to extend the results of a groups-comparison study to other people (and unless care is taken, sometimes even to a subject who participated in the study, as was illustrated in Figure 1) is that the groups-design experiment does not demonstrate a functional relation between the behavior of any subject and some aspect of his or her environment. In other words, from the perspective of behavior analysis, there is nothing in the results of a groups-design experiment that can have external validity; there is nothing to generalize. Johnston and Pennypacker (1993a) made this point repeatedly and well. Between groups designs tend to put the cart before the horse. The tactic of exposing different levels of the independent variable to different groups composed of a large number of subjects and treating their responses collectively by asking if they represent the untested portion of the population provides comparisons that describe no member of any group. By failing to focus on individuals with careful attention to experiment control, these traditional methods greatly decrease the chances of discovering orderly relationships in the first place, thereby making the questions of subject generality moot. (1993a, p. 352) The researcher’s first objective is to obtain data that truly represent the relationship between the experimental conditions and the dependent variable for each subject. If this is not accomplished, nothing else matters. Only when the findings are “true” does the question of the meaningfulness or universality of these results become relevant. (1993a, p. 250) Groups-comparison designs and statistical inference have long dominated research in psychology, education, and the other social sciences. Despite its long-standing dominance, the extent to which this research tradition has contributed to an effective technology of behavior change is highly questionable (Baer, 1977b; Birnbrauer, 1981; Michael, 1974). The field of education is perhaps the most telling example of the inability of groups-design research to provide data that lead to improved practice (Greer, 1983; Heward & Cooper, 1992). Instructional methods in the classroom are often influenced more by fad, the personal style of individual teachers, and ideology than by the cumulative knowledge and understanding provided by rigorous and sustained experimental analysis of the variables of which learning is a function (Heron, Tincani, Peterson, & Miller, 2005; Kozloff, 2005; Zane, 2005). The methods of groups-comparison experimentation are simply inappropriate for answering the questions of primary interest to the applied behavior analyst— empirical questions that can be pursued only by analysis of repeated measures of individual behavior under all relevant conditions. We agree with Johnston and Pennypacker (1993b): We find the reasoning underlying all such procedures alien to both the subject matter and the goals of a natural science of behavior and regard the utility of group comparisons as extremely limited, no matter how elegant the mathematical treatment of data they afford. . . . [group comparison experimentation constitutes] a process of scientific inquiry that is almost totally inverted; instead of using questions about natural phenomena to guide decisions about experimental design, models of design are allowed to dictate both the form and content of the questions asked. Not only is this antithetical to the established role of experimentation in science, the types of questions allowed by groups comparison designs are largely inappropriate or irrelevant to gaining an understanding of the determinants of behavior. (pp. 94– 95) Our discussion of the inherent limitations of groupscomparison designs for behavior analysis should not be confused with a position that group designs and statistical inference have no value as research methods for seeking empirical knowledge about the world. On the contrary, group-comparison designs and statistical inference are highly effective tools for seeking answers to the kinds of questions for which they were devised. Properly designed and well-executed groups-comparison experiments can provide answers with a specific degree of confidence (i.e., probability) to questions involved in many large-scale evaluations. For example, a government body is less interested in the effects of a new regulation on any individual person (and even less interested in whether a functional relation exists between the regulation and that person’s behavior) than it is in the probability that the behavior of a predictable percentage of the population will be affected by the regulation. The former concern is a behavioral one, and the experimental methods of behavior analysis provide the means to address it. The latter concern is an actuarial one, and it is best pursued with the methods of random sampling, groups comparison, and statistical inference. External Validity and Applied Behavior Analysis The external validity (generality) of research findings in applied behavior analysis is assessed, established, and specified through the replication of experiments. In order to know whether a particular result will be obtained for a subject in another study or under applied circumstances, what we really need to know is what variables are necessary to make the effect occur, what variables will prevent it from occurring, and what variables will modulate it. . . . This information cannot be learned by increasing the size of the control and experimental groups. It requires conducting a series of experiments that identify and study variables that might fall into one of these three categories. (Johnson & Pennypacker, 1993a, p. 251) Replication in this context means repeating a previous experiment. 9 Sidman (1960) described two major types of scientific replication— direct and systematic. Direct Replication In a direct replication , the researcher makes every effort to duplicate exactly the conditions of an earlier experiment. If the same subject is used in a direct replication, the study is an intrasubject direct replication. Intrasubject replication within experiments is a defining characteristic of applied behavior analysis research and the primary tactic for establishing the existence and reliability of a functional relation. An intersubject direct replication maintains every aspect of the earlier experiment except that different, although similar, subjects are involved (i.e., same age, similar repertoires). Intersubject replication is the primary method for determining the extent to which research findings have generality across subjects. The many uncontrolled variables in natural settings make the direct replication of experiments outside of the 9 Johnston and Pennypacker (1980) pointed out a distinction between replicating an experiment and reproducing its results. They stated that the quality of the replication should be judged only by “the extent to which equivalent environmental manipulations associated with [the original experiment] are duplicated. . . . Thus, one replicates procedures in an effort to reproduce effects” (pp. 303– 304). However, when most researchers report a “failure to replicate,” they mean that the results of the replication did not match those obtained in the earlier research (e.g., Ecott, Foate, Taylor, & Critchfield, 1999; Friedling & O’Leary, 1979). laboratory extremely difficult. Nevertheless, intersubject replication is the rule rather than the exception in applied behavior analysis. Although numerous single-subject studies involve just one subject (e.g., Ahearn, 2003; Dixon & Falcomata, 2004; Kodak, Grow, & Northrup, 2004; Tarbox, Williams, & Friman, 2004), the vast majority of published studies in applied behavior analysis include direct intersubject replications. This is because each subject is usually considered an intact experiment. For example, a behavior analysis study in which the independent variable is manipulated in exactly the same way for six subjects in the same setting yields five intersubject replications. Systematic Replication The direct replication of experiments demonstrates the reliability of a functional relation, but the generality of that finding to other conditions can be established only through repeated experimentation in which the conditions of interest are purposefully and systematically varied. In a systematic replication the researcher purposefully varies one or more aspects of an earlier experiment. When a systematic replication successfully reproduces the results of previous research, it not only demonstrates the reliability of the earlier findings but also adds to the external validity of the earlier findings by showing that the same effect can be obtained under different conditions. In a systematic replication, any aspect of a previous experiment can be altered: subjects, setting, administration of the independent variable, target behaviors. Although systematic replication offers greater potential rewards than direct replication does because it can provide new knowledge about the variables under investigation, it entails some risk. Sidman (1960) described systematic replication as a gamble, but one well worth taking. If systematic replication fails, the original experiment will still have to be redone, else there is no way of determining whether the failure to replicate stemmed from the introduction of new variables in the second experiment, or whether the control of relevant factors was inadequate in the first experiment. On the other hand, if systematic replication succeeds, the pay-off is handsome. Not only is the reliability of the original finding increased, but also its generality with respect to other organisms and to other experimental procedures is greatly enhanced. Furthermore, additional data are now available which could not have been obtained by a simple repetition of the first experiment. (pp. 111– 112) Sidman went on to explain that economic husbandry of limited resources must also play an important role in
the scientist’s determination of how a research program should proceed. Direct replication of a long and costly experiment can provide data on only the reliability of a functional relation, whereas systematic replication can provide information on the reliability and generality of the phenomena under investigation, as well as new information for additional experimentation. The external validity of results of groups-comparison research is viewed as an inherent characteristic of a given experiment, as something that can be directly assessed by examining the methods used to conduct the study (e.g., sampling procedures). If that logic is extended to singlesubject experiments, then the findings of single-subject experiments cannot be said to have any external validity. But as Birnbrauer (1981) pointed out, external validity is not something a single study has, but rather the product of many studies . External validity can be pursued only through the active process of systematic replication. Generality is established, or more likely limited, by accumulating studies which are internally valid and by placing the results into a systematic context, i.e., seeking out the principles and parameters that particular procedures appear to be enunciating. The most informative studies ask how can an earlier positive result be repeated in the present circumstances, with the present problem? (p. 122) Much of the applied behavior analysis literature consists of systematic replications. Indeed, one could argue quite persuasively that almost any applied behavior analysis study is a systematic replication of at least some aspect of an earlier experiment. Even when the authors have not pointed it out, virtually every published experiment reveals significant procedural similarity with previous experiments. However, as we are using the term here, systematic replication refers to concerted and directed efforts to establish and specify the generality of a functional relation. For example, Hamlet, Axelrod, and Kuerschner (1984) found a functional relation between demanded eye contact (e.g., “[Name], turn around”) and compliance with adult instructions in two 11-year-old school children. Included in the same published report were the results of six replications conducted by the same researchers over a period of one year with nine students aged 2 to 21 years. Similar results were reproduced in eight of the nine replication subjects. Although some might consider this an example of direct intersubject replication, Hamlet and colleagues’ replications were conducted in various settings (i.e., classrooms, homes, institutions) and therefore should be viewed as a series of systematic replications that demonstrated not only the reliability of the results but also considerable generality across subjects of different ages in different settings. Systematic replications across subjects sometimes reveal different patterns of effects, which the researcher might then study as a function of specific subject characteristics or contextual variables. For example, Hagopian, Fisher, Sullivan, Acquisto, and LeBlanc (1998) reported the results of a series of systematic replications with 21 inpatient cases of functional communication training with and without extinction and punishment. 10 Lerman, Iwata, Shore, and DeLeon (1997) found that thinning FR 1 schedules of punishment to intermittent punishment produced different effects on the selfinjurious behavior of five adults with profound mental retardation. Some systematic replications are attempts to reproduce the results reported by another researcher in a slightly different situation or context. For example, Saigh and Umar (1983) successfully reproduced in a Sudanese classroom the positive results originally reported with the Good Behavior Game in an American classroom (Barrish, Saunders & Wolf, 1969). Saigh and Umar reported that a “considerable degree of support for the cross-cultural utility of the game was established” (p. 343). Researchers sometimes report multiple experiments, with each experiment serving as a systematic replication investigating the variables influencing a given functional relation. For example, Fisher, and colleagues (1993) conducted four studies designed to explore the effectiveness of functional communication training (FCT) with and without extinction and punishment. Systematic replication is evident when a research team pursues a consistent line of related studies over time. Examples of this approach to replication can be found in Van Houten and colleagues’ studies investigating variables affecting driver behavior and pedestrian safety (e.g., Huybers, Van Houten, & Malenfant, 2004; Van Houten & Nau, 1981, 1983; Van Houten, Nau, & Marini, 1980; Van Houten & Malenfant, 2004; Van Houten, Malenfant, & Rolider, 1985; Van Houten & Retting, 2001); Neef, Markel, and colleagues’ experiments on the impulsivity of students with attention-deficit/hyperactivity disorder (ADHD) (e.g., Bicard & Neef, 2002; Ferreri, Neef, & Wait, 2006; Neef, Bicard, & Endo, 2001; Neef, Bicard, Endo, Coury, & Aman, 2005; Neef, Markel et al., 2005); and Miltenberger and colleagues’ line of research on teaching safety skills to children (e.g., Himle, Miltenberger, Flessner, & Gatheridge, 2004; Himle, Miltenberger, Gatheridge, & Flessner, & 2004; Johnson, Miltenberger et al., 2005; Johnson, Miltenberger et al., 2006; Miltenberger et al., 2004; Miltenberger et al., 2005).

In many instances, the systematic replications necessary to explore and extend a significant line of research require the independent efforts of investigators at different sites who are aware of, and build on, one another’s work. When independent teams of researchers at different geographical locations report similar findings, the net result is a body of knowledge with significant scientific integrity and technological value. This collective effort speeds and enhances the refinement and rigorous testing of interventions that is necessary to the development and refinement of evidence-based practices (Horner et al., 2005; Peters & Heron, 1993). One such example of independent research teams at various sites reporting systematic replications is a growing body of studies exploring the effects of response cards on students’ academic engagement, learning, and deportment during group instruction. Investigators have reported a similar pattern of results— increased participation during instruction, improved retention of lesson content, and/or reductions in off-task and disruptive behaviors— with response cards in a wide range of students (general education students, special education students, and ESL learners), curriculum content (e.g., math, science, social studies, spelling), and instructional settings (e.g., elementary, middle, secondary, and college classrooms) (e.g., Armendariz & Umbreit, 1999; Cavanaugh, Heward, & Donelson, 1996; Christle & Schuster, 2003; Davis & O’Neill, 2004; Gardner, Heward, & Grossi, 1994; Kellum, Carr, & Dozier, 2001; Lambert, Cartledge, Lo, & Heward, 2006; Marmolejo, Wilder, & Bradley, 2004). Evaluating Applied Behavior Analysis Research A list of all the expectations and characteristics of exemplary research in applied behavior analysis would be very long. Thus far we have identified a considerable number of requirements for good applied behavior analysis. Our purpose now is to summarize those requirements in a sequence of questions one might ask in evaluating the quality of research in applied behavior analysis. Those questions can be organized under four major headings: internal validity, social validity, external validity, and scientific and theoretical significance. Internal Validity To determine whether an analysis of behavior has been made, the reader of an applied behavior analysis study must decide whether a functional relation has been demonstrated. This decision requires a close examination of the measurement system, the experimental design, and the degree to which the researcher controlled potential confounds, as well as a careful visual analysis and interpretation of the data. Definition and Measurement of the Dependent Variable The initial step in evaluating internal validity is to decide whether to accept the data as valid and accurate measures of the target behavior over the course of the experiment. Some of the important issues to be considered in this decision are captured by the questions shown in Figure 8. Graphic Display If the data are accepted as a valid and accurate representation of the dependent variable over the course of the experiment, the reader should next assess the extent of stability of the target behavior during each phase of the study. Before evaluating the stability of the data paths, however, the reader should examine the graphic display for any sources of distortion (e.g., scaling of axes, distortions of time on the horizontal axis). The researcher or consumer who suspects that any element of the graph may encourage interpretations unwarranted by the data should replot the data using a new set of appropriately scaled axes. In an assessment of the stability of the dependent variable within the different phases of an experiment, the length of the phase or condition must be considered as well as the presence of trends in the data path. The reader should ask whether the conditions in effect during each phase were conducive to practice effects. If so, were these effects allowed to play themselves out before experimental variables were manipulated? Meaningfulness of Baseline Conditions The representativeness or fairness of the baseline conditions as the basis for evaluating subsequent performance in the presence of the independent variable should be assessed. In other words, were the baseline conditions meaningful in relation to the target behavior, setting, and research questions addressed by the experiment? For example, consider two experiments by Miller, Hall, and Heward (1995) that evaluated the effects of two procedures for conducting 1-minute time trials during a daily 10-minute practice session on the rate and accuracy with which students answered math problems. Throughout all conditions and phases of both experiments, the students were instructed to answer as many problems as they could and they received feedback on their performance. Throughout both experiments, students’ worksheets were marked and scored as follows: Experimenters marked each student’s worksheets by putting an ‘X’ next to incorrect answers. The number of correct answers over the total number of problems attempted was marked at the top of the first worksheet along with a positive comment to encourage the students to keep trying. If a student’s score was lower than his or her highest previous score, comments such as “Keep trying, Sally!” “Work faster!” or “Keep working on it!” were written. Whenever a student achieved his or her highest score to date, comments such as “Great job, Jimmy! This is your best ever!” were written on the packet. In the event a student equaled her highest previous score, “You tied your best score!” was written on the packet. At the beginning of each session, scored and marked worksheets from the prior day’s session were returned to the students. Each session during the 10-minute continuous work period condition that functioned as the baseline condition began with the classroom teachers saying to the students: “I want you to work hard and try to do your best. Answer as many problems as you can. Don’t worry if you do not answer all of the problems. There are more problems in the packet than anyone can do. Just try your best” (p. 326). The initial baseline (A) phase was followed by the two time-trial conditions (B and C) in an A-B-A-B-C-BC design. Results for students in both classrooms showed a clear functional relation between both of the time-trial conditions and increased correct rate and accuracy over the baseline condition. However, if the classroom teachers had not instructed and reminded the students to do their best and to answer as many problems as they could prior to each baseline session, and if the students had not received feedback on their worksheets, the improved performance during the time-trial conditions would have been suspect. Even if a clear functional relation had been demonstrated against such baseline conditions, applied researchers and consumers could, and should, question the importance of such results. Maybe the children simply did not know they were expected to work fast. Perhaps the students would have solved problems in the baseline condition at the same high rates that they did in the time-trial conditions if they had been told to “go fast” and received feedback on their performance, praise for their improvements, and encouragement to answer more problems. By including the daily instruction to work hard and answer as many problems as they could and returning the worksheets to the students as components of the baseline condition, Miller and colleagues obtained meaningful data paths during baseline against which to test and compare the effects of the two time-trial conditions. Experimental Design The experimental design should be examined to determine the type of experimental reasoning it affords. What elements of the design enable prediction, verification, and replication? Is the design appropriate for the research questions addressed by the study? Does the design effectively control for confounding variables? Does the design provide the basis for component and/or parametric analyses if such questions are warranted? Visual Analysis and Interpretation Although various statistical methods for evaluating behavioral data and determining the existence of functional relations in single-subject designs have been recommended (e.g., Gentile, Rhoden, & Klein, 1972; Hartmann, 1974; Hartmann et al., 1980; Jones, Vaught, & Weinrott, 1977; Pfadt & Wheeler, 1995; Sideridis & Greenwood, 1996), visual inspection remains the most commonly used, and we believe the most appropriate, method for interpreting data in applied behavior analysis. We will briefly present four factors that favor visual analysis over tests of statistical significance in applied behavior analysis. First, applied behavior analysts have little interest in knowing that a behavior change is a statistically significant outcome of an intervention. Applied behavior analysts are concerned with producing socially significant behavior changes: “If a problem has been solved, you can see that; if you must test for statistical significance, you do not have a solution” (Baer, 1977a, p. 171). Second, visual analysis is well suited for identifying variables that produce strong, large, and reliable effects, which contribute to an effective, robust technology of behavior change. On the other hand, powerful tests of statistical analysis can detect the slightest possible correlation between the independent and dependent variables, which may lead to the inclusion of weak, unreliable variables in the technology. Two types of errors are possible when determining experimental effect (see Figure 9). A Type I error (also called a false positive ) is made when the researcher concludes that the independent variable had an effect on the dependent variable, when in truth no such relation exists in nature. A Type II error (also called a false negative ) is the opposite of a Type I error. In this case, the researcher concludes that an independent variable did not have an effect on the dependent variable, when in truth it did. Ideally, a researcher using well-reasoned experimental tactics coupled with a sound experimental design and buttressed by appropriate methods of data analysis will conclude correctly that a functional relation between the independent and dependent variables exists (or does not exist). Baer (1977b) pointed out that the behavior analyst’s reliance on visual inspection to determine experimental effects results in a low incidence of Type I errors but increases the commission of Type II errors. The researcher who relies on tests of statistical significance to determine experimental effects makes many more Type I errors than the behavior analyst, but misses few, if any, variables that might produce some effect. Scientists who commit relatively many Type 1 errors are bound to memorize very long lists of variables that are supposed to affect diverse behaviors, some predictable portion of which are not variables at all. By contrast, Exists Concludes Relations Researcher Functional Yes No Functional Relations Exist in Nature Yes No Correct Conclusion Type I Error (false positive) Type II Error (false negative) Correct Conclusion Figure 9 Ideally, an experimental design and methods of data analysis help a researcher conclude correctly that a functional relation between the independent and dependent variables exists (or does not exist) when in fact such a relation does (or does not) exist in nature. Concluding that the results of an experiment reveal a functional relation when no such relation exists in nature is a Type I error. Conversely, concluding that that independent variable did not have an effect on the dependent variable when such a relation did occur is a Type II error.

scientists who commit very few Type 1 errors have relatively short lists of variables to remember. Furthermore, and much more important, it is usually only the very robust, uniformly effective variables that will make their list. Those who will risk Type 1 errors more often will uncover a host of weak variables. Unquestionably, they will know more, although some of that more is wrong, and much of it is tricky. . . . Those who keep their probability of Type 2 errors low do not often reject an actually functional variable, relative to those whose Type 2 error probability is higher. Again, unquestionably, the practitioner with the lower probability of Type 2 errors will know more; but again, the nature of that more is seen often in its weakness, inconsistency of function, or its tight specialization. . . . Individual-subject-design practitioners . . . necessarily fall into very low probabilities of Type 1 errors and very high probabilities of Type 2 errors, relative to their group-paradigm colleagues. As a result, they learn about fewer variables, but these variables are typically more powerful, general, dependable, and— very important— sometimes actionable. These are exactly the variables on which a technology of behavior might be built. (Baer, 1977b, pp. 170– 171) A third problem with using statistical methods to determine the existence of functional relations in behavioral data occurs with borderline data sets containing significant amounts of variability. Such data sets should motivate a researcher to engage in additional experimentation in an effort to achieve more consistent experimental control and to discover the factors causing the variability. The researcher who forgoes the additional experimentation in favor of accepting the results of a test of statistical significance as evidence of a functional relation risks leaving important findings in the realm of the unknown. The situation where a significance test might seem helpful is typically one involving sufficient uncontrolled variability in the dependent variable that neither the experimenter nor his readers can be sure that there is an interpretable relationship. This is evidence that the relevant behavior is not under good experimental control, a situation calling for more effective experimentation, not a more complex judgmental aid. (Michael, 1974, p. 650) Fourth, statistical tests of significance can be applied only to data sets that conform to predetermined criteria. If statistical methods for determining experimental effects were to become highly valued in applied behavior analysis, researchers might begin to design experiments so that such tests could be computed. The resultant loss of flexibility in experimental design would be counterproductive to the continued development of behavior analysis (Johnston & Pennypacker, 1993b; Michael, 1974). Social Validity The reader of a published study in applied behavior analysis should judge the social significance of the target behavior, the appropriateness of the procedures, and the social importance of the outcomes (Wolf, 1978). Many considerations should guide the applied behavior analyst’s selection of target behaviors. The social validity of the dependent variable should be assessed in light of those factors. Ultimately, all of the issues and considerations relative to target behavior selection point to one question: Will an increase (or decrease) in the measured dimension of this behavior improve the person’s life directly or indirectly? The independent variable should be evaluated not only in terms of its effects on the dependent variable, but also in terms of its social acceptability, complexity, practicality, and cost. Regardless of their effectiveness, treatments that are perceived by practitioners, parents, and/or clients as unacceptable or undesirable for whatever reason are unlikely to be used. Consequently, such treatments will never have the chance to contribute to a technology of behavior change. The same can be said of independent variables that are extremely complex and thus difficult to learn, teach, and apply. Similarly, treatment procedures that require large amounts of time and/or money to implement have less social validity than do procedures that can be applied quickly and/or inexpensively. Even though behavior change is clearly visible on a graphic display, it may not represent a socially valid improvement for the participant and/or significant others in his environment. In evaluating the results of an applied behavior analysis study, the reader should ask questions such as these: Is the participant (or significant others in the participant’s life) better off now that the behavior has changed? Will this new level of performance result in increased reinforcement (or decreased punishment) for the subject now or in the future? (Hawkins, 1984). In some instances it is relevant to ask whether the subject (or significant others) believes that her behavior has improved (Wolf, 1978). Maintenance and Generalization of Behavior Change Improvements in behavior are most beneficial when they are long-lasting, appear in other appropriate environments, and spill over to other related behaviors. Producing these kinds of effects is a major goal of applied behavior analysis. When evaluating applied behavior

analysis research, consumers should consider the maintenance and generalization of behavior change in their evaluation of a study. An impressive behavior change that does not last or is limited to a specialized training setting may not be socially significant. Did the researchers report the results of assessment of maintenance and generalization through follow-up observations and measurement in nontraining environments? Better yet, if maintenance and/or generalization were not evident in such follow-up observations, did the experimenters modify their design and implement procedures in an attempt to produce and analyze the occurrence of maintenance and/or generalization? Additionally, the reader should ask whether response generalization— changes in functionally similar but untreated behaviors concomitant with changes in the target behavior(s)—is an appropriate concern in a given study. If so, did the experimenters attempt to assess, analyze, or discuss this phenomenon? External Validity As discussed earlier in this chapter, the generality of the findings of a given experiment to other subjects, settings, and behaviors cannot be assessed solely on inherent aspects of the study itself. The generality of a behavior– environment relation can be established only through the active process of systematic replication. Therefore, the reader of an applied behavior analysis study should compare the study’s results with those of other published research with which it shares relevant features. The authors of a published report identify in the paper’s introduction the experiments that they believe are most relevant. To make an effective judgment of the external validity of the data from a given study, the reader must often locate previous studies in the literature and compare the results of those studies with those of the current experiment. Even though external validity should not be considered a characteristic of a study per se (Birnbrauer, 1981), various features of an experiment suggest to the reader an expected, or likely, level of generality for the results. For example, an experiment that demonstrated a functional relation of similar form and degree in six subjects of different ages, backgrounds, and current repertoires would indicate a higher probability of generality to other subjects than would an identical study demonstrating the same results across six subjects of the same age, background, and current repertoire. Similarly, if the experiment was conducted in various settings and a number of different people administered the independent variable, additional confidence in the external validity of the results may be warranted. Theoretical Significance and Conceptual Sense A published experiment should also be evaluated in terms of its scientific merit. It is possible for a study to clearly demonstrate a functional relation between the independent variable and a socially important target behavior— and thus be judged significant from an applied perspective— yet contribute little to the advancement of the field. 11 It is possible to reliably reproduce an important behavior change while at the same time not fully understand which variables are responsible for the observed functional relation. Sidman (1960) differentiated this kind of simple reliability from “knowledgeable reproducibility,” a more complete level of analysis in which all of the important factors have been identified and are controlled. The Need for More Thorough Analyses of Socially Important Behavior Even though no behavior analyst would argue the necessity of systematic replication and the central role it plays in the development of an effective technology of behavior change, and even though the literature provides evidence that at least a loose form of systematic replication is commonly practiced, a more critical examination of the literature suggests the need for more thorough analyses of the functional relations under study. Numerous authors have discussed the importance of focusing on the analytic side of applied behavior analysis as much as the applied side (e.g., Baer, 1991; Birnbrauer, 1979, 1981; Deitz, 1982; Hayes, 1991; Iwata, 1991; Michael, 1980; Morris, 1991; Johnston, 1991; Pennypacker, 1981). After examining the majority of the experimental articles published in the first 10 volumes of the Journal of Applied Behavior Analysis (1968 to 1977), Hayes, Rincover, and Solnick (1980) concluded that a technical drift had occurred in the field away from conceptual analyses and toward an emphasis on client cure. They warned of a likely loss of scientific understanding as a result of focusing purely on the technical aspects of improving behavior in applied settings, and they recommended an increased effort to perform more thorough analyses of behavior.

The importance of component analyses, parametric analyses, and other more sophisticated analytic attempts are often to be found less in “control” (in an immediately applied sense) and more in “understanding” (in a scientific sense). One may easily control, say, aggressive behavior through the use of punishment without having contributed significantly to an understanding of aggression. . . . For example, if one has a package program that is effective, there may be little obvious value in doing a component analysis. But these more complicated analyses may increase our knowledge of the actual functional variables and subsequently increase our ability to generate more efficient and general behavioral programs. Perhaps, we have gone too far in our attempt to be immediately applied at the expense of being ultimately more effective, in failing to encourage more analogue and analytical studies that have treatment implications. (Hayes, Rincover, & Solnick, 1980, pp. 282– 283) Baer, Wolf, and Risley (1987), writing in the 20th anniversary issue of the Journal of Applied Behavior Analysis, emphasized the need to shift from demonstrations of behavior changes— as convincing as they might be— to a more complete analysis and conceptual understanding of the principles that underlie the successful demonstrations. Twenty years ago, analytic meant a convincing experimental design, and conceptual meant relevance to a comprehensive theory about behavior. Now, applied behavior analysis is considered an analytic discipline only when it demonstrated convincingly how to make specified behavior changes and when its behavior-change methods make systematic, conceptual sense. In the past 20 years, we have sometimes demonstrated convincingly that we had changed behavior as specified, but by methods that did not make systematic, conceptual sense— it was not clear why those methods had worked. Such cases let us see that we were sometimes convincingly applied and behavioral, yet even so, not sufficiently analytic. (p. 318) We agree with the need for more sophisticated, thorough analyses of the variables controlling socially important behavior. Fortunately, examination of the recent literature reveals numerous examples of the component and parametric analyses that are necessary steps to a more complete understanding of behavior— an understanding that is prerequisite to the development of a thoroughly effective technology of behavior change. Several of the studies cited earlier in this chapter as examples of systematic replication incorporated component and parametric analyses. The extent of a phenomenon’s generality is known only when all of the necessary and sufficient conditions for its reproducibility have been specified. Only when all of the variables influencing a functional relation have been identified and accounted for can an analysis be considered complete. Even then, the notion of a complete analysis is misleading: “Further dissection or elaboration of either variable in a functional relation inevitably reveals fresh variability, and analysis proceeds anew. . . . the analysis of behavior can never be complete” (Pennypacker, 1981, p. 159). Evaluation of scientific significance takes into consideration such things as the authors’ technological description of the experiment as well as their interpretation and discussion of the results. Are the procedures described in sufficient detail so that at least the unique aspects of the study can be replicated? 12 Readers should consider the level of conceptual integrity displayed in an experimental report. Does the literature review reveal a careful integration of the study with previous research? Does the literature review provide sufficient justification for the study’s research questions? Are the authors’ conclusions based on the data obtained in the study? Have the authors respected the difference between basic principles of behavior and behavior change tactics? Do the authors speculate beyond the data without making it clear that they are doing so? Do the authors suggest directions for additional research to further analyze the problem studied? Is the study important for reasons other than the results actually obtained? For example, an experiment that demonstrates a new measurement technique, investigates a new dependent or independent variable, or incorporates a novel tactic for controlling a confounding variable can contribute to the scientific advancement of behavior analysis, even though the study failed to achieve experimental control or produce socially significant behavior change. Numerous criteria and considerations are involved in evaluating the “goodness” of a published study in applied behavior analysis. Although each criterion is important on one level or another, it is unlikely that any experiment will meet all of the criteria. And, in fact, it is unnecessary for an experiment to do so to be considered good. Nevertheless, incorporating as many of these considerations as possible into a study enhances its social significance and scientific value as an applied behavior analysis.

Summary Importance of the Individual Subject in Behavioral Research 2. The focus on the behavior of individual subjects has enabled applied behavior analysts to discover and refine effective interventions for a wide range of socially significant behavior. 3. Knowing that the average performance of a group of subjects changed may not reveal anything about the performance of individual subjects. 4. To be most useful, a treatment must be understood at the level at which people come into contact with it and are affected by it: the individual level. 5. When repeated measurement reveals significant variability, the researcher should seek to identify and control the factors responsible for it. 6. Attempting to cancel out variability through statistical manipulation neither eliminates its presence in the data nor controls the variables responsible for it. 7. The researcher who attributes the effects of unknown or uncontrolled variables to chance is unlikely to identify and analyze important variables. 8. To control the effects of any variable, a researcher must either hold it constant throughout the experiment or manipulate it as an independent variable. 9. 10. A great strength of within-subject experimental designs is the convincing demonstration of a functional relation made possible by replication within the design itself. 11. The overall performance of a group is socially significant in many situations. When group results do not represent individual performances, researchers should supplement group data with individual results. 12. When the behavior analyst cannot control access to the experimental setting or identify individual subjects, the dependent variable must consist of the responses made by individuals who enter the experimental setting. Importance of Flexibility in Experimental Design 13. A good experimental design is any sequence and type of independent variable manipulations that produces data that effectively and convincingly address the research question(s). 14. To investigate the research question(s) of interest, an experimenter must often build an experimental design that employs a combination of analytic tactics. The most effective experimental designs use ongoing evaluation of data from individual subjects as the basis for em- 15. ploying the three elements of baseline logic— prediction, verification, and replication. Internal Validity: Controlling Potential Sources of Confounding in Experimental Design 16. Experiments that demonstrate a clear functional relation between the independent variable and the target behavior are said to have a high degree of internal validity. 17. The strength of an experimental design is determined by the extent to which it (a) demonstrates a reliable effect and (b) eliminates or reduces the possibility that factors other than the independent variable produced the behavior change. 18. The phrase control of behavior is technically inaccurate because the experimenter controls only some aspect of the subject’s environment. 19. A confounding variable is an uncontrolled factor known or suspected to have exerted influence on the dependent variable. 20. Steady state responding is the primary means by which applied behavior analysts assess the degree of experimental control. 21. Confounding variables can be viewed as related primarily to one of four elements of an experiment: subject, setting, measurement of the dependent variable, and independent variable. 22. A placebo control is designed to separate any effects that may be produced by a subject’s expectations of improvement as a result of receiving treatment from the effects actually produced by the treatment. 23. With a double-blind control procedure neither the subject(s) nor the observers know when the independent variable is present or absent. 24. Treatment integrity and procedural fidelity refer to the extent to which the independent variable is implemented or carried out as planned. 25. Low treatment integrity invites a major source of confounding into an experiment, making it difficult, if not impossible, to interpret the results with confidence. 26. One threat to treatment integrity, treatment drift, occurs when application of the independent variable during later phases of an experiment differs from the way the treatment was applied at the outset of the study. 27. Achieving a high level of treatment integrity begins with an operational definition of treatment procedures. Treatments that are simple, precise, and brief, and that require relatively little effort, are more likely to be delivered with consistency than those that are not.
28. 29. Researchers should not assume that a person’s general competence or experience in the experimental setting, or that providing the intervention agent with detailed written instructions or a script, will ensure a high degree of treatment integrity. Treatment integrity (or procedural fidelity) data measure the extent to which the actual implementation of experimental procedures matches their descriptions in the method section of a research report. Social Validity: Assessing the Applied Value of Behavior Changes and the Treatments That Accomplish Them 30. 31. The social validity of an applied behavior analysis can be assessed in three ways: the social significance of the target behavior, the appropriateness of the procedures, and the social importance of the results. 32. Social validity assessments are most often accomplished by seeking consumer opinions. 33. Socially valid goals can be determined empirically by assessing the performance of individuals judged to be highly competent, and experimentally manipulating different levels of performance to determine socially valid outcomes. 34. Several scales and questionnaires for obtaining consumers’ opinions of the acceptability of behavioral interventions have been developed. Methods for assessing the social validity of outcomes include (a) comparing participants’ performance to the performance of a normative sample, (b) using a standardized assessment instrument, (c) asking consumers to rate the social validity of participants’ performance, (d) asking experts to evaluate participants’ performance, and (e) testing participants’ newly learned level of performance in the natural environment. External Validity: Replicating Experiments to Determine the Generality of Research Findings 35. 36. External validity refers to the degree to which a functional relation found reliable and socially valid in a given experiment will hold under different conditions. 37. The proper inferences about the results of a groups-design study are from the sample to the population, not from the sample to the individual. 38. Because a groups-design experiment does not demonstrate a functional relation between the behavior of any subject and some aspect of his or her environment, the external validity of the results is moot. Although groups-comparison designs and tests of statistical significance are necessary and effective tools for cer- 39. 40. tain types of research questions, they have contributed little to an effective technology of behavior change. 41. The generality of research findings in applied behavior analysis is assessed, established, and specified by the replication of experiments. 42. In a direct replication the researcher makes every effort to duplicate exactly the conditions of an earlier experiment. In a systematic replication the researcher purposefully varies one or more aspects of an earlier experiment. 43. When a systematic replication successfully reproduces the results of previous research, it not only demonstrates the reliability of the earlier findings but also adds to the external validity of the earlier findings by showing that the same effect can be obtained under different conditions. 44. Systematic replications occur in both planned and unplanned ways through the work of many experimenters in a given area, and they result in a body of knowledge with significant scientific integrity and technological value. Evaluating Applied Behavior Analysis Research 45. The quality and value of an applied behavior analysis study may be evaluated by seeking answers to a sequence of questions related to the internal validity, social validity, external validity, and the scientific and theoretical significance of the study. 46. A Type I error occurs when a researcher concludes that the independent variable had an effect on the dependent variable when it did not. A Type II error occurs when a researcher concludes that the independent variable did not have an effect on the dependent variable when it did. Visual analysis effectively identifies variables that produce strong, large, and reliable effects, which contribute to an effective and robust technology of behavior change. Statistical analysis detects the slightest possible correlations between the independent and dependent variables, which may lead to the identification and inclusion of weak and unreliable variables in the technology. 47. 48. A study can demonstrate a functional relation between the independent variable and a socially important target behavior— and thus be significant from an applied perspective— yet contribute little to the advancement of the field. 49. Only when all of the variables influencing a functional relation have been identified and accounted for can an analysis be considered complete. When evaluating the scientific significance of a research report, readers should consider the technological description of the experiment, the interpretation and discussion of the results, and the level of conceptual sense and integrity.

Chapter 11 Positive Reinforcement 
automatic reinforcement conditioned reinforcer generalized conditioned reinforcer positive reinforcement Key Terms positive reinforcer Premack principle reinforcer assessment response-deprivation hypothesis stimulus preference assessment unconditioned reinforcer


In looking back, it seems to me that the most important thing I learned in graduate school was from another student, Burrhus Frederic Skinner (I called him Burrhus, others called him Fred). This man had a box, within which was a smaller box, within which he would place a hungry laboratory rat. When the animal, in its explorations, would depress a lever that projected from one wall, a pellet of food would be discharged into a tray beneath the lever. Under such conditions, the rat would learn, in a matter of minutes, sometimes seconds, how to get its meal by depression of the lever. It would even keep on pressing, sometimes at a rapid rate, when pellets were delivered only now and then; and if the food supply was cut off entirely, the animal would still keep working for awhile. taught, but it generally consisted of the student being seated and oriented toward the appropriate object or person (e.g., looking at course materials or the lecturing teacher) and class participation (e.g., writing the assignment, answering the teacher’s question). The independent variable was teacher attention, cued by an observer who held up a small square of colored paper not likely to be noticed by the target student. On this signal, the teacher attended to the child by moving to his desk, making a verbal comment, giving him a pat on the shoulder, or the like. The effects of contingent teacher attention on the behavior of all six students were striking. Figure 1 shows the results for Robbie, a third-grader chosen to participate because he was “a particularly disruptive student who studied very little” (p. 3). During baseline, Robbie engaged in study behavior for an average of 25% of the observed intervals. The remainder of the time he snapped rubber bands, played with objects in his pocket, talked and laughed with classmates, and played with an empty milk carton from this earlier-served drink. The majority of attention Robbie received during baseline followed these nonstudy behaviors. During baseline, Robbie’s teacher often urged him to work, told him to put his milk carton away, and told him to stop bothering his classmates. Following baseline, the experimenters showed the teacher a graph of Robbie’s study behavior, presented the results of previous studies in which contingent adult attention had improved child behavior, and discussed the fundamentals of providing social reinforcement. Hall and colleagues (1968) described the procedure implemented during the two reinforcement phases as follows: —Fred Keller (1982, p. 7) Although some people still believe that findings from laboratory research on animal learning are not applicable to human behavior, by the mid1960s applied researchers had established the significance of positive reinforcement in education and treatment. “It is safe to say that without Skinner’s detailed laboratory analyses of reinforcement (Skinner, 1938), there would be no field of ‘applied behavior analysis’ today, least not as we know it” (Vollmer & Hackenberg, 2001, p. 241). Positive reinforcement is the most important and most widely applied principle of behavior analysis. Fittingly, the lead article in the first issue of the Journal of Applied Behavior Analysis reported several experiments showing the effects of positive reinforcement on student behavior (Hall, Lund, & Jackson, 1968). Six elementary students who were disruptive or dawdled frequently participated in this classic study. The dependent variable, study behavior, was defined individually for each student depending on the subject matter being Whenever Robbie had engaged in 1 min of continuous study the observer signaled his teacher. On this cue, the 278 teacher approached Robbie, saying, “Very good work Robbie,” “I see you are studying,” or some similar remark. She discontinued giving attention for nonstudy behaviors including those which were disruptive to the class. (p. 4) During Reinforcement 1, Robbie’s study behavior increased to a mean of 71%. When a reversal to baseline conditions was introduced, his study behavior decreased to a mean of 50%; but when Robbie’s teacher again provided attention for study behavior (Reinforcement 2), his study behavior recovered and stabilized at a level ranging between 70 and 80% of the observed intervals. Results of follow-up observations over a 14-week period after signaling the teacher had been discontinued showed that Robbie’s study behavior had maintained at 79%. The teacher reported positive behavior changes associated with Robbie’s increased study behavior. By the final week of Reinforcement 2, Robbie completed his spelling assignments more consistently, his disruptive behavior had diminished, and he continued to study while drinking his milk and did not play with the carton afterwards. The intervention used by Robbie’s teacher to help him be more successful in the classroom was based on the principle of positive reinforcement. In this chapter we examine the definition and nature of positive reinforcement, describe methods for identifying potential reinforcers and assessing their effects, outline experimental control techniques for verifying whether a positive reinforcement contingency is responsible for increased responding, and offer guidelines for using positive reinforcement effectively. Definition and Nature of Positive Reinforcement The principle of reinforcement is deceptively simple. “The basic operant functional relation for reinforcement is the following: When a type of behavior (R) is followed by reinforcement (S R ) there will be an increased future frequency of that type of behavior” (Michael, 2004, p. 30). 1 However, as Michael and other authors have pointed out, three qualifications must be considered regarding the conditions under which the effects reinforcement will occur. These qualifications are (a) the delay between the response and onset of the consequence, (b) the stimulus conditions in effect when the response was emitted, and 1 Various terms, such as strengthening the behavior or increasing the likelihood of future responding, are sometimes used by behavior analysts to describe the basic effect of reinforcement. Although such terms appear occasionally in this book, recognizing Michael’s (1995) concern that use of such terms “encourages a language of intervening variables, or an implied reference to something other than an observable aspect of behavior” (p. 274), we most often use increased future frequency to refer to the primary effect of reinforcement. (c) the strength of the current motivation with respect to the consequence. In this section we examine these qualifications and several other concepts requisite to acquiring a full understanding of how reinforcement “works.” Operation and Defining Effect of Positive Reinforcement Positive reinforcement has occurred when a response is followed immediately by the presentation of a stimulus and, as a result, similar responses occur more frequently in the future. Figure 2 illustrates the two-term contingency— a response followed closely in time by the presentation of a stimulus— and the effect on future responding that define positive reinforcement. This twoterm contingency is the fundamental building block for the selection of all operant behavior (Glenn, Ellis, & Greenspoon, 1992). The stimulus presented as a consequence and responsible for the subsequent increase in responding is called a positive reinforcer, or, more simply, a reinforcer. Teacher attention in the form of positive comments was the reinforcer that increased Robbie’s study behavior. Cold water flowing into a cup and the sight of a colorful bird are the reinforcers for the two behaviors shown in Figure 2. It is important to remember that a reinforcer does not (and cannot) affect the response that it follows. Reinforcement only increases the frequency with which similar responses are emitted in the future. It is not correct to say the operant reinforcement “strengthens the response which precedes it.” The response has already occurred and cannot be changed. What is changed is the future probability of responses in the same class. It is the operant as a class of behavior, rather than the response as a particular instance, which is conditioned. (Skinner, 1953, p. 87) Skinner (1966) used rate of responding as the fundamental datum for his research on reinforcement. To strengthen an operant is to make it occur more frequently. 2 However, rate (or frequency) is not the only dimension of behavior selected, shaped, and maintained by reinforcement. Reinforcement can also strengthen the duration, latency, magnitude, and/or topography of behavior. For example, if reinforcement only follows those responses that fall within a range of magnitude— that is, above a minimum force but below a maximum force— and responses 2 When the consequence that produced the increase in responding is best described as the termination or withdrawal of an already present stimulus, negative reinforcement has occurred. The fundamental nature and qualifying conditions for positive reinforcement and negative reinforcement are the same. Negative reinforcement is examined in detail in chapter entitled “Negative Reinforcement.”

outside of that range of magnitudes are not followed by reinforcement, the effect will be a higher frequency of responses within that range. Reinforcement contingent on responses meeting multiple criteria will strengthen a subset of responses meeting those criteria (e.g., responses by a golfer practicing 10-foot putts must fall within a narrow range of force and form to be successful). Importance of Immediacy of Reinforcement Emphasizing the importance of the immediacy of reinforcement is essential. The direct effects of reinforcement involve “temporal relations between behavior and its consequences that are on the order of a few seconds” (Michael, 2004, p. 161). Research with nonhumans suggests that at one end of the continuum as much as 30 seconds can elapse without critical loss of effect (e.g., Byrne, LeSage, & Poling, 1997; Critchfield & Lattal, 1993; Wilkenfeld, Nickel, Blakely, & Poling, 1992). However, a response-to-reinforcement delay of 1 second will be less effective than a 0-second delay. This is because behaviors other than the target behavior occur during the delay; the behavior temporally closest to the presentation of the reinforcer will be strengthened by its presentation. As Sidman (1960) described, “If the reinforcer does not immediately follow the response that was required for its production, then it will follow some other behavior. Its major effect will then be upon the behavior that bears, adventitiously to be sure, the closest prior temporal relationship to the reinforcement” (p. 371). Malott and Trojan Suarez (2004) discussed the importance of immediacy as follows: If the reinforcer is to reinforce a particular response, it must immediately follow that response. But how immediate is immediate? We don’t have any experimental data on this one for human beings, but the research on nonverbal animals suggests that a minute or two pushes the limit (even 30 seconds is hard). And if you talk to most behavior analysts working with nonverbal children, they’d agree. They’d quit their jobs if they had to wait 60 seconds before delivering each reinforcer to their children. Such a delay is a good way to ensure that no learning would occur, even with people— at least no desirable learning. So, if you’re trying to reinforce a response, don’t push that 60-second limit. Push the other end— the 0-second end. The direct effect of reinforcement drops off quickly as you increase the delay, even to 3 or 4 seconds. And even a 1-second delay may reinforce the wrong behavior. If you ask a young child to look at you and deliver the reinforcer 1 second after the response, you’re liable to reinforce looking in the wrong direction. So one problem with delayed reinforcement is that it reinforces the wrong response— the one that occurred just before the delivery of the reinforcer. (p. 6) A common misconception is that delayed consequences can reinforce behavior, even if the consequences occur days, weeks, or even years after the responses occurred. “When human behavior is apparently affected by long-delayed consequences, the change is accomplished by virtue of the human’s complex social and verbal history, and should not be thought of as an instance of the simple strengthening of behavior by reinforcement” (Michael, 2004, p. 36). For example, suppose that a piano student practiced dutifully every day for several months in preparation for a statewide competition, at which she received a firstplace award for her solo piano performance. Although some might believe that the award reinforced her persistent daily practice, they would be mistaken. Delayed consequences do not reinforce behavior directly. Delayed consequences can, when combined with language, influence future behavior through instructional control and rule following. A rule is a verbal description of a behavioral contingency (e.g., “Turnip seeds planted by August 15 will yield a crop before a killing freeze”). Learning to follow rules is one way that a person’s behavior can come under the control of consequences that are too delayed to influence behavior directly. A statement by the piano teacher such as “If you practice your assignments every day for one hour between now and the competition, you could win first place” could have functioned as a rule that influenced the piano student’s daily practice. The student’s daily practice was rule governed if daily practice occurred because of her teacher’s rule. 3 The following conditions provide strong indicators that behavior is the result of instructional control or rule following rather than a direct effect of reinforcement (Malott, 1988, Michael, 2004). • No immediate consequence for the behavior is apparent. • The response– consequence delay is greater than 30 seconds. • Behavior changes without reinforcement. • A large increase in the frequency of the behavior occurs following one instance of reinforcement. • No consequence for the behavior exists, including no automatic reinforcement, but the rule exists. Reinforcement Is Not a Circular Concept A commonly held misconception is that reinforcement is the product of circular reasoning and therefore contributes nothing to our understanding of behavior. Circular reasoning is a form of faulty logic in which the name used to describe an observed effect is mistaken as the cause for the phenomenon. This confusion of cause and effect is circular because the observed effect is the sole basis for identifying the presumed cause. In circular reasoning, the suspected cause is not independent of its effect— they are one and the same. Here is an example of circular reasoning that occurs often in education. A student’s persistent difficulties in learning to read (effect) leads to a diagnosis of a learning disability, which is then offered as an explanation for the reading problem: “Paul’s reading problem is due to his learning disability.” How do you know Paul has a learning disability? Because he hasn’t learned to read. Why 3 Excellent discussions of rule-governed behavior can be found in Baum (1994); Chase and Danforth (1991); Hayes (1989); Hayes, Zettle, and Rosenfarb (1989); Malott and Garcia (1991); Malott and Trojan Suarez (2004); Reitman and Gross (1996); and Vaughan (1989). hasn’t Paul learned to read? Because his learning disability has prevented him from learning to read. And around and around it goes. Similarly, it would be circular reasoning if we said that teacher attention increased Robbie’s study behavior because it is a reinforcer. However, the correct use is to say that because Robbie’s study behavior increased when (and only when) it was followed immediately by teacher attention, teacher attention is a reinforcer. The difference is more than the direction of the relation, or a semantic sleight-of-hand. In circular reasoning the suspected cause is not manipulated as an independent variable to see whether it affects the behavior. In circular reasoning such experimental manipulation is impossible because the cause and effect are the same. Paul’s learning disability cannot be manipulated as an independent variable because, as we used the concept in this example, it is nothing more than another name for the dependent variable (effect). Reinforcement is not a circular concept because the two components of the response– consequence relation can be separated, allowing the delivery of a consequence to be manipulated to determine whether it increases the frequency of the behavior it follows. Epstein (1982) described it as follows: If we can show that a response increases in frequency because (and only because) it is followed by a particular stimulus, we call that stimulus a reinforcer and its presentation, reinforcement. Note the lack of circularity. Reinforcement is a term we invoke when we observe certain relations between events in the world. . . . [However,] If we say, for example, that a particular stimulus strengthens a response behavior because it is a reinforcer, we are using the term reinforcer in a circular fashion. It is because it strengthens behavior that we call the stimulus a reinforcer. (p. 4) Epstein (1982) went on to explain the difference between using an empirically demonstrated principle such as reinforcement in a theoretical account of behavior and using a circular argument. In some of his writings, Skinner speculates that certain behavior (for example, verbal behavior) has come about through reinforcement. He may suggest, for example, that certain behavior is strong because it was reinforced. This use of the concept is not circular, only speculative or interpretive. Using the language of reinforcement in this way is reasonable when you have accumulated a large data base. . . . When Skinner attributes some everyday behavior to past reinforcers, he is making a plausible guess based on a large data base and principles of behavior established under controlled conditions. (p. 4) Used properly, reinforcement describes an empirically demonstrated (or speculative, in a theoretical or conceptual analysis) functional relation between a stimulus change (consequence) immediately following a response and an increase in the future frequency of similar responses. Table 1 shows restrictions and examples of appropriate use of the terms reinforcer, reinforcing, reinforcement, and to reinforce suggested by Catania (1998). Box 1 describes four mistakes commonly made when speaking and writing about reinforcement. Reinforcement Makes Antecedent Stimulus Conditions Relevant Reinforcement does more than increase the future frequency of behavior it follows; it also changes the function of stimuli that immediately precede the reinforced behavior. By virtue of being temporally paired with the response-reinforcer contingency, certain antecedent events acquire the ability to evoke (make more likely) instances of the reinforced response class. A discriminative stimulus (S D , pronounced “ess-dee”) is an antecedent stimulus correlated with the availability of reinforcement for a particular response class. Responding in the presence of the S D produces reinforcement, and responding in the S D ’s absence (a condition called stimulus delta [S ∆ , pronounced “ess-delta”]) does not. As a result of this history of reinforcement, a person learns to make more responses in the presence of the S D than in its absence. The behavior is then considered to be under stimulus control . With the addition of the S D , the two-term contingency for reinforcement is transformed to the three-term contingency of the discriminated operant. Figure 3 shows examples of three-term contingencies for positive reinforcement. Assuming that cold water is currently reinforcing and the person has a history of receiving cold water only under blue taps, he is more likely to hold his cup under the blue tap on the cooler (than, say, a red tap). Similarly, assuming that seeing a colorful bird is currently reinforcing and a person has a history of seeing birds more often when looking toward chirping sounds (than, say, other sounds or silence), turning one’s head and looking to the left will occur at a higher frequency when the chirping sound is heard. Reinforcement Depends on Motivation The phrase assuming that cold water is currently reinforcing in the previous paragraph holds another key to understanding reinforcement. Although reinforcement is commonly thought of as a way of motivating people— and it can be— the momentary effectiveness of any stimulus change as reinforcement depends on an existing level of motivation with respect to the stimulus change in question. Motivating operations alter the current value of stimulus changes as reinforcement (Michael 2004). Motivating operations (MOs) are environmental variables that have two effects on behavior: (1) They alter the operant reinforcing effectiveness of some specific stimuli, objects, or events (the value-altering effect); Box 1 Common Mistakes in Talking and Writing about Reinforcement A standard set of technical terms is prerequisite to the meaningful description of any scientific activity. Effectively communicating the design, implementation, and outcomes of an applied behavior analysis depends on the accurate use of the discipline’s technical language. The language of reinforcement includes some of the most important elements of the behavior analyst’s vocabulary. In this box we identify four mistakes made frequently by students of applied behavior analysis when describing reinforcement-based interventions. Perhaps the most common mistake— confusing negative reinforcement with punishment— is not discussed here. Practice refers to the form and manner in which the target skill is emitted (e.g., answering as many math problems as you can in 1 minute). Practicing is a behavior that could be reinforced with various consequences such as a preferred activity (e.g., “Practice solving these math problems; then you can have 10 minutes of free time”). Depending on a learner’s history and preferences, the opportunity to practice a certain skill may function as a reinforcer for practicing another skill (e.g., “Finish your math problems; then you’ll get to do 10 minutes of repeated reading practice”). Reinforcing the Person Although it is proper to speak of presenting a reinforcer to a learner (e.g., “The teacher gave a token to Bobby each time he asked a question”), statements such as, “The teacher reinforced Bobby when he asked a question” and “Chloe was reinforced with praise each time she spelled a word correctly” are incorrect. Behaviors are reinforced, not people. Bobby’s teacher reinforced question asking, not Bobby. Of course, reinforcement acts on and affects the overall person, in that it strengthens behaviors within the person’s repertoire. However, the procedural focus and the primary effect of reinforcement are on the behaviors that it follows. Artificial Reinforcement A distinction between natural and artificial reinforcers is made sometimes, as in this statement, “As the students’ success rates improved, we gradually stopped using artificial reinforcers, such as stickers and trinkets, and increased the use of natural reinforcers.” Some authors have suggested that applications of the principles of behavior result in “artificial control” (e.g., Smith, 1992). A behavior– consequence contingency may be effective or ineffective as reinforcement, but none of its elements (the behavior, the consequence, or the resultant behavior change) is, or can be, artificial. The reinforcement contingencies and stimuli used as reinforcers in any behavior change program are always contrived— otherwise there would be no need for the program— but they are never artificial (Skinner, 1982). The meaningful distinction when talking about reinforcement contingencies is not between the natural and the artificial, but between contingencies that already exist in a given setting prior to a behavior change program and contingencies that are contrived as part of the program (Kimball & Heward, 1993). Although the ultimate effectiveness of a behavior change program may depend on shifting control from contrived to naturally occurring contingencies, there is no such thing as artificial reinforcement. Practice As Reinforcement for a Skill Educators will sometimes say that students should practice a skill because “practicing reinforces the skill.” The phrase poses no problem if the speaker is describing a common outcome of practice with the everyday language connotation of reinforce, as in “to make something stronger” (e.g., to reinforce concrete by embedding steel rods in it). Well-designed drill and practice on a skill usually yields stronger performance in the form of better retention, reduced latency, higher response rates, and/or increased endurance (e.g., Johnson & Layng, 1994; Swanson & Sachse-Lee, 2000). Unfortunately, a phrase such as “practicing reinforces the skill” is often misused and misinterpreted as technical usage of the language of operant conditioning. Although a skill that has been practiced is often stronger as a result of the practice, the practice itself could not be a reinforcer for the behavior practiced. Reinforcement and Feedback As Synonyms Some speakers and writers mistakenly use reinforcement and feedback interchangeably. The two terms refer to different operations and outcomes, though some of each term encompasses parts of the other term’s meaning. Feedback is information a person receives about a particular aspect of his or her behavior following its completion (e.g., “Very good, Kathy. Two quarters equal 50 cents.”). Feedback is most often provided in the form of verbal descriptions of performance, but it can also be provided by other means such as vibration or lights (e.g., Greene, Bailey, & Barber, 1981). Because feedback is a consequence that often results in the increased future frequency of behavior, it sometimes leads to the faulty assumption that reinforcement must involve feedback or that reinforcement is just a behaviorist’s term for feedback. Reinforcement always increases the future frequency of responding. Feedback may result in (a) an increase in the future frequency of the student’s performance as a reinforcement effect and/or as a prompt or instruction on how to respond next time (e.g. “You’re handwriting is improving, Jason, but don’t forget to cross your Ts”), and/or (b) a reduction in the frequency of some aspect of the learner’s performance as a function of punishment or instruction (e.g., “You dropped your elbow on that pitch. Don’t do that.”). Feedback may have multiple effects, increasing one aspect of performance and decreasing another. Feedback may also have no effect on future responding whatsoever. Reinforcement is defined functionally by its effect on future responding; feedback is defined by its formal characteristics (information about some aspect of performance). The operation of either concept is neither necessary nor sufficient for the other. That is, reinforcement may occur in the absence of feedback, and feedback may occur without a reinforcement effect. Sometimes Commonsense Language Is Better The technical language of behavior analysis is complex, and mastering it is no simple matter. Beginning students of behavior analysis are not the only ones who commit terminology errors. Well-trained practitioners, established researchers, and experienced authors also make mistakes now and then when speaking and writing about behavior analysis. Using behavioral concepts and principles— such as positive reinforcement— to confidently explain complex situations involving multiple processes and uncontrolled and unknown variables is a mistake that catches the most attentive and conscientious of behavior analysts at times. Instead of invoking the terminology and concepts of reinforcement to explain the influence of temporally distant consequences on behavior, it is probably wiser to follow Jack Michael’s (2004) advice and simply use everyday descriptive language and commonsense relations. Incorrectly used technical language is worse than common sense language because it suggests that the situation is well understood, and it may displace serious attempts at further analysis. Until we are able to provide an accurate analysis of the various processes relevant to indirect effects [of reinforcement], we are better off using ordinary descriptive language. Thus, say “the successful grant application is likely to encourage future efforts in the same direct,” but don’t say it as though you had the science of behavior behind you. Stop referring to successful settlements of a labor dispute as reinforcement for striking, and successful election of a political candidate as reinforcement for political activity. . . . Don’t talk about good grades as reinforcement for effective study behavior, although they are no doubt responsible for maintaining it in some cases. Just say that they’re responsible for maintaining it. Restraint of this sort will deprive some of us of an opportunity to (incorrectly) display our technical knowledge, but so much the better. (p. 165, emphasis in original) and (2) They alter the momentary frequency of all behavior that has been reinforced by those stimuli, objects, or events (the behavior-altering effect). The value-altering effect, like response-reinforcement delay, is relevant to the effectiveness of the reinforcer at the time of conditioning, and stating that the consequence is a form of reinforcement implies that a relevant MO is in effect and at sufficient strength. (p. 31) In other words, for a stimulus change to “work” as reinforcement at any given time, the learner must already want it. This is a critical qualification in terms of the environmental conditions under which the effects of reinforcement will be seen. Michael (2004) explained this qualification as follows: The behavior-altering effect is relevant to the increased future frequency of the reinforced behavior, and must be added as a third qualification to the operant reinforcement relation: In a given stimulus situation (S) when a type of behavior (R) is followed immediately by reinforcement (S R ) there will be an increase in the future frequency of that type of behavior in the same or similar stimulus conditions, but the increased frequency will only be seen when the MO relevant to the reinforcement that was used is again in effect. (p. 31, emphasis in original) Motivating operations take two forms. An MO that increases the current effectiveness of a reinforcer is called an establishing operation (EO) (e.g., food deprivation makes food more effective as a reinforcer); an MO that decreases the current effectiveness of a reinforcer is an abolishing operation (AO) (e.g., food ingestion reduces the effectiveness of food as a reinforcer). 4

Adding the establishing operation (EO) to a discriminated operant results in a four-term contingency as shown in Figure 4. Spending several hours in a hot and stuffy room without water is an EO that (a) makes water more effective as a reinforcer and (b) increases the momentary frequency of all behaviors that have produced water in the past. Similarly, a park ranger stating prior to a hike that any hiker who describes the coloring of the bird that makes a certain chirping sound will receive a $5 token for the gift shop is an EO that will (a) make seeing a bird that makes the chirping sound effective as reinforcement and (b) increase the frequency of all behaviors (e.g., turning one’s head and looking around) that have produced similar consequences (in this case, seeing the source of sounds) in the past. In plain English, establishing operations (EOs) determine what an individual wants at any particular moment. EOs are dynamic, always changing. The reinforcer value (the want) goes up with increasing levels of deprivation and goes down with levels of satiation. Vollmer and Iwata (1991) demonstrated how the reinforcing effectiveness of three classes of stimuli— food, music, and social attention— varied under conditions of deprivation and satiation. Participants were five adults with developmental disabilities, and the dependent variable was the number of responses per minute on two motor tasks— pressing a switch or picking up small blocks from a container and putting them through the hole in the top of another container. All sessions lasted 10 minutes and began with the experimenter saying, “Do this, [participant’s name],” and modeling the response. During baseline, participants’ responses received no programmed consequences. During the deprivation and satiation conditions, responses were followed by presentation of either food, music, or social attention. Initially each response was followed by the programmed consequence; this gradually shifted to every third, fifth, or tenth response being followed by the consequence. Different procedures were used to create deprivation and satiation conditions for each stimulus class. With food, for example, baseline and deprivation condition sessions were conducted 30 minutes prior to a participant’s scheduled lunchtime; sessions during the satiation condition were conducted within 15 minutes after the participant had eaten lunch. For social attention, baseline and deprivation condition sessions were conducted immediately following a 15-minute period in which the participant had either been alone or had been observed to have had no social interaction with another person. Immediately prior to each session in the satiation condition, the experimenter provided continuous social interaction (e.g., played a simple game, conversation) with the participant for 15 minutes. All five participants responded at higher rates under the deprivation condition than during the satiation condition. Figure 5 shows the effects of deprivation and satiation of social attention on the effectiveness of social attention as a reinforcer for two of the study’s participants, Donny and Sam. Other researchers have reported similar findings concerning the effects of depri- vation and satiation of various stimuli and events as motivating operations that affect the relative effectiveness of reinforcement (e.g., Gewirtz & Baer, 1958; Klatt, Sherman, & Sheldon, 2000; North & Iwata, 2005; Zhou, Iwata, & Shore, 2002).

Automaticity of Reinforcement A reinforcing connection need not be obvious to the individual reinforced. —B. F. Skinner (1953, p. 75) The fact that a person does not have to understand or verbalize the relation between his actions and a reinforcing consequence, or for that matter even be aware that a consequence has occurred, for reinforcement to occur is known as the automaticity of reinforcement. Skinner (1983) provided an interesting example of automaticity in the third and final volume of his autobiography, A Matter of Consequences. He described an incident that took place at a meeting of distinguished scholars who had been invited to discuss the role of intention in political activity. At one point during the meeting, the psychologist Erich Fromm began to argue that “people were not pigeons,” perhaps implying that an operant analysis based on positive reinforcement could not explain human behavior, which is the product of thought and free will. Skinner recounted what happened next: I decided that something had to be done. On a scrap of paper I wrote, “Watch Fromm’s left hand. I am going to shape [reinforce by successive approximations] a chopping motion” and passed it down the table to Halleck [a member of the group]. Fromm was sitting directly across the table and speaking mainly to me. I turned my chair slightly so that I could see him out of the corner of my eye. He gesticulated a great deal as he talked, and whenever his left hand came up, I looked straight at him. If he brought the hand down, I nodded and smiled. Within five minutes he was chopping the air so vigorously that his wristwatch kept slipping out over his hand. (p. 150– 151, words in brackets added) Arbitrariness of the Behavior Selected So far as the organism is concerned, the only important property of the contingency is temporal. —B. F. Skinner (1953, p. 85) No logical or adaptive connection between behavior and a reinforcing consequence is necessary for reinforcement to occur. In other words, reinforcement will strengthen any behavior that immediately precedes it. This arbitrary nature of the behavior selected is critical to understanding reinforcement. All other relations (e.g., what’s logical, desirable, useful, appropriate) must compete with the critical temporal relation between behavior and consequence. “To say that reinforcement is contingent upon a response may mean nothing more than that it followed the response . . . conditioning takes place presumably be286 cause of the temporal relation only, expressed in terms of the order and proximity of response and reinforcement” (Skinner, 1948, p. 168). Skinner (1948) demonstrated the arbitrary nature of the behaviors selected by reinforcement in one of his most famous experimental papers, “‘Superstition’ in the Pigeon.” He gave pigeons a small amount of food every 15 seconds, “with no reference whatsoever to the bird’s behavior” (p. 168). The fact that reinforcement will strengthen whatever behavior it immediately follows was soon evident. Six of the eight birds developed idiosyncratic behaviors “so clearly defined, that two observers could agree perfectly in counting instances” (p. 168). One bird walked counterclockwise around the cage; another repeatedly thrust its head into one of the upper corners of the cage. Two birds acquired a “pendulum motion of the head and body, in which the head was extended forward and swung from right to left with a sharp movement followed by a somewhat slower return” (p. 169). The pigeons had exhibited none of those behaviors at “any noticeable strength” during adaptation to the cage or before the food was periodically presented. Whatever behavior the pigeons happened to be executing when the food hopper appeared tended to be repeated, which made it more likely to be occurring when food appeared the next time. That is, reinforcement was not contingent (in the sense of, dependent) on the behavior; it was only a coincidence that reinforcement sometimes followed the behavior. Such accidentally reinforced behavior is called “superstitious” because it has no influence on whether reinforcement follows. Humans engage in many superstitious behaviors. Sports provide countless examples: A basketball player tugs on his shorts before shooting a foul shot, a golfer carries his lucky ball marker, a batter goes through the same sequence of adjusting his wristbands before each pitch, a college football fan wears a goofy-looking necklace made of inedible nuts to bring good luck to his team. 5 The importance of understanding the arbitrariness of reinforcement goes far beyond providing a possible explanation for the development of harmless superstitious and idiosyncratic behaviors. The arbitrary nature of selection by reinforcement may explain the acquisition and maintenance of many maladaptive and challenging behaviors. For example, a caregiver’s well-meaning social attention provided in an attempt to console or divert a person who is hurting himself may help shape and maintain the very behavior the caregiver is trying to prevent or 5 It is a mistake to assume that all superstitious behavior is the direct result of adventitious reinforcement. Many superstitious behaviors are probably the result of following cultural practices. For example, high school baseball players may wear their caps inside out and backwards when a lateinning rally is needed because they have seen major leaguers don such “rally caps” in the same situation.

eliminate. Kahng, Iwata, Thompson, and Hanley (2000) documented with a functional analysis that social reinforcement maintained the self-injurious behavior (SIB) and aggression of three adults with developmental disabilities. Kahng and colleagues’ data support the hypothesis that aberrant behaviors may have been selected and maintained by social attention because of the arbitrariness of reinforcement. Automatic Reinforcement Some behaviors produce their own reinforcement independent of the mediation of others. For example, scratching an insect bite relieves the itch. Behavior analysts use the term automatic reinforcement to identify a behavior– reinforcement relation that occurs without the presentation of consequences by other people (Vaughan & Michael, 1982; Vollmer, 1994, 2006). Automatic reinforcement occurs independent of social mediation by others. Response products that function as automatic reinforcement are often in the form of a naturally produced sensory consequence that “sounds good, looks good, tastes good, smells good, feels good to the touch, or the movement itself is good” (Rincover, 1981, p. 1). Persistent, nonpurposeful, repetitive self-stimulatory behaviors (e.g., flipping fingers, head rolling, body rocking, toe walking, hair pulling, fondling body parts) may produce sensory stimuli that function as automatic reinforcement. Such “self-stimulation” is thought to be a factor in the maintenance of self-injurious behavior (Iwata, Dorsey, Slifer, Bauman, & Richman, 1994), stereotypic repetitive movements, and “nervous habits” such as hair pulling (Rapp, Miltenberger, Galensky, Ellingson, & Long, 1999), nail biting, chewing on the mouth or lips, and object manipulation such as continually twirling a pencil or fondling jewelry (Miltenberger, Fuqua, & Woods, 1998). The response product that functions as automatic reinforcement may be an unconditioned reinforcer or a once neutral stimulus that, because it has been paired with other forms of reinforcement, has become a conditioned reinforcer. Sundberg, Michael, Partington, and Sundberg (1996) described a two-stage conditioning history that may account for this type of conditioned automatic reinforcement. For example, a person may persist in singing or humming a song while coming home for a movie despite no obvious direct reinforcement for singing. In order for this behavior to occur as automatically reinforced behavior, a special two-stage conditioning history is necessary. In stage one, some stimulus (e.g., a song) must be paired with an existing form of conditioned or unconditioned reinforcement (e.g., an enjoyable movie, popcorn, relaxation). As a result, the new stimulus can become a form of conditioned reinforcement (e.g., hearing the song may now be a new form of conditioned reinforcement). In stage two, the emission of a response (for whatever reason) produces a response product (i.e., the auditory stimuli produced by singing the song) that has topographical similarity to that previously neutral stimulus (e.g., the song), and may now have self-strengthening properties. (pp. 22– 23) Several theorists have suggested that automatic reinforcement may help to explain the extensive babbling of infants and how babbling shifts naturally, without apparent intervention from others, from undifferentiated vocalizations to the speech sounds of their native language (e.g., Bijou & Baer, 1965; Mowrer, 1950; Skinner, 1957; Staats & Staats, 1963; Vaughan & Michael, 1982). Caregivers frequently talk and sing while holding, feeding, and bathing a baby. As a result of repeated pairing with various reinforcers (e.g., food, warmth), the sounds of a caregiver’s voice may become conditioned reinforcers for the baby. The baby’s babbling is automatically reinforced when it produces sounds that match or closely approximate the caregiver’s. At that point, “The young child alone in the nursery may automatically reinforce his own exploratory vocal behavior when he produces sounds that he has heard in the speech of others” (Skinner, 1957, p. 58). Although the idea that automatic reinforcement is a factor in early language acquisition has been proposed for many years, experimental analyses of the phenomenon have appeared in the literature only recently (e.g., Miguel, Carr, & Michael, 2002; Sundberg et al., 1996; Yoon & Bennett, 2000). Sundberg and colleagues (1996) reported the first study showing the effects of a stimulus– stimulus pairing procedure on the frequency with which children emitted new vocal sounds without direct reinforcement or prompts to respond. Five children, ages 2 to 4 and representing a broad range of language abilities, served as subjects. During the prepairing (baseline) condition, the parents and in-home trainers sat a few feet away from the child and recorded each word or vocal sound emitted by the child as he played with a train set and several toys. Data were collected in consecutive 1-minute intervals. The adults did not interact with the subject during the prepairing baseline. The stimulus– stimulus pairing procedure consisted of a familiar adult approaching the child, emitting a target vocal sound, word, or phrase, and then immediately delivering a stimulus that had been established previously as a form of reinforcement for the child (e.g., tickles, praise, bouncing in a parachute held by adults). This stimulus– stimulus pairing procedure was repeated 15 times per minute for 1 or 2 minutes. The adult used a variety of pitches and intonations when voicing the target sound, word, or phrase.

During the postpairing condition, which began immediately after the stimulus– stimulus pairings, the adult moved away from the child and conditions were the same as during the prepairing condition. The stimulus– stimulus pairing of a vocal sound, word, or phrase with an established reinforcer was followed by an increased frequency of the targeted word during the postpairing condition for all five children. Figure 6 shows the results of a representative sample of one of three pairings conducted with Subject 2, a 4-year-old boy with autism. Subject 2 had a verbal repertoire of more than 200 mands, tacts, and intraverbals, but rarely emitted spontaneous vocalizations or engaged in vocal play. 6 During the prepairing condition, the child did not say the target word and emitted four other vocalizations at a mean rate of 0.5 per minute. The stimulus– stimulus pairing procedure consisted of pairing the word apple with tickles approximately 15 times in 60 seconds. Immediately after the pairing, the subject said “apple” 17 times in 4 minutes, a rate of 4.25 responses per minute. In addition, the child said “tickle” four times within the 6 Mands, tacts, and intraverbals— three elementary verbal operants first described by Skinner (1957)—are explained in chapter entitled “Verbal Behavior.” first minute of the postpairing condition. Sundberg and colleagues’ results provide evidence that the children’s vocal response products may have functioned as automatic conditioned reinforcement after being paired with other forms of reinforcement. Kennedy (1994) noted that applied behavior analysts use two meanings of the term automatic reinforcement. In the first instance, automatic reinforcement is determined by the absence of social mediation (Vollmer, 1994; 2006). In the second instance, when functional behavior assessments do not identify a reinforcer for a persistent behavior, some behavior analysts hypothesize that automatic reinforcement is the controlling variable. When SIB occurs in the absence of social attention or any other known form of reinforcement, automatic reinforcement is often assumed to be involved (e.g., Fisher, Lindauer, Alterson, & Thompson, 1998; Ringdahl, Vollmer, Marcus, & Roane, 1997; Roscoe, Iwata, & Goh, 1998). Determining that a behavior may be maintained by automatic reinforcement, and when possible isolating or substituting the source of that reinforcement (e.g., Kennedy & Souza, 1995; Shore, Iwata, DeLeon, Kahng, & Smith, 1997), has important implications or designing interventions to either capitalize on the automatic reinforcing nature of the behavior or counteract it. In summing up the uses and limitations of automatic reinforcement as a concept, Vollmer (2006) suggested that: • Practitioners should recognize that not all reinforcement is planned or socially mediated. • Some behaviors maintained by automatic reinforcement (e.g., self-stimulation, stereotypy) may not be reduced or eliminated with certain procedures (e.g., timeout, planned ignoring, or extinction). • Affixing the label automatic reinforcement to an observed phenomenon too quickly may limit our analysis and effectiveness by precluding further efforts to identify the actual reinforcermaintaining behavior. • When socially mediated contingencies are difficult to arrange or simply not available, practitioners might consider automatic reinforcement as a potential aim. Classifying Reinforcers In this section we review the technical classification of reinforcers by their origin as well as several practical categories by which practitioners and researchers often describe and classify reinforcers by their formal characteristics. The reader should recognize, however, that all reinforcers, regardless of type or classification, are the same in their most important (i.e., defining) characteristic: All reinforcers increase the future frequency of behavior that immediately precedes them. Classification of Reinforcers by Origin There are two basic types of reinforcers by origin— that is, whether a reinforcer is the product of the evolution of the species (an unconditioned reinforcer) or the result of the learning history of the individual (a conditioned reinforcer). Unconditioned Reinforcers A stimulus change that functions as reinforcement even though the learner has had no particular learning history with it is called an unconditioned reinforcer. (Some authors use the terms primary reinforcer and unlearned reinforcer as synonyms for unconditioned reinforcers.) Because unconditioned reinforcers are the product of the evolutionary history of a species (phylogeny), all biologically intact members of a species are more or less susceptible to reinforcement by the same unconditioned reinforcers. For example, food, water, oxygen, warmth, and sexual stimulation are examples of stimuli that do not have to undergo a learning history to function as reinforcers. Food will function as an unconditioned reinforcer for a human deprived of sustenance; water will function as an unconditioned reinforcer for a person deprived of liquid, and so forth. Human touch may also be an unconditioned reinforcer (Gewirtz & Pelaez-Nogueras, 2000). PelaezNogueras and colleagues (1996) found that infants preferred face-to-face interactions that included touch stimulation. Two conditioning treatments were implemented in alternated counterbalanced order. Under the touch condition, infants’ eye-contact responses were followed immediately by adult attention (eye contact), smiling, cooing, and rubbing the infants’ legs and feet. Eye-contact responses during the no-touch condition were followed by eye contact, smiles, and coos from the adult, but no touching. All of the babies in the study emitted eye contact for longer durations, smiled and vocalized at higher rates, and spent less time crying and protesting in the contingent condition that included touch. From these results and several related studies, PelaezNogueras and colleagues concluded that “these results suggest that . . . touch stimulation can function as a primary reinforcer for infant behavior” (p. 199). Conditioned Reinforcers A conditioned reinforcer (sometimes called a secondary reinforcer or learned reinforcer ) is a previously neutral stimulus change that has acquired the capability to function as a reinforcer through stimulus– stimulus pairing with one or more unconditioned reinforcers or conditioned reinforcers. Through repeated pairings, the previously neutral stimulus acquires the reinforcement capability of the reinforcer(s) with which it has been paired. 7 For example, after a tone has been paired repeatedly with food, when food is delivered as a reinforcer, the tone will function as a reinforcer when an EO has made food a currently effective reinforcer. Neutral stimuli can also become conditioned reinforcers for humans without direct physical pairing with another reinforcer through a pairing process Alessi (1992) called verbal analog conditioning. For example, a class of preschool children who have been receiving M&M candies for good school work might be shown pieces of cut up yellow construction paper and told, “These pieces of yellow paper are what big kids work for” (Engelmann, 1975, pp. 98– 100). Many children in the group immediately refuse M&Ms, and work extra hard, but accept only pieces of yellow paper as their rewards. We might say that the pieces of yellow paper act as “learned reinforcers.” Laboratory research tells us that neutral stimuli become reinforcers only through direct pairing with primary reinforcers (or other “learned reinforcers”). Yellow paper was not paired with any reinforcer and certainly not with the primary (M&Ms) reinforcers. Yellow paper acquired reinforcing properties even more powerful than the primary M&Ms reinforcers, as demonstrated by the children’s refusal to accept M&Ms, demanding instead pieces of yellow paper. (For the sake of this example, assume that the children had not been satiated with M&Ms just before the session.) (p. 1368) occurrence. A conditioned reinforcer is called a generalized conditioned reinforcer because it is effective as reinforcement across a wide range of EO conditions. Because of their versatility across EO conditions, generalized conditioned reinforcers offer great advantages for practitioners, who often have limited control of the EOs for particular reinforcers. Generalized conditioned reinforcers provide the basis for implementing a token economy, a reinforcement-based system capable of improving multiple behaviors of multiple participants (e.g., Higgins, Williams, & McLaughlin, 2001; Phillips, Phillips, Fixen, & Wolf, 1971). In a token economy, participants receive tokens (e.g., points, check marks, poker chips) contingent on a variety of target behaviors. Participants accumulate the tokens and exchange them at specific times for their choices from a menu of backup reinforcers (e.g., free time, computer time, snacks). It is sometimes thought that the “power” of a conditioned reinforcer is determined by the number of times it has been paired with other reinforcers. However, a statement such as, “The more often the tone is paired with food, the more reinforcing the tone will become” is not completely accurate. Although numerous pairings will increase the likelihood that the tone will function as a conditioned reinforcer in the first place (though a single pairing is sometimes sufficient), the momentary effectiveness of the tone as a reinforcer will be a function of the relevant EO for the reinforcer(s) with which the conditioned reinforcer has been paired. A tone that has been paired only with food will function as an effective reinforcer for a food-deprived learner, but the tone will have little effect as a reinforcer if the learner has just consumed a lot of food, regardless of the number of times it has been paired with food. A generalized conditioned reinforcer is a conditioned reinforcer that as a result of having been paired with many unconditioned and conditioned reinforcers does not depend on a current EO for any particular form of reinforcement for its effectiveness. For example social attention (proximity, eye contact, praise) is a generalized conditioned reinforcer for many people because it occurs simultaneously with many reinforcers. The more reinforcers with which a generalized conditioned reinforcer has been paired, the greater is the likelihood that it will be effective at any given time. Because it can be exchanged for a nearly limitless variety of backup reinforcers, money is a generalized conditioned reinforcer whose effectiveness is usually independent of current establishing operations. It is sometimes thought that a conditioned reinforcer is called a generalized conditioned reinforcer because it can function as reinforcement across a wide range of behaviors. But this is not so— any reinforcer is capable of strengthening any behavior that immediately precedes its Classification of Reinforcers by Formal Properties When applied behavior analysts describe reinforcers by their physical properties— a practice that can enhance communication among researchers, practitioners, and the agencies and people they serve— reinforcers are typically classified as edible, sensory, tangible, activity, or social. Edible Reinforcers Researchers and practitioners have used bites of preferred foods, snacks, or candy, and sips of drinks, as reinforcers. One interesting and important use of edibles as reinforcers is in the treatment of chronic food refusal in children. For example, Riordan, Iwata, Finney, Wohl, and Stanley (1984) used “highly preferred food items” as reinforcers to increase the food intake of four children at a hospital treatment facility. The treatment program consisted of dispensing the high-preference food items (e.g., cereal, yogurt, canned fruit, ice cream) contingent on the consumption of a target food item (e.g., vegetables, bread, eggs). Edible reinforcers were also used by Kelley, Piazza, Fisher, and Oberdorff (2003) to increase cup drinking by Al, a 3-year-old boy who had been admitted to a day treatment program for food refusal and bottle dependency. The researchers measured the percentage of trials in which Al consumed 7.5 ml of three different liquids from the cup. During baseline, when Al was praised if he consumed the drink, his consumption averaged 0%, 44.6% and 12.5% of trials for orange juice, water, and a
chocolate drink, respectively. During the positive reinforcement component of the cup-drinking intervention, each time Al consumed the drink the therapist praised him (as was done in baseline) and delivered a level spoon of peaches (a preferred food) to his mouth. Al consumed all three beverages on 100% of the trials during the positive reinforcement condition. Sensory Reinforcers Various forms of sensory stimulation such as vibration (e.g., massager), tactile stimulation (e.g., tickles, strokes with a feather boa), flashing or sparkling lights, and music have been used effectively as reinforcers (e.g., Bailey & Meyerson, 1969; Ferrari & Harris, 1981; Gast et al., 2000; Hume & Crossman, 1992; Rincover & Newsom, 1985; Vollmer & Iwata, 1991). Tangible Reinforcers Items such as stickers, trinkets, school materials, trading cards, and small toys often serve as tangible reinforcers. An object’s intrinsic worth is irrelevant to its ultimate effectiveness as a positive reinforcer. Virtually any tangible item can serve as a reinforcer. Remember Engelmann’s (1975) kindergarten students who worked for yellow slips of paper! Activity Reinforcers When the opportunity to engage in a certain behavior serves as reinforcement, that behavior may be called an activity reinforcer. Activity reinforcers may be everyday activities (e.g., playing a board game, leisure reading, listening to music), privileges (e.g., lunch with the teacher, shooting baskets in the gym, first in line), or special events (e.g., a trip to the zoo). McEvoy and Brady (1988) evaluated the effects of contingent access to play materials on the completion of math worksheets by three students with autism and behavior disorders. During baseline, the teacher told the students to complete the problems as best that they could, and that they should either complete other unfinished assignments or “find something else to do” if they finished the worksheets before a 6-minute timing elapsed. No other prompts or instructions were given for completing the worksheets. The teacher praised the completion of the worksheets. On the first day of intervention for each student, he was taken to another room and shown a variety of toys and play materials. The teacher told the student he would have approximately 6 minutes to play with the materials if he met a daily criterion for completing math problems. Figure 7 shows the results. During baseline, the rate at which all three students correctly completed problems was either low (Dicky) or highly variable (Ken and Jimmy). When contingent access to the play activities was introduced, each student’s completion rate increased and eventually exceeded criterion levels. Premack (1959) hypothesized that activity reinforcers can be identified by looking at the relative distribution of behaviors in a free operant situation. Premack believed that behaviors themselves could be used as reinforcers and that the relative frequency of behavior was an important factor in determining how effective a given behavior might be as a reinforcer if the opportunity to engage in the behavior is contingent on another behavior. The Premack principle states that making the opportunity to engage in a behavior that occurs at a relatively high free operant (or baseline) rate contingent on the occurrence of low-frequency behavior will function as reinforcement for the low-frequency behavior. For a student who typically spends much more time watching TV than doing homework, a contingency based on the Premack principle (informally known as “Grandma’s Law”) might be, “When you have finished your homework, you can watch TV.” Building on Premack’s concept, Timberlake and Allison (1974) proposed the response-deprivation hypothesis as a model for predicting whether access to one behavior (the contingent behavior) will function as reinforcement for another behavior (the instrumental response) based on the relative baseline rates at which each behavior occurs and whether access to the contingent behavior represents a restriction compared to the baseline level of engagement. Restricting access to a behavior presumably acts as a form of deprivation that serves as an EO, thus making the opportunity to engage in the restricted behavior an effective form of reinforcement (Allison, 1993; Iwata & Michael, 1994). Iwata and Michael (1994) cited a series of three studies by Konarski and colleagues as demonstrating the veracity and applied implications of the responsedeprivation hypothesis. In the first study, when students were given access to coloring (a high-probability behavior contingent on completing math problems (a lowprobability behavior), they spent more time doing math, but only if the reinforcement schedule represented a restriction of the amount of time spent coloring compared to baseline (Konarski, Johnson, Crowell, & Whitman, 1980). The researchers found that a contingency in which students could earn more time coloring than they did in baseline for completing math problems was ineffective. These basic findings were reproduced in a subsequent study in which access to reading (or math, depending on the subject) was contingent on math (or reading) (Konarski, Crowell, Johnson, & Whitman (1982). In the third study, Konarski, Crowell, and Duggan (1985) took the response-deprivation hypothesis a step further by examining the “reversibility of reinforcement” within subjects; that is, engaging in either of two activities— reading or math— could serve as reinforcement for increased performance in the other activity, in a response-deprivation condition for the contingent activity. Response deprivation for writing as the contingent response resulted in increases in math (instrumental response); conversely, response deprivation for math as the contingent response produced increases in reading. In all three studies, response restriction was the key factor in determining whether access to the contingent response would be reinforcing. Iwata and Michael (1994) concluded that the collective results of Konarski and colleagues’ studies illustrate each of three predictions based on the responsedeprivation hypothesis (assume the ratio of baseline rates of doing homework to watching TV is 1:2 in the following examples): • Reinforcement of a low-rate target behavior when access to a high-rate contingent behavior is restricted below baseline levels (e.g., 30 minutes of homework gets access to 30 minutes of TV). • Nonreinforcement of a low-rate behavior when access to a high-rate contingent behavior is not restricted below baseline levels (e.g., 30 minutes of homework gets access to 90 minutes of TV). • Reinforcement of a high-rate target behavior when access to the low-rate behavior is restricted below baseline levels (e.g., 30 minutes of TV yields 5 minutes of homework). Although recognizing that practitioners seldom design reinforcement programs to increase the rate of behaviors such as TV watching that already occur at high rates, Iwata and Michael (1994) noted that: There are a number of instances in which one may wish to produce highly accelerated performance (e.g., as in superlative academic or athletic performance that is good to begin with). In such cases, one need not find another activity that occurs at a higher rate to serve as reinforcement if one could arrange a suitable deprivation schedule with an activity that occurs at a relatively low rate. (p. 186) As with all other descriptive categories of reinforcers, there is no a priori list that reveals what activities will or will not function as reinforcers. An activity that serves as effective reinforcement for one learner might have quite another effect on the behavior of another learner. For example, in Konarski, Crowell, and colleagues’ (1982) study, access to math functioned as reinforcement for doing more reading for three students, whereas getting to read was the reinforcer for completing math problems for a fourth student. Many years ago, a classic cartoon brought home this crucial point very well. The cartoon showed two students dutifully cleaning the chalkboard and erasers after school. One student said to the other, “You’re cleaning erasers for punishment!? I get to clean erasers as a reward for completing my homework.” Social Reinforcers Physical contact (e.g., hugs, pats on the back), proximity (e.g., approaching, standing, or sitting near a person), attention, and praise are examples of events that often serve as social reinforcers. Adult attention is one of the most powerful and generally effective forms of reinforcement for children. The nearly universal effects of contingent social attention as reinforcement has led some behavior analysts to speculate that some aspects of social attention may entail unconditioned reinforcement (e.g., Gewirtz & Pelaez-Nogueras, 2000; Vollmer & Hackenberg, 2001). The original experimental demonstrations and discovery of the power of adults’ social attention as reinforcement for children’s behavior took place in a series of four studies designed by Montrose Wolf and carried out by the preschool teachers at the Institute of Child Development at the University of Washington in the early 1960s (Allen, Hart, Buell, Harris, & Wolf, 1964; Harris, Johnston, Kelly, & Wolf, 1964; Hart, Allen, Buell, Harris, & Wolf, 1964; Johnston, Kelly, Harris, & Wolf, 1966). Describing those early studies, Risley (2005) wrote: We had never seen such power! The speed and magnitude of the effects on children’s behavior in the real world of simple adjustments of something so ubiquitous as adult attention was astounding. Forty years later, social reinforcement (positive attention, praise, “catching them being good”) has become the core of most American advice and training for parents and teachers— making this arguably the most influential discovery of modern psychology. (p. 280) Because of the profound importance of this longknown but underused phenomenon, we describe a second study showing the effects of contingent attention as reinforcement for children’s behavior. The first volume of the Journal of Applied Behavior Analysis included no fewer than seven studies building on and extending Wolf and colleagues’ pioneering research on social reinforcement. 8 R. Vance Hall and colleagues conducted two of those studies. Like the Hall, Lund, and Jackson (1968) study, from which we selected the example of a teacher’s use of positive reinforcement with Robbie that introduced this chapter, the three experiments reported by Hall, Panyan, Rabon, and Broden (1968) continue to serve as powerful demonstrations of the effects of teacher attention as social reinforcement. A first-year teacher whose class of 30 sixth-graders exhibited such high rates of disruptive and off-task behaviors that the school principal described the class as “completely out of control” participated in one of the experiments. Throughout the study, Hall, Panyan, and colleagues (1968) measured teacher attention and students behavior during a continuous 30-minute observation period in the first hour of the school day. The researchers used a 10-second partial-interval observation and recording procedure to measure study behavior (e.g., writing the assignment, looking in the book, answering the teacher’s question) and nonstudy behavior (e.g., talking out, being out of seat, looking out the window, fighting or poking a classmate). The observers also recorded the occurrence of teacher attention in each interval. Each instance of teacher verbal attention, defined as a comment directed to a student or group of students, was recorded with a “+” if it followed appropriate study behavior, and with a “ Ϫ ” if it followed nonstudy behavior. During baseline the class had a mean percentage of intervals of study behavior of 44%, and the teacher made an average of 1.4 comments following study behavior per session (see Figure 8). “Almost without exception those [comments] that followed study behavior were approving and those that followed nonstudy behavior were in the form of a verbal reprimand” (Hall, Panyan et al., 1968, p. 316). The level of study behavior by the class was 90% on one day when the helping teacher presented a demonstration lesson (see data points marked by solid arrow). On three occasions during baseline (data points marked by open arrows), the principal met with the teacher to discuss his organizational procedures in an effort to improve the students’ behavior. These counseling 8 The first volume of the Journal of Applied Behavior Analysis (1968) is a treasure trove of classic studies in which simple and elegant experimental designs revealed the powerful effects of operant conditioning and contingency management. We strongly encourage any serious student of applied behavior analysis to read it from cover to cover.
sessions resulted in the teacher writing all assignments on the board (after the first meeting) and changing the seating chart (after the third meeting). Those changes had no apparent effect on the students’ behavior. Prior to the first day of the reinforcement condition, the teacher was shown baseline data on the class study behavior and the frequency of teacher attention following study behavior. The teacher was instructed to increase the frequency of positive comments to students when they were engaged in study behavior. After each session during this condition the teacher was shown data on the level of class study behavior and the frequency of his comments that followed study behavior. During the first reinforcement phase, teacher comments following study behavior increased to a mean frequency of 14.6, and the mean level of study behavior was 72%. The teacher, principal, and data collectors reported that the class was under better control and that noise had decreased significantly. During a brief return of baseline conditions, the teacher provided “almost no reinforcement for study behavior,” and a sharp downward trend in class study behavior was observed. The teacher, principal, and data collectors all reported that disruptive behavior and high noise levels had returned. The reinforcement conditions were then reinstated, which resulted in a mean frequency of 14 teacher comments following study behavior, and a mean level of 76% of intervals of study behavior. Identifying Potential Reinforcers In the laboratory, we had learned to use a simple test: Place a candy in the palm of our hand, show it to the child, close our fist fairly tightly around the candy, and see if the child will try to pull away our fingers to get at the candy. If he or she will do that, even against increasingly tightly held fingers, the candy is obviously a reinforcer. —Murray Sidman (2000, p. 18) The success of many behavior change programs requires an effective reinforcer that the practitioner or researcher can control. Fortunately, identifying effective and accessible reinforcers for most learners is relatively easy. Sidman (2000) described a quick and simple method for determining whether candy would likely function as a reinforcer. However, every stimulus, event, or activity that might function as a reinforcer cannot be held in the palm of the hand. Identifying robust and reliable reinforcers for many learners with severe and multiple disabilities poses a major challenge. Although many common events serve as effective reinforcers for most people (e.g., praise, music, free time, tokens), these stimuli may not serve as reinforcers for all learners. Time, energy, and resources would be lost if planned interventions were to fail because a practitioner used a presumed, instead of an actual, reinforcer. Also, reinforcer preferences shift, and the transitory nature of preference has been reported repeatedly in the literature (Carr, Nicholson, & Higbee, 2000; DeLeon et al., 2001; Kennedy & Haring, 1993; Logan & Gast, 2001; Ortiz & Carr, 2000; Roane, Vollmer, Ringdahl, & Marcus, 1998). Preference assessments may change with the person’s age, interest level, time of day, social interactions with peers, and the presence of certain establishing operations (EOs) (Gottschalk, Libby, & Graff, 2000). What a teacher asks in September to determine preferences may have to be repeated a month later (or sooner). Likewise, a therapist who asks a client what is reinforcing during a morning session may find that this stimulus is not stated as a preferred item in an afternoon session. After reviewing 13 published studies that evaluated preferences and reinforcers for people with profound multiple disabilities, Logan and Gast (2001) concluded that preferred stimuli do not always function as reinforcers, and preferred stimuli at one point in time changed later. Additionally, people with severe-to-profound developmental disabilities may engage in activities for such a limited time that it is difficult to clearly determine whether a stimulus change is a reinforcer. To meet the challenge of identifying effective reinforcers, researchers and practitioners have developed a variety of procedures that fall under the twin headings of stimulus preference assessment and reinforcer assessment. Stimulus preference assessment and reinforcer assessment are often conducted in tandem, as described by Piazza, Fisher, Hagopian, Bowman, and Toole (1996): During preference assessments, a relatively large number of stimuli are evaluated to identify preferred stimuli. The reinforcing effects of a small subset of stimuli (i.e., the highly preferred stimuli) are then evaluated during reinforcer assessment. Although the preference assessment is an efficient procedure identifying potential reinforcers from a large number of stimuli, it does not evaluate the reinforcing effects of the stimuli. (pp. 1– 2) Stimulus preference assessment identifies stimuli that are likely to serve as reinforcers, and reinforcer assessment puts the potential reinforcers to a direct test by presenting them contingent on occurrences of a behavior and measuring any effects on response rate. In this section we describe a variety of techniques developed by researchers and practitioners for conducting stimulus preference assessments and reinforcer assessment (see Figure 9). Together these methods form a continuum of approaches ranging from simple and quick to more complex and time-consuming. Stimulus Preference Assessment Stimulus preference assessment refers to a variety of procedures used to determine (a) the stimuli that the person prefers, (b) the relative preference values of those stimuli (high preference versus low preference), and (c) the conditions under which those preference values change when task demands, deprivation states, or schedules of reinforcement are modified. Generally speaking, stimulus preference assessment is usually conducted using a two-step process: (1) A large pool of stimuli that might be used as reinforcers is gathered, and (2) those stimuli are presented to the target person systematically to identify preference. It is essential for practitioners to narrow the field of possible stimuli to those that have good odds of functioning as reinforcers. In more specific terms, stimulus preference assessments can be conducted using three basic methods: asking the person (or his or her significant others) to identify preferred stimuli; observing the person interacting or engaging with various stimuli in a free operant situation; and measuring the person’s responses to trial-based tests of paired or multiply presented stimuli. In choosing which method to use, practitioners must balance two competing perspectives: (a) gaining the maximum amount of preference assessment data within the least amount of time, but without false positives (i.e., believing a stimulus is preferred when it is not), versus (b) conducting a more timeand labor-intensive assessment that will delay intervention, but may yield more conclusive results. activities, but also special items and activities. Is simply a sheet with numbered lines on which family members identify potential rewards they would like to earn by completing tasks on contingency contracts. • Choice format. This format could include asking questions such as the following: “Which would you do a lot of hard work to get? Would you rather get things to eat, like chips, cookies, popcorn, or get to do things, like art projects, play computer games, or go to the library?” (Northup, George, Jones, Broussard, & Vollmer, 1996, p. 204) • Rank-ordering. The learner can be given a list of items or stimuli and instructed to rank-order them from most to least preferred. For learners with limited language skills, pictures of items, icons, or, preferrably, the actual stimuli can be presented. For example, a teacher, while pointing to an icon, might ask a student, “Do you like to drink juice, use the computer, ride the bus, or watch TV?” Students simply nod yes or no. Surveys have been developed to assess students’ preferences. For example, elementary school teachers might use the Child Reinforcement Survey, which includes 36 rewards in four categories: edible items (e.g., fruit, popcorn), tangible items (e.g., stickers), activities (e.g., art projects, computer games), and social attention (e.g., a teacher or friend saying, “I like that”) (Fantuzzo, Rohrbeck, Hightower, & Work, 1991). Other surveys are the School Reinforcement Survey Schedule for students in grades 4 through 12 (Holmes, Cautela, Simpson, Motes, & Gold, 1998) and the Reinforcement Assessment for Individuals with Severe Disabilities (Fisher, Piazza, Bowman, & Almari, 1996). Although asking for personal preferences is relatively uncomplicated, the procedure is not foolproof with respect to confirming that a preferred choice will later serve as a reinforcer. “Poor correspondence between verbal self-reports and subsequent behavior has been long noted and often demonstrated” (Northup, 2000, p. 335). Although a child might identify watching cartoons as a preferred event, watching cartoons may function as a reinforcer only when the child is at home on Saturday mornings, but not at Grandma’s house on Sunday night. Further, surveys may not differentiate accurately between what children claim to be high-preference and lowpreference items for reinforcers. Northup (2000) found that preferences of children with attention-deficit/hyperactivity disorder (ADHD) did not rise beyond chance levels when survey results were later compared to reinforcer functions. “The relatively high number of false positives and low number of false negatives again suggest that surveys may more accurately identify stimuli that are not Asking about Stimulus Preferences A person’s preference for various stimuli might be determined by merely asking what she likes. Asking can greatly reduce the time needed for stimulus preference assessment, and it often yields information that can be integrated in an intervention program. Several variations of asking exist: asking the target person, asking significant others in the person’s life, or offering a pretask choice assessment. Asking the Target Person. A straightforward method for determining stimulus preference is to ask the target person what he likes. Typical variations include asking open-ended questions, providing the person with a list of choices or asking him to rank-order a list of choices. • Open-ended questions. Depending on the learner’s language abilities, an open-ended assessment of stimulus preference can be done orally or in writing. The person may be asked to name preferences among general categories of reinforcers— for example, What do you like to do in your free time? What are your favorite foods and drinks? Are there any types of music or performers whose music you like? An open-ended assessment can be accomplished simply by asking the learner to list as many favorite activities or items as possible. She should list not only everyday favorite things and activities, but also special items and activities.
It simply needs to ask numbered lines on which family members identify potential rewards they would like to earn by completing tasks on contin-gency contracts.

Choice format. The format could include asking
questions such as the following: "Which would
you do a lot of hard work to get? Would you rather
get things to eat, like chips, cookies, popcorn, or
go to dinner, like art projects, play computer
games, or go to the library?" (Northup, George,
Jones, Broussard, & Vollmer, 1996).

Rank-ordering. The learner can be given a list of
items or stimuli and instructed to rank order them
from most to least preferred.

For learners with advanced language skills, pictures of
items, icons, or, preferably, the actual stimuli can be pre-
sented. For example, a teacher, while pointing to an icon,
might ask, "Do you like to drink juice, use the
computer, ride the bus, or watch TV?" Students simply
point to or verbally state their preferences.

Surveys have been developed to assess students'
preferences. For example, elementary school teachers
might use the Child Reinforcement Survey, which in-
cludes 36 rewards in three categories: edible items (e.g.,
fruit, popcorn), tangible items (e.g., stickers), activities
(e.g., art projects, computer games), and social attention
(e.g., teacher praise) (Fantuzzo, Rohrbeck, Hightower,
& Work, 1991). Other surveys are the School Reinforce-
ment Survey Schedule for students in grades 4 through 12
(Cautela, Stroopson, Mates, & Gold, 1998) and the
Reinforcement Assessment for In-
dividuals with Severe Disabilities (Fisher, Piazza, Bow-
man, & Amari, 1996).

Although asking for personal preferences is relatively
uncomplicated, the procedure is not foolproof with re-
spect to students accurately identifying what will serve
as a reinforcer. Poor correspondence between verbal
self-reports and subsequent behavior has been long noted
and often demonstrated" (Northup, 2000, p. 335). Al-
though a child might identify watching cartoons as a pre-
ferred event, watching cartoons may function as a
reinforcer only when the child is at home on Saturday
morning, but not at grandma's house on Sunday night.
Further, survey data may not differentiate accurately be-
tween children with ADHD and others with respect to
preference items for reinforcers. Northup (2000) found
that preferences of children with attention-deficit/hyper-
activity disorder (ADHD) did not use beyond chance lev-
els low numbers of false negatives were seen compared to
functioning. The relatively high number of false positives
and low number of false negatives were seen suggest that sur-
veys may more accurately identify stimuli that are not

reinforcers than those that are" (p. 337). Merely asking children their preferences once might lead to false positives (i.e., children may choose an event or stimulus as a reinforcer, but rarely find it reinforcing). Taking Significant Others. A pool of potential reinforcers can be obtained by asking parents, siblings, friends, or caregivers to identify the activities, items, foods, hobbies, or toys that they believe the learner prefers. For example, the Reinforcer Assessment for Individuals with Severe Disabilities (RAISD) is an interview protocol that asks caregivers to identify preferred activities, foods, toys, and stimuli across multiple sensory and social domains (Fisher et al., 1996). Significant others also rank-order the selected preferences based on likely high- versus low-preference items. Finally, significant others are asked to identify the conditions under which they predict that specific items might function as reinforcers (e.g., cookies will still work, just cookies alone). Again, although stimuli that are identified as highly preferred by caregivers often are not always effective as reinforcers, they often are. Offering a Direct Choice. In this method the practitioner asks the person to choose what she wants to earn for doing a task. The participant then chooses one item from two or three offered presented items. The items offered as choices in these contingent task choices will have been identified as preferred stimulus in an other assessment procedure. For instance, a teacher might make the following statement: "Robyn, when you finish your math problems, you may have 10 minutes to play Battleship with Martin, read quietly, or help Ms. Green prepare the social studies paper. Which activity do you want to work for?" A learner's choice of a consequence will not necessarily be a more effective reinforcer than one selected by the researcher or practitioner (Smith, Iwata, & Shore, 1995). Free Operant Observation. The principle that a person engages in most often when free to choose from among behaviors may serve as effective reinforcers when made contingent on engaging in low-probability behaviors. Observers begin by recording what activities the target person engages in when she can choose among a period of unrestricted access to numerous activities is called free operant observation. A total duration measure of the time the person engages with each stimulus item or activity is recorded. The longer the person engages with an item, the more likely the item is reinforcing. Procedurally, the person has unconstrained and simultaneous access to a predetermined set of items or activities to the materials and activities that are naturally available to the environment. There are no response requirements, and all stimulus items are available and within the person's sight and reach. An item is never removed. Five operant observation lasts for 5 to 10 minutes. Care (2010) free operant responding is less likely to produce aberrant behavior that might otherwise be observed if a stimulus is removed. Free operant observations can be conducted or conducted in immediate settings. Controlled Free Operant Observation. Practitioners use controlled observation to determine whether, when, and for how long a learner to which the person engages with each of a predetermined set of activities and materials. This observation is controlled because the researcher or practitioner determines and limits the number and type of items that may be of interest to the learner. Free operant assessment presupposes that the person has had sufficient time to move about and explore the environment and has had an adequate experience each of the stimuli, materials, or activities. Just prior to the free operant observation period, the learner is provided brief instructions and exposure to all items. All of the items are then placed within view and easy access of the person who then has approximately 5 minutes to choose among them freely. Observers record the total duration of time that the learner engages with each stimulus item or activity. Naturalistic Free Operant Observation. Naturalistic observation of free operant responding are conducted in the learner's everyday environment (e.g., playground, classroom, home). An undergraduate in Mike's class observed him during his daily routine at home and records the number of minutes the learner devotes to each activity. For instance, Figure 10 shows how a teenager, Mike, distributed his time during 2 hours of free time each day after school Mike's parents collected these data by keeping a chart of the total number of minutes their son was engaged in each activity. The summary chart for the week shows that Mike played computer video games, watched television, and listened to music on his personal CD player most often during this time Mike spent 10 minutes reading a library book, and just played with a new construction toy for a brief time on Wednesday. Two activities—watching television and playing video games—occupied the most often and for the longest duration. If Mike's parents wanted to apply the Premack principle introduced earlier in this chapter to increase the amount of time he spends reading his textbooks, they might consider using Mike's probability behaviors). they might make watching television or playing video games (i.e., high-probability behaviors) contingent on a certain amount of time spent leisure reading or playing with the construction toy. The many variations of trial-based stimulus preference assessment can be grouped by presentation formats such as single stimulus (successive choice), paired stimuli (forced choice), and multiple stimuli. Single Stimulus. A single-stimulus presentation method, also called the successive choice method, represents the most basic assessment available for determining preference. Simply stated, a single stimulus is presented and the person's reaction to it is noted. Presenting single stimulus at a time "may be well suited for individuals who have difficulty selecting among two or more stimuli" (Hagopian et al., 2001, p. 247). These stimuli across all sensory systems (i.e., visual, auditory, vestibular, tactile, olfactory, gustatory, and multimodal) are presented to the person one at a time sequentially and the person's reaction to each stimulus is recorded (Baum, Jacobs et al., 2001; Pace et al., 1985). Approach or rejection responses are recorded in terms of occurrence (e.g., frequency = number of touches per minute), or duration (i.e., time spent engaged with an item). After recording, the next item in the sequence is presented. For example, a mirror might be presented to determine the duration of time the person gaze into the mirror, touches it, picks it up, etc., pushes it away). Each item/stimulus would be presented several times, and the order of presentation should be varied. Paired Stimuli. In trial in the paired-stimuli presentation method, also sometimes called the "forced choice" method, consists of the simultaneous presentation of two stimuli. The observer records which of the two stimuli the learner chooses. During the course of the assessment, each stimulus is paired approximately equally with another stimulus in the series (e.g., Fisher et al., 1992). Data from all paired stimuli are scored to show how many times each stimulus is chosen. The stimuli are then rank-ordered in terms of high, medium, or low preference. Piazza and colleagues (1996) used 56 to 120 paired-stimuli trials to determine high, middle, and low preference. Pace and colleagues (1985) found that paired-stimuli presentations.

yielded more accurate distinctions between high- and low-preference items than did single-stimulus presentations. Paired-stimuli sometimes outperform single-stimulus presentations formats with respect to ultimately identifying reinforcers (Paclawskyj & Vollmer, 1995). Because every possible pair of stimuli must be presented, paired-stimuli assessing may take more time than the simultaneous presentation of an array of multiple stimuli. However, DeLeon and Iwata (1996) and Fisher and Iwata (1996) argue that although the paired stimuli method may be more efficient because "the more consistent results produced by the PS method may indicate that stable preference can be determined in fewer, even single, sessions" (p. 520). Multiple Stimuli. The multiple-stimuli presentation method is an extension of the paired-stimuli procedure developed by Fisher and colleagues (1992). The person chooses a preferred stimulus from an array of multiple stimuli presented simultaneously (Windsor et al., 1994). By presenting multiple stimuli together, assessment time is reduced. For example, instead of presenting a series of trials consisting of all possible pairs of stimuli from a group of six stimuli and continuing until all pairs have been presented, all six stimuli are presented simultaneously. The two major variations of the multiple-stimuli preference assessment are multiple stimuli with replacement and multiple stimuli without replacement. The difference between the two is which stimuli are or are not replaced after the person indicates a preference among the displayed items in preparation for the next trial. In the multiple stimuli with replacement procedure, the item chosen by the learner remains in the array and items that were not selected are replaced with new items. In the multiple stimuli without replacement procedure, the chosen item is removed from the array, the order or placement of the remaining items is randomized, and the next trial begins with a reduced number of items in the array. In some cases with trial begins by asking the person "Which one do you want the most?" (Hughes et al., 2000) "Choose one" (Graff & Alhearn, 2004) and then continuing until all items from the original array, or be gradually reducing array, have been selected. The entire sequence is usually repeated several times, although a single round of trials may identify stimuli that function to reinforce behavior. The stimuli presented in each trial might be tangible objects themselves, pictures of the items, or verbal descriptions. Hughes, Carte, and Harrison (1995) provided a variation of the multiple-stimuli procedure that included stimulus preference selection based on a tangible object versus a picture of the object. The tangible object assessment produced greater variation and distribution of preference than the picture objects did. Graff, Almeda, Graft, and Alhearn (2000) found that the tangible object assessment took about as twice as long verbal preference assessment, but the clients completed the verbal preference assessment in one session. DeLeon and Iwata (1996) used an adaptation of the multiple stimuli and paired stimulus presentations they referred to as free operant observation, with replacement was needed to determine stimulus preference. Basically, in the brief stimulus assessment, once a particular stimulus item is chosen, that item is not returned to the array. Subsequent trials used a reduced number of items from which to choose (Carr et al., 2000; DeLeon et al., 2001; Roane et al., 1998). DeLeon and Iwata (1996) found that multiple stimuli without replacement identified preferred items in approximately half the time of the multiple stimuli with replacement method (see also Ortiz & Carr 2000). Wilson colleagues (2000), "With a brief stimulus preference procedure, practitioners have a method for reinforcer identification that is both efficient and accurate" (pp. 72-73). Guidelines for Selecting and Using Stimulus Preference Assessments. Practitioners can combine assessment procedures to compare single versus paired, paired versus multiple, or free operant versus trial-based methods (Ortiz & Carr, 2000). In day-to-day practice, brief stimulus presentations using comparative approaches might facilitate reinforcer identifications, thereby speeding up possible interventions using those reinforcers. In summary, the goal of stimulus preference assessment is to identify items that are likely to function as reinforcers. Each method for assessing preference has advantages and limitations with respect to identifying preferences (Roane et al., 1998). Practitioners may find the following guidelines helpful when conducting stimulus preference assessments (DeLeon & Iwata, 1996; Govic-Sacks et al., 2000; Hughes et al., 2000; Ortiz & Carr, 2000; Roane et al., 1998; Roane, Iwata, & Kang, 2997): • Monitor the learner's activities during the time period before the stimulus preference assessment session to be aware of EOs that may affect the results. • Use stimulus preference assessment options that balance the cost-benefit of brief assessments (but possible false positives) with more prolonged assessments that may delay reinforcer identification. • Balance using a stimulus preference method that may yield a ranking of preferred stimuli against an assessment method that saves assessment ranking.

but occurs more frequently, to counteract shifts in preference. • When time is limited, conduct a brief stimulus preference assessment with fewer items in an array. • When possible, combine data from multiple assessment methods and sources (e.g., stimulus preference (e.g., asking the learner and significant others, free operant observation, paired choice, and trial-based methods). Reinforcer Assessment. The only way to tell whether or not a given event is reinforcing for a given organism under given conditions is to make a direct test. -B. F. Skinner (1953, pp. 72-73) Highly preferred stimuli may not always function as reinforcers (Graff et al., 2000). Even the small child prior out of Skinner's data may not have functioned as reinforcement under certain conditions. Conversely, least preferred stimuli might serve as reinforcers under some conditions (Gottschalk et al., 2000). The only way to know for sure whether a given stimulus will be a reinforcer is to present it immediately following the occurrence of a behavior and note its effects on responding. Researchers working with different organisms under different methods used to present one or more stimuli contingent on a target response and then measuring future effects on the rate of responding. Researchers have developed reinforcement assessment methods to determine the relative effects of a given stimulus as reinforcement under different and changing conditions and to assess the comparative effectiveness of multiple stimuli as reinforcers for a given behavior under specific conditions. Reinforcer assessments could be conceptualized as testing stimuli suspected of being reinforcers contingent on responding within concurrent, multiple, or progressive-ratio reinforcement schedules. Concurrent Schedule Reinforcer Assessment. When two or more contingencies of reinforcement operate independently and simultaneously on the same behavior a concurrent schedule of reinforcement is in effect. When used as a vehicle for reinforcer assessment, a concurrent schedule arrangement essentially pits two stimuli against each other to see which will produce the larger increase in responding when presented as a consequence for responding. If a learner allocates a greater proportion of responses to one component of the concurrent schedule over the other, the stimulus used as a contingent consequence in that component is the more effective reinforcer. For example, using a concurrent schedule in this way shows the relative effectiveness of highly preferred (HP) versus low preference (LP) stimuli as reinforcers (Koehler, Iwata, Roscoe, Rolider, & O'Steen, 2005; Piazza et al., 1996). Concurrent schedules may also be used to determine differences between relative and absolute reinforcement effects of stimuli. That is, will an LP stimulus now presented contingently in the absence of the HP stimulus serve as a reinforcer? Roscoe and colleagues (1999) used concurrent schedules to compare the effects of HP and LP stimulus reinforcers on the behavior of three participants in all conditions. Following the preference assessments, a concurrent schedule of reinforcement was established using the high-preference and low-preference stimuli. The target response was pressing either of two microswitch panels. Each panel was a different color. Pressing a panel would illuminate a small light in the center of the panel. A training condition took place prior to baseline to establish panel pressing in the subject's repertoire and to eliminate any position bias. During the initial baseline condition neither panel resulted in a programmed consequence. During the reinforcement phase, an HP stimulus was placed on a plate behind one of the panels and an LP stimulus was placed on a plate behind another panel. All responses to either panel resulted in the participant immediately receiving the item on the plate behind the respective panel (i.e., an FR 1 schedule of reinforcement). During this arrangement schedule of reinforcement that enabled participants to choose reinforcers on the same FR 1 schedule, the majority of participants allocated most of their responding to the panel that produced the HP stimulus as reinforcement (e.g., see results for Sean, Peter, and Matt on Figure 11). However, these same participants, when later presented with the opportunity to obtain LP stimuli as reinforcers via a single-schedule contingency (i.e., only one panel FR1), showed LP stimuli obtained when the HP stimulus in the concurrent schedule. The study by Roscoe and colleagues (1999) demonstrated how concurrent schedules may be used to identify the relative effects of stimuli as reinforcers. The study also showed that the potential effects of a stimulus as a reinforcer may be masked or overshadowed when that stimulus is pitted against another stimulus on a concurrent schedule. In such cases, a potentially reinforcing stimulus might be abandoned prematurely. schedule and on a fixed-time schedule (i.e., response independent) in the other component. For example, if a practitioner wanted to use a multiple schedule to assess whether social attention functioned as a reinforcer, she would provide social attention contingent on responses during one component of the multiple schedule (e.g., response-dependent) in effect, and during the other component the practitioner would present the same amount and kind of social attention except on a fixed-time schedule, in dependent of cooperative play (i.e., noncontingent reinforcement). The teacher could apply the response-dependent schedule during the morning play period, and the response-independent schedule during the afternoon play period. If social attention functioned as a reinforcer under the cooperative play would likely show higher baseline rate in the morning periods, and because of no relationship with cooperative play, attention would likely have no effect in the afternoon period. This situation follows a multiple schedule because it reflects two class of behavior (i.e., cooperative play), a discriminative stimulus for each contingency in effect (i.e., morning and afternoon play periods), and different conditions for reinforcement (i.e., response-dependent and response in dependent). Progressive-Ratio Schedule Reinforcer Assessment. Stimulus preference assessments with low response requirements (e.g., FR 1) may not predict the effectiveness of the stimulus as a reinforcer when presented with higher response requirements (e.g., on an FR 10 schedule, a student must complete 10 problems to obtain reinforcement). (DeLeon, Iwata, Goh, and Worsdell (1997) noted: Current assessment methods may make inaccurate predictions about reinforcer efficacy when the task used in treatment requires significant effort or when the delivery of reinforcement... for some classes of reinforcers, simultaneous increases in absolute requirements may suggest small differences in preferred that are undetected when requirements are low. In such cases, a stimulus preference assessment might give a response requirement (FR) schedule when one exists under increased response requirements. (pp. 440, 446). Progressive-ratio schedules provide a framework for assessing the relative effectiveness of stimulus as reinforcers. Under progressive-ratio schedule of reinforcement the response requirements for reinforcement are increased systematically, over time independent of the participant's behavior. In a progressive-ratio schedule, the practitioner gradually requires more responses per presentation of the  

preferred stimulus until a breaking point is reached and the response rate declines (Roane, Lerman, & Vorndran, 2001). For example, initially each response produces reinforcement (FR 1), then reinforcement is delivered after every second response (FR 2), then perhaps after every fifth, tenth, and twentieth response (FR 5, FR 10, and FR 20). At some point, a preferred stimulus may no longer function as reinforcement (Tustin, 1994). DeLeon and colleagues (1997) used a progressive ratio within a concurrent schedule to test the relative effectiveness of two similarly preferred stimuli (e.g., cookie and cracker) and two dissimilar stimuli (e.g., drink and balloon) as reinforcers for micro switch panel pressing for Elaine and Rick, two adults with mental retardation. One panel was blue and one was yellow. The experimenters placed two reinforcers on separate plates, and put one plate behind each of the panels. Each trial (24 per session for Rick; 14 per session for Elaine) consisted of the subject pushing either one of the panels and immediately receiving the item on the plate behind that panel. During the first phase, an FR 1 schedule was used (i.e., each response produced the item on the plate). Later, the response requirement for obtaining the items was gradually increased (FR 2, FR 5, FR 10, and FR 20). Elaine and Rick made responses that produced the two dissimilar items at roughly the same rates during the FR 1 phase (see the top two graphs in Figure 12). As response requirements for receiving the dissimilar stimuli increased, Elaine and Rick continued to evenly allocate responding between the two panels. However, when initially equivalent and similar reinforcers (food in FR 1) were compared under increasing schedule requirements, the differences in responses rates on the two panels revealed clear and consistent preferences (see the bottom two graphs in Figure 12). For example, when Elaine needed to work more to receive food, she allocated the majority of her responses to the panel that produced chips rather than the one that produced pretzels. In the same way, as the number of responses required to receive reinforcement increased, Rick showed a clear preference for cookies over crackers. These results suggest that "for some classes of reinforcers, simultaneous increases in schedule requirements may magnify small differences in preference that are undetected when requirements are low" (DeLeon et al., 1997, p. 446). Progressive response requirements within a concurrent schedule also may reflect the effects of increasing response requirements on the choice between reinforcers and also reveal under what conditions two reinforcers are substitutable for each other. If two reinforcers serve the same function (i.e., are made effective by the same establishing operation), an increase in the price (i.e., response requirement) for one of the reinforcers will lead to allocation of responding to the other if a substitute reinforcer is available (Green & Freed, 1993). DeLeon and colleagues (1997) used a hypothetical person with a slight preference for Coke over Pepsi as an analogy to explain the results shown in Figure 12. Assuming that Coke and Pepsi are both available for $1.00 per serving and that a person has only a slight preference for Coke, the individual may allocate choices rather evenly, perhaps as a function to substitute situation for the preferred item, but with slightly more overall occurrences of Coke. Now assume that the cost of each is increased to $2.00 per serving. This slight overall preference for Coke is likely to be expressed. By contrast, a similar arrangement involving Coke and hot chocolate may produce different results. Again, at $1.00 per unit, roughly equal selection between the two options would occur between these items that are momentarily equally available. However, these items serve distinctly different functions and are not substitutable; that is, the person is not free to trade one for the other and is continually motivated to obtain both items in order to maintain their separate functions. The person is more likely to continue choosing both items, even when the price for both stimuli(costs) changes substantially. The same might be said for the results obtained in the present study. When choices involved two dissimilar items, such as a cookie and cracker, concurrent availability resulted in equivalent access because the expression of slight preference for one of the items. However, when reinforcers that were initially to be substitutes, such as cookie and massager, were concurrently available and equally preferred, increases in cost revealed their relative effects as preference. (pp. 446-447). Although Stimulus X and Stimulus Y may each function as reinforcers when task demands are low, or when the reinforcement schedule is dense, when the task demands increase or when the schedule becomes leaner (i.e., more responses required per reinforcement, participants may choose only Stimulus Y. DeLeon and colleagues (1997) pointed out that practitioners who are alert to these relationships might be more skeptical in believing that reinforcement effects will be sustained under those more vigourous conditions, and judicious in how they plan reinforcement delivery relative to task assignments once some understanding emerges. They thought more refined analyses of these types of preferred stimuli when task demands are high rather than substituting item for other equally preferred stimuli when task demands are low. Control Procedures for Positive Reinforcement. Positive reinforcement control procedures are used to manipulate the contingent presentation of a potential reinforcer to assess its effects on the target response frequency of behavior. Control, as the term is used here, requires an experimental demonstration that the presentation of a stimulus contingent on the occurrence of a target response functions as positive reinforcement. Control is demonstrated by comparing response rates in the absence and presence of contingencies, and then showing that with the absence and presence of contingencies, the process can be turned on and off (e.g., Rand & Blair, Wolf, & Kitzinger, 1968). Historically, researchers and practitioners have used the reversal technique as the major control technique for positive reinforcement. Briefly, the reversal technique includes two conditions and a minimum of four phases (i.e., ABAB). In the A condition, the behavior is measured over time until it achieves stability in the absence of the reinforcement contingency. The absence of the control measure is the reinforcement contingency. In the B condition that reinforcement contingency is introduced to demonstrate the effects of the stimulus change. The presence of the reinforcement contingency is the experimental condition. If the rate of responding increases in the presence of the contingency, the analyst then withdraws the reinforcement contingency and returns to the A and B conditions to learn whether the absence and presence of the contingency gives without more the target behavior down and up. However, varying conditions as the control condition during baseline may be contraindicated for three potential problems. First, withdrawing reinforcement may result in unwanted side effects (e.g., an initial increase in response rate, emotional responses, aggression) that affect the demonstration of control. Second, in some situations it may be impossible to withdraw the reinforcement contingency completely (Thompson & Iwata, 2005). For example, it is unlikely that a teacher could completely prevent a student from contacting

during the A condition. In addition to these problems, Thompson and Iwata (2005) noted that although extinction has often been successful in reversing the behavioral effects of positive reinforcement, it too is a control procedure presents unsatisfactory difficulties. Extinction does not always work under the temporal parameters of applied research because reinforcement of target responses, because mere stimulus presentation cannot be ruled out as an equally viable explanation. (p. 261, emphasis added). According to Thompson and Iwata (2005), "the ideal control procedure for positive reinforcement eliminates the contingent relation between the occurrence of the target response and the presentation of the stimulus while controlling for the effects of stimulus presentation" (p. 259). They reviewed the effectiveness of three variations of the reversal technique as control procedures for determining reinforcement effects: noncontingent reinforcement (NCR), differential reinforcement of other behavior (DRO), and differential reinforcement of alternative behavior (DRA). Noncontingent Reinforcement. Non-contingent reinforcement (NCR) is the presentation of a potential reinforcer on a fixed-time (FT) or variable-time (VT) schedule independent of the occurrence of the target behavior. The control procedure using NCR for the potential reinforcer eliminates the contingent relation between the target behavior and the stimulus presentation while allowing any effects of the stimulus presentation alone to be detected. Thus, whereas Thompson and Iwata (2005) criteria for an ideal control procedure for positive reinforcement. The NCR reversal technique should entail a minimum of four phases (AB/BC): A is a baseline condition, B is an NCR condition in which the potential reinforcer is presented on a fixed or variable-interval schedule independent of the target behavior, and C is a condition in which the potential reinforcer is presented contingent on the occurrence of the target behavior. The B and C conditions are then repeated to learn whether the level of responding decreases and increases as a function of the absence and presence of the response-consequence contingency. The quality, amount, and rate of reinforcement should be approximately equal in the B and C conditions. If this is the case, only contingent B and C conditions can evoke persistent responding. Whatever the cause, persistent responding is a limitation of the NCR control procedure because unless achieving a reversal effect (reduced responding) is more time-consuming than the reversal technique with extinction. Achieving the effect may require lengthy contact with the NCR schedule. Differential Reinforcement of Other Behavior. If a practitioner uses differential reinforcement of other behavior (DRO) delivers a positive reinforcer whenever the target behavior has not occurred during a set time interval. The DRO reversal technique includes a minimum of four phases (i.e., AB/BC): A is a baseline condition, B is a reinforcement condition in which the potential reinforcer is presented contingent on the occurrence of the target behavior, and C is a DRO condition in which the potential reinforcer is presented contingent on the absence of the target behavior. The analyst may repeat the B and C conditions to determine whether the level of responding decreases and increases as a function of the absence and presence of the response-consequence contingency. The DRO schedule allows for the continued presentation of the reinforcement contingency during the reversal phase of the control procedure. In one condition, the contingency is active for the occurrence of the target behavior. In another condition, the contingency is active for the omission of the target behavior. The DRO control procedure may produce the reversal effect in less time than the NCR schedule, perhaps because of the elimination of accidental reinforcement of the target behavior. Differential Reinforcement of Alternative Behavior. When differential reinforcement of an alternative behavior (DRA) is used as a control condition, a potential reinforcer is presented contingent on occurrences of a designated alternative to the target behavior. The DRA reversal technique includes a minimum of five phases (i.e., AB/BC): A is a baseline condition, B is a reinforcement condition in which the potential reinforcer is presented contingent on the occurrence of the target behavior, and C is a condition in which the potential reinforcer is presented contingent only on the occurrence of an alternative behavior. The analyst may repeat the B and C conditions to whether the level of responding decreases and increases as a function of the absence and presence of the response-consequence contingency. Thompson and Iwata (2005) summarized the limitations of using DRO and DRA as control conditions procedures to test for positive reinforcement.

(DRO and DRA) introduce a new contingency that was not present in the original reinforcement arrangement. As a result, reductions in target response under a contingency reversal might be attributed to either (a) elimination of the reinforcer or (b) introduction of reinforcement for the absence of the target response (or for the occurrence of a competing response). In addition, given that reinforcement is provided contingent on time characteristics or responses that produce reinforcement (DRA), the behavior may not quickly succeed (DRO) or reallocated toward responses that produce reinforcement (DRA). In fact, the reinforcement in the control condition may be low relative to the rate of reinforcement in the experimental condition. Therefore, this experimental and control condition. If responding is not quickly reduced (DRO) or reallocated toward strategy is functionally similar to the conventional extinction procedure. (p. 267)

Using Reinforcement Effectively. Most practitioners issue guidelines for applying positive reinforcement effectively. These guidelines come from three main sources: the research literature of the experimental analysis of behavior, applied behavior analysis and our personal experience.

Set an Easily Achieved Initial Criterion for Reinforcement. A common mistake in applying reinforcement is setting the initial criterion for reinforcement too high, which prohibits the learner's behavior from contacting the reinforcement contingency. An essential strategy practitioners should establish an initial criterion so that the participant's first responses produce reinforcement, and then increase the criterion for reinforcement gradually as performance improves. Hersen (1990) suggested the following method for establishing initial criteria for reinforcement based on the learner's level of responding during baseline (see Figure 13).

Use High-Quality Reinforcers of Sufficient Magnitude. Reinforcers that maintain responding on simple tasks may not have the potency to produce similar levels of responding on more difficult or longer tasks. Practitioners will likely need to use a reinforcer of higher quality for behaviors that require more effort or endurance. A highly preferred stimulus, chosen during preference assessments, sometimes functions as a high-quality reinforcer. Neef and colleagues (1992), for example, found that behaviors that received a lower reinforcer rate but a higher quality reinforcer increased in frequency, whereas behavior that received a higher reinforcer rate but a lower quality reinforcer decreased in frequency. Reinforcer quality is related also to other consequences for responding currently available in the learner.

Applied behavior analysts define the magnitude (or amount) of a reinforcer as (a) the duration of time for access to the reinforcer, (b) the number of reinforcers per unit of time (i.e., reinforcer rate), or (c) the intensity of the reinforcer. Increases in reinforcer magnitude may correlate with the effectiveness of the contingent response-reinforcer relation. However, the effects of reinforcer magnitude are not well understood because few applied studies have examined the effects of magnitude on responding in a single-operant arrangement (Lerman, Kelly, Vorndran, Kuhn, & LaRue, 2002, p. 30). Consideration of how much reinforcement we should follow the maxim "the most." Manifest abundantly, but don't give away the store. We suggest that the amount of reinforcement be proportional to the quality of the reinforcer and the effort required to emit the target response.

Use Varied Reinforcers to Maintain Potent Establishing Operations. Reinforcers often decrease in potency with frequent use. Presenting an overabundance of a specific reinforcer is likely to diminish the momentary effectiveness of the reinforcer due to satiation. Practitioners can minimize satiation effects by using a variety of reinforcers. If reading comic books is a highly preferred activity, alternating different genres of comics may help reduce satiation.

For a behavior you wish to increase, set the initial criterion higher than the child's average baseline performance and lower than his best performance during baseline. For a behavior you want to decrease in frequency, the initial criterion for reinforcement should be set below the child's average performance during baseline and greater than or equal to his lowest (next) baseline performance. (p. 7)

Conversely, known reinforcers that are not always available may have increased effectiveness when they are introduced. If a teacher has demonstrated that "being first in line" is a reinforcer, but uses this reinforcer only once per week, the reinforcement effect will be greater than if "being first in line" is used frequently.

A variety of reinforcers may also help preferred stimuli to function as reinforcers. For example, Bowman et al. (1997) found that some learners responded better to a variety of less preferred stimuli as compared to a continuous access to a single, more highly preferred stimulus. Also, using a variety of reinforcers may keep the potency of any one particular reinforcer higher. For example, Hall (1981) found that students' correct responding and ontask behavior were higher when they had access to one of three randomly selected snacks in each session than in a reinforcement condition in which one of the stimuli was presented following each successful trial. Even within a session, teachers could let students select a variety of consequence from a menu. Similarly, varying a property of a reinforcer may keep its reinforcing potency for a longer time. If comic books are used as reinforcers, having several different genres of comic books available is likely to maintain their potency.

Use Direct Rather than Indirect Reinforcement Contingencies When Possible. With a direct reinforcement contingency, emitting the target response produces direct access to the reinforcer. The contingency does not require any intervening steps. With an indirect reinforcement contingency, the response does not produce reinforcement directly. The practitioner presents the reinforcer. Some research suggests that direct contingencies may produce better results than indirect ones (Doğan & Williams, 1980; Williams, Koegel, & Bell, 1981). Thompson and Iwata (2000), for example, linked the definitions of direct and indirect contingencies to the difference between automatic reinforcement (i.e., direct) and socially mediated reinforcement (i.e., indirect) and summarized their research on response acquisition under direct and indirect contingencies in the following way:

Under both contingencies, completion of identical task topographies (any of several types of container) produced access to identical E. However, under the direct contingency, the reinforcer was placed inside the container to be opened; under the indirect contingency, the therapist held the reinforcer and delivered it to the participant upon task completion. One participant immediately performed the task under the direct contingency, but required more trials under the indirect contingency. Three participants showed either more rapid acquisition of task completion under the direct under the direct contingency. The remaining two participants showed improved performance only under the direct reinforcement contingency. Data taken on the occurrence of "irrelevant" behaviors under the indirect contingency and on the time spent between trials while performing the tasks provided more evidence that these behaviors may have interfered with task performance and that their occurrence was a function of differential stimulus control. (p. 1)

Where possible, practitioners should use direct reinforcement contingency, especially with learners with limited behavioral repertoires.

Combine Response Prompts and Reinforcement. Response prompts are supplementary antecedent stimuli used to occasion a correct response in the presence of an SD that will eventually control the behavior. Applied behavior analysts give response prompts before or during the performance of a target behavior. The three major forms of response prompts are verbal instructions, modeling, and physical guidance.

Concerning verbal instructions, sometimes describing the response in detail with verbal instructions may function as a motivating operation for learners with verbal skills, thereby, making it more likely that the learner will more quickly contact the reinforcer. For example, Mayfield and Chase (2002) explained their reinforcement contingency to college students who were learning five basic algebra rules:

The reinforcement procedures were described to the participants in the general instructions administered at the beginning of Session 1: "From this test on you will be penalized for correct answers on all the test, and not penalized for incorrect answers. During the session following blank participants were presented with a record of their trial earnings on the test. This was the only feedback provided concerning their performance on the test. (p. 111)

Horner, Vollmer, and Rapp (2004) used verbal response prompts during an assessment of the vocal verbal metal responses of three participants with autism. Each vocalization assessment session consisted of 10 trials, each 1 min in duration. A nonspecific prompt (the experimenter's name) was presented at 0, 20, and 40 s into start this, sit down fit it?) 10 s after the choice of the trial. Prompt including a model of the target vocal utterance (e.g., "If you want this, say 'chip'") was delivered 20 s into the trial. The participant was prompted to give any verbal response after the presentation of the targeted response if 50 s. If no want this, say ..." at the end of the trial.

Reinforce Each Occurrence of the Target Behavior Initially. Provide reinforcement for each occurrence of the target behavior (i.e., continuous reinforcement) initially. This rule is particularly true during the initial stages of learning a new behavior. After the behavior is established, gradually thin the rate of reinforcement so that some but not all occurrences of the behavior are reinforced (i.e., intermittent reinforcement). For example, a teacher might initially reinforce each correct response to addition problems printed on flash cards and then use a ratio schedule to thin reinforcement. To help responses shift from initial learning, provide reinforcement following the correct responses for a few trials, then reinforce each set of four correct responses, and so on. Hartley and colleagues (2001) gradually moved from a very dense fixed interval (FI 1 second) schedule of reinforcement to an FI schedule, the first target response following the end of the interval produced reinforcement) to an FI schedule with the following increments of intervals: 2 s, 4 s, 8 s, 16 s, 25 s, 35 s, and finally to an FI 58 seconds. For example, target responses that occurred before the end of the FI 58 seconds were not reinforced, but the first response after 58 seconds was reinforced.

Use Contingent Attention and Descriptive Praise. As discussed earlier in this chapter, social attention and praise are powerful reinforcers for many learners. However, behavioral improvements following praise often involve something more, or altogether different from, the direct effects of positive reinforcement. Let's look at the common conceptual mistake of assuming that increased responding following social attention or a description of the behavior is always a function of reinforcement.

Consider the common use of descriptive praise, involving some general sort of social approval (a mild plus some comment such as "Good boy") and, in addition, a brief description of the behavior that is responsible for praise ("I like the way you tie ..."). When such praise is provided immediately following the behavior, it probably functions as a form of information as a rule, much as if the praise had said, "If you want my continued approval you have to ...". For example, a factory supervisor walks up to an employee who is cleaning up an oil spill on the factory floor, smiles broadly, and says, "George, I really like the way you're cleaning up that oil spill before anyone steps in it. That's very considerate of you." Now suppose that George cleans up spills from that time on. Was it rather larger change in behavior considering that it was followed by only a single instance of reinforcement and might one not use praise as reinforced nor simply as reinforcement but rather as a form of rule or instruction, and that George, for various reasons, provided himself with similar instruction every time another spill occurred. (pp. 164-165, emphasis in original)

A study by Goetz and Baer (1973) investigating the effects of teacher praise on preschool children's creative play with building blocks used descriptive praise in one condition of the study. The teacher remarked with interest, enthusiasm, and delight every time that the child placed and/or rearranged the block, or as to create a form that had not been produced previously in that day ("That's different!", "Oh, that's a new one!", "That's a different!") The authors noted that such praise increased the rate function of block form diversity during each phase of contingent descriptive praise. Goetz and Baer did not conduct a component analysis to determine how much of the gain in improved performance could be attributed to reinforcement in the form of positive attention ("That's very nice!") vs to the feedback they presented ("That's different!") which carried information about what to follow ("Building different things with the blocks gets the teacher's attention"). The authors examined this.

for some children, either [receiving attention and descriptive praise] might be be more or less effective than the other for others, and for still other children, the mix of the two will be more effective than either alone. If so, then the applied progress a package of positive attention and descriptive praise is probably the best technique to apply to children in general. (p. 216, words in brackets added)

We recommend that in the absence of data showing that attention and praise have produced counter therapeutic effects on a given learner, practitioners incorporate contingent praise and attention into any intervention entailing positive reinforcement.

Gradually Increase the Response-to-Reinforcement Delay. We recommended in the previous guideline that practitioners reinforce each occurrence of a target behavior during the initial stages of learning, and then thin the delivery of reinforcers by switching to an intermittent schedule of reinforcement. Because the consequences that maintain responding in natural environments are often delayed, Stromer, McComas, and Rehfeldt (2000) reminded us that using continuous and intermittent schedules of reinforcement might be just the first steps of programming a behavioral response to everyday situations: "Establishing the initial instance of a behavioral repertoire typically involves the use of reinforcement contingencies that occur immediately after the target response occurs. However, the role of the applied behavior analyst also involves the strategic use of delayed reinforcement behavior than can yield delayed reinforcement are highly adaptive in everyday life, but they may be difficult to establish and maintain" (p. 359).

Examples of the tactics that applied behavior analysts use to help people learn to respond effectively under delayed reinforcement include (a) beginning with a reinforcement time interval that begins with a short delay and is then gradually increased till the extent of the delay (Dixon, Rehfeldt, & Randich, 2003; Schweitzer & Sulzer-Azaroff, 1988), (b) a gradual increase in work requirements during the delay (Dixon & Holcomb, 2000); (c) an activity during the delay to "bridge the gap" between the target behavior and reinforcer (Skinner, DeBeer, & Greer, 1972); and importantly, (d) verbal assistance in the form of rules and/or other prompts to maintain responding during the delay (e.g., "The calculator will show the amount of money to be placed in a savings account for you. You will be given all the nickels in your savings account on [day]." (Neef, Mace, & Shade, 1993, p. 59).

Gradually Shift from Continuous to Naturally Occurring Reinforcers. We end this chapter with an extract from Murray Sidman's (2000) delightful and thought-provoking account of what he learned in the "early days" of applying behavioral principles to human behavior. In describing a project from 1965 to 1972 that emphasized the use of positive reinforcement with boys between the ages of 15 and 20 years who were diagnosed with mental retardation and living in a state institution, Sidman recollected how introducing tokens as generalized conditioned reinforcers eventually led to praise from the project staff:

[Moving from continuous reinforcement to an intermittent schedule of reinforcement is sometimes described as a means of generating resistance to extinction. "Continuous reinforcement" is shorthand for "a continuous schedule of reinforcement" but was ruled "intermittent reinforcement" more accurately implied the more economical use of time than was implied the more economical use of time than was implied by continuous reinforcement.] "Schedules of Reinforcement" a reinforcement is delivered immediately after the response but given less response required. For example, you fixed interval (FI) schedule of reinforcement specifies that you will deliver reinforcement after a designated time interval. Tasks-to-reinforcement or response cost delay describes the reinforcement delivered on an interval or ratio schedule of reinforcement, but the contingency has been held constant throughout both the stimulus and their response.

and later to learning itself, becoming powerful reinforcers for the boys. We began with tokens, which had the advantage of being visible and easily handled. Later, after the boys had learned to save tokens and to understand numbers, we were able to introduce points. For some, points led only to enjoy learning, and as the boys told us, they were when they earned the tokens and points that brought them other reinforcers, our pleasure also became important to them, and we became able to use praise as a reinforcer. A boy learned more and more.

Summary
Definition and Nature of Positive Reinforcement

Positive reinforcement is nominally defined by a two-term contingency: A response is followed immediately by the presentation of a stimulus, and, as a result, similar responses occur more frequently in the future.
The stimulus change responsible for the increase in responding is called a reinforcer.
The importance of the immediacy of reinforcement must be emphasized; a response-to-reinforcement delay of just 1 second can diminish measurable effects because the behavior-consequence check of the presentation of the reinforcer will be strengthened by its presentation.
The effects of long-delayed consequences on human behavior should not be attributed to the direct effect of reinforcement.
A misconception held by some is that reinforcement is a circular concept. Circular reasoning is a form of illogic in which cause and effect are confused and independent of each other. Reinforcement is not a similar concept because the two components of the response-consequence relation can be separated and the consequence manipulated to determine if it increases the frequency of the behavior it follows.
In addition to increasing the future frequency of the a target behavior, a reinforcement changes the function of antecedent stimuli. As antecedent stimuli that evoke behavior because it has been correlated with the availability of reinforcement is called a discriminative stimulus (SD).
A discriminative operant is defined by a three-term contingency of SD → R → SR+
The momentary effectiveness of any stimulus change as reinforcement depends on an existing level of motivation with respect to that stimulus change. An establishing operation (EO) (e.g., deprivation) increases the current effectiveness of some reinforcers; an abolishing operation (AO) (e.g., satiation) decreases the effectiveness of a reinforcer.
A complete description of reinforcement of a discriminated operant entails a four-term contingency: EO → SD → R → SR+
Automaticity of reinforcement refers to the fact that a person does not have to understand or be aware of the relation between his behavior and a reinforcing consequence for reinforcement to occur.
Reinforcement strengthens any behavior that immediately precedes it; the logical or pragmatic connection between the behavior and the reinforcement consequence is not necessary.
The development of operations reveals that reinforcement prior when reinforcement is presented on a fixed-time schedule (irrespective of the subject's behavior) demonstrates the arbitrary nature of the behavior selected by reinforcement.
Automatic reinforcement occurs when behavior produces their own reinforcement independent of the mediation of others.
Classifying Reinforcers
14. Unconditioned reinforcers are stimuli that function as reinforcement without requiring a learning history. These stimuli are the product of phylogenic development, meaning that all members of a species are susceptible to the same properties of stimuli.
15. Conditioned reinforcers are previously neutral stimuli that function as reinforcers as a result of being paired with one or more other reinforcers.
16. A generalized conditioned reinforcer is a conditioned reinforcer that has been paired having been paired with many unconditioned and conditioned reinforcers does not depend on a current EO for any particular form of reinforcement for its effectiveness.
17. When reinforcers are described by their physical properties, they are typically classified as edible, sensory, tangible, activity, or social reinforcers.
18. The Premack principle states that making the opportunity to engage in a high-probability behavior contingent on the occurrence of low-frequency behavior will function as reinforcement for the low-frequency behavior.

The response-deprivation hypothesis is a model for predicting whether contingent access to one behavior will function as reinforcement for engaging in another behavior based on whether access to the contingent behavior represents a restriction of the activity compared to the baseline level of engagement.
Identifying Potential Reinforcers
20. Stimulus preference assessment refers to a variety of procedures used to determine (a) the stimuli that a person prefers, (b) the relative preference values (high versus low) of those stimuli, and (c) the conditions under which those preferences values remain in effect.

Stimulus preference assessments can be performed by asking the target person and/or significant others what the target person prefers, conducting direct preference observations, and conducting trial-based assessments (i.e., single-, paired-, or multiple-stimulus presentations).
Preferred stimuli do not always function as reinforcers, and stimulus preferences often change over time.
Reinforcer assessment refers to a variety of direct, data-based methods for determining the relative effects of a given stimulus as reinforcement under different and changing conditions on the comparative effectiveness of multiple stimuli as reinforcers for a given behavior under specific conditions. Reinforcer assessment is often conducted with systematic schedules of reinforcement, multiple schedules of reinforcement, and progressive reinforcement schedules.
Control Procedures for Positive Reinforcement
24. Positive reinforcement control procedures are used to manipulate the presentation of a potential reinforcer and observe any effects on the future frequency of behavior. Positive reinforcement control procedures require a believable demonstration that the contingent presentation following the occurrence of a target response functions as positive reinforcement. Control is demonstrated by comparing rates of responding in the absence and presence of contingency, and then showing that with the absence and presence of the contingency the behavior can be turned on (up) and/or (up) and down.

In addition to a reversal design using the withdrawal of reinforcement contingency (i.e., extinction) as the control condition, noncontingent reinforcement (NCR), differential reinforcement of other behavior (DRO), and differential reinforcement of alternative behavior (DRA) can be used as control conditions for reinforcement.
Using Reinforcement Effectively
26. Guidelines for increasing the effectiveness of positive reinforcement interventions include:

Set an easily achieved initial criterion for reinforcement
Use high quality reinforcers of sufficient magnitude
Use varied reinforcers
Use a direct rather than indirect reinforcement contingency whenever possible
Combine response prompts and reinforcement
Reinforce each occurrence of the behavior initially, then gradually thin reinforcement schedule
Use contingent praise and attention
Gradually increase the response-to-reinforcement delay
Gradually shift from contrived to naturally occurring reinforcers 


Chapter 12 Negative Reinforcement

Key Terms:
avoidance contingency, escape contingency, negative reinforcement, conditioned negative reinforcer, free-operant avoidance, unconditioned negative reinforcer, discriminated avoidance

Positive versus Negative Reinforcement: The use of positive reinforcement in educational and therapeutic programs is so commonplace that the terms positive reinforcement and reinforcement have become almost synonymous; in fact, the usual lay term for reinforcement is simply reward. Positive reinforcement involves an increase in responding in the presence of stimuli. When used incorrectly in this way, responding can lead to the termination of a stimulus, as in turning off an alarm clock as the morning, which results in the cessation of noise. When responding increases as a result of stimulus termination, learning has occurred through negative reinforcement. This chapter expands the discussion of operant contingencies to include negative reinforcement. We define negative reinforcement, distinguish between escape and avoidance contingencies, describe the effective requirements of negative reinforcement, illustrate why negative reinforcement may be used to strengthen behavior, and discuss critical issues that arise when using negative reinforcement. Readers interested in more in-depth discussions of basic and applied research on negative reinforcement are referred to reviews by Hineline (1977) and Iwata (1987).

Definition of Negative Reinforcement: A negative reinforcement contingency is one in which the occurrence of a response produces the removal, termination, reduction, or postponement of a stimulus, which leads to an increase in the future occurrence of that response. A full description of negative reinforcement requires specification of its four-term contingency (see Chapter 4): (a) motivating operations (see Chapter 3), (b) behavior maintained by negative reinforcement is an antecedent event in whose presence escape/termination of the event is reinforcing, (b) the discriminative stimulus (SD) is another antecedent event in whose presence a response is more likely to be reinforced, (c) the response is the act that produces reinforcement, and (d) the reinforcer is the termination of the event that served as the EO.

Positive versus Negative Reinforcement: Positive and negative reinforcement have a similar effect on behavior in that both produce an increase in responding. They differ, however, with respect to the type of stimulus change that follows behavior, as illustrated in Figure 2. In both examples, a stimulus change (consequence) strengthens the behavior that preceded it. Taking the ability to make a sandwich is strengthened by obtaining food, carrying out protection is strengthened by blocking the rain. However, behavior maintained by positive reinforcement produces a stimulus that was absent prior to responding (food available), whereas behavior producing negative reinforcement terminates a stimulus that was present prior to responding. Food was unavailable prior to asking for it but available after (positive reinforcement); rain was landing on one's clothing before raising the newspaper but not after (negative reinforcement). Thus, the key distinction between positive and negative reinforcement is based on the type of stimulus change that occurs following a response. Many stimulus changes that interest us can be defined as either positive or negative operations. For example, one can readily see the effect of turning on a television (positive reinforcement) or turning off the light in a bedroom (negative reinforcement). Other stimulus changes exist on a continuum from less to more, such as turning up the volume of a stereo to hear it better (positive reinforcement) or turning it down when it is too loud (negative reinforcement). Sometimes, however, it is difficult to determine whether an increase in responding resulted from positive or negative reinforcement because the stimulus change is ambiguous. For example, although a change in temperature can be measured quantitatively so we know that it either increased or decreased following behavior, it is unclear whether turning on a heater when the temperature is 40° F is an example of positive reinforcement because the response "produced heat" or negative reinforcement because the response "removed cold." Another example can be found in a classic study by Long (1959), who examined the running behavior of rats in a classroom. During baseline, students were moved in or out of the classroom periodically during long intervals of time.

work periods. During treatment, the students were given 5 minutes out time if they remained in their desks during 10-minute work periods, and in-seat behavior increased. At first glance, the time-out contingency appears to involve negative reinforcement (termination of the in-session-out contingent on appropriate behavior). As Osborne (1969) pointed out, however, in analyzing the action, etc.) to which the students had access during free time may have functioned as positive reinforcement. Given the ambiguous nature of some stimulus changes, Michael (1975) suggested that the distinction between positive and negative reinforcement, based on whether a stimulus is presented or removed, may be unnecessary. Instead, he emphasized the importance of specifying the type of experimental change produced by a response. For example, one study of academic tasks specified both the "prechange" and "postchange" condition. This strategy, he proposed, would eliminate the necessity of describing the transition between prechange and postchange conditions as one involving the presentation or removal of stimuli and would facilitate a more complete understanding of functional relations between environment and behavior. Little has changed since the publication of Michael's (1975) argument. The distinction between positive and negative reinforcement continues to be emphasized in every text on learning principles, and citations to the term negative reinforcement have even increased in applied research (Iwata, 2006). In an article based on the discussion, Baron and Galizio (2005) reiterated Michael's points and added some additional points of confusion. This terminological issue is a complex one that can be considered from several perspectives—conceptual, procedural, and historical—and is unresolved at the current time. Readers interested in the topic are referred to a series of commentaries published in The Behavior Analyst (Baron, 2006; Lattal & Lattal, 2006; Marr, 2006; Michael, 2006; Sidman, 2006) and three rejoinders (Baron & Galizio, 2006).

Negative Reinforcement versus Punishment: Negative reinforcement is sometimes confused with punishment for two reasons: First, because the term for positive reinforcement is reward, people mistakenly consider negative reinforcement to be aversive. Second, the opposition that exists between reinforcement and punishment, however, often refers to "good" and "bad" but to two types of stimulus changes (presentation versus termination) that follows behavior (Catania, 1998). A second source of confusion stems from the fact that the stimuli involved in both negative reinforcement and punishment are considered "aversive" to most people. Although it is true that the same stimulus may serve as a negative reinforcer in one context and as a punisher in a different context, both the nature of the behavior change and the type of stimulus increase in responding; in a punishment contingency, a stimulus that was absent is presented following a response, which leads to a decrease in responding. Thus, a response that terminates loud noise would increase as a function of negative reinforcement, but one that produces loud noise would decrease as a function of punishment.

Escape and Avoidance Contingencies: In its simplest form, negative reinforcement involves an escape contingency, in which a response terminates (produces escape from) an ongoing stimulus. An early study by Keller (1941) illustrates typical laboratory research on escape learning. A rat was placed in an experimental chamber, and a bright light was turned on that quickly changed to a press lever, which turned off the light. Others' (1969) study on free-time contingencies may also serve as an example of escape learning in an applied context. To the extent that the important feature of the contingency was the termination of work requirements, in-seat behavior during 10-minute work periods produced 5 minutes of escape. Although escape contingencies are commonly encountered in everyday life (e.g., we turn off loud noises, shield our eyes from the sun, flee from an aggressor), most behavior maintained by negative reinforcement is characterized by an avoidance contingency, in which a response prevents or postpones the presentation of a stimulus. Returning to the previous laboratory example, an experimenter can add to the escape contingency an arrangement in which another stimulus such as a tone signals the onset of the light. The rat then learns to respond in the presence of the tone (eliminates the presentation of the light) or postpones its onset. This type of arrangement has been called discriminated avoidance, in which responding in the presence of a signal prevents the onset of a stimulus from which escape is a reinforcer. Because responses in the presence of the tone are reinforced, whereas those in the absence of the tone have no effect, the tone is a discriminative stimulus. Thus, training with a signal produced increased likelihood of reinforcement for responding. Avoidance behavior also can be acquired in the absence of a signal. Suppose the experimenter arranges a schedule in which the bright light turns on at 30-5 second every 30 seconds, and a response (or some number of responses) at any time during the interval resets the clock to zero. This type of arrangement is known as free-operant avoidance because the avoidance behavior is "free" to occur at any time and will delay the presentation of the bright light. The first three types of contingencies described earlier was illustrated in an ingenious study by Arin, Rubin, O'Brien, Ayllon, and Roll (1968) on postural slouching (see Figure 2). Participants wore an apparatus that closed an electrical circuit when slouching occurred. Closing of the switch produced an audible click, which was followed 2 seconds later by a 55-db tone. Potential reinforcement was available when participants sat upright (escape) but prevented the tone if correction occurred during the 2-second following the click (discriminated avoidance). Furthermore, maintenance of correct posture prevented the click (free-operant avoidance). A hypothetical example involving homework management also illustrated these contingencies. A parent who sends a child to his or her room immediately following school and allows the child to leave the room only after completing homework has arranged an escape contingency, whereas homework completion produces escape from the bedroom. A parent who signals.

start your homework in 10 minutes, you'll have to do it in your bedroom") has arranged a discriminated avoidance contingency: Starting homework following the warning avoids having to do it in the bedroom. Finally, the parent who waits until late in the evening to impose the inroom requirement has arranged a free-operant avoidance contingency. However, completing at any time after school avoids having to do it in the bedroom later.

Characteristics of Negative Reinforcement:
Responses Acquired and Maintained by Negative Reinforcement: If a well-known fact that aversive stimulation produces a variety of responses (Hincheline, 1977). Some of these may be considered behavior (acting to reduce the shock), others are involuntary (such as shaking). But the focus is this chapter is on operant behaviors. Recall that the presentation of an aversive stimulus serves as an EO for escape and occasions behavior that has produced escape from similar stimulation in the past. Any response that successfully terminates the stimulation will be strengthened, as a result, a wide range of behaviors may be acquired and maintained by negative reinforcement. All of these behaviors are adaptive in the sense that they weaken the stimulation that comprises the environment. Some behaviors, however, are more effectively maintained than others. As will be seen later in the chapter, negative reinforcement may play an important role in the development of academic skills, but it also can account for the development of disruptive or dangerous behavior.

Events That Serve as Negative Reinforcers: In discussing the types of stimuli that can strengthen behavior through negative reinforcement, a problem arises when attempting to use the same terminology as that applied to the description of positive reinforcers. It is quite common to refer to positive reinforcers by listing things such as food, money, praise, and so on. In fact, however, the presentation of the stimulus that strengthens behavior (food presentation, and not food per se, is a positive reinforcer). This observation may help identify what stimulus and assume that "presentation" is understood. It is a similar way, to say that negative reinforcers include shock, noise, parental nagging, and so on, is an incomplete description. It is important to remember that a stimulus described as a negative reinforcer refers to its removal because, as noted previously, the same stimulus serves as an EO when presented prior to behavior and as punishment when presented following behavior.

Learning History: As is the case with positive reinforcers, negative reinforcers influence behavior because (a) we have the inherited capacity to respond to them or (b) their effects have been established through a history of learning. Stimuli whose removal strengthens behavior at an objective level may be viewed as untrained and negative reinforcers. These stimuli are typically noxious events such as shock, high noise, intense light, extended high or low temperature, or strong pressure against the body. In fact, any source of pain or discomfort (e.g., a headache) will occasion behavior, and any response that successfully eliminates the discomfort will be strengthened. Other stimuli are conditioned negative reinforcers, which are previously neutral events that acquire their effects through pairing with untrained negative reinforcers or with a positive reinforcer. A bicycle, for example, usually began its toxic when a person's heavily overcast sky became dark clouds has been highly correlated with bad weather. Various forms of social coercion, such as parental nagging, are perhaps the most commonly encountered conditioned negative reinforcers. For example, reminding a child to clean his bedroom may have little effect on the child's behavior unless that child responds to (cleans) the room until it is clean. In the event that the nagging is reliably "backed up" by sending the child to his room, the child will eventually respond simply to stop or prevent the nagging. It is interesting to note that in the case of negative reinforcement, neutral events (dark sky, nagging) function as both (a) discriminative stimuli because responding in their presence constitutes avoidance of aversive consequence, and (b) conditioned negative reinforcers because, due to their pairing with another consequence, they become stimuli to avoid or escape.

The Nature of Negative Reinforcement: Another way to classify negative reinforcement is based on how they are removed (i.e., their source). A distinction is made between socially mediated reinforcement, in which the consequence results from the action of another person, and automatic reinforcement, in which the consequence is produced directly by a response independent of the action of another. This distinction also applies to negative reinforcement. Returning to the example in Figure 1, we can see that termination of the construction noise was an instance of social negative reinforcement (the occupants in room closed the window). The person "being bothered" by the noise, however, could simply have walked across the room and closed the window (automatic reinforcement). The classroom noise example illustrates the fact that many reinforcers can be removed or terminated either way. One can consult a physician when experiencing a headache (social) or take a pain medication (automatic), ask the teacher for help with a difficult problem (social) or persist until it is solved (automatic), and so on.

Consideration of the source of negative reinforcement may facilitate the design of behavior change intervention, as either route may be more effective. For example, when faced with a perplexing work task, an employee may finish it incorrectly (just to get it out of the way) (automatic reinforcement) or ask for help (social reinforcement). Aside from requesting the employee, the quickest solution would be to reinforce the employee's seeking help by offering assistance. Ultimately, however, the supervisor would want to teach the employee the necessary skills to complete the work tasks independently.

Identifying the Context of Negative Reinforcement: There are several ways to identify reinforcers; the difference with negative reinforcers is that equal emphasis must be placed on the antecedent event (EO) as well as on the consequence. Because escape occurs only when behavior occurs, the negative reinforcer may be gone and cannot be observed. The identification of EOs may be difficult with people who have limited verbal ability and cannot tell someone they are experiencing aversive stimulation. These people may engage in other behaviors, such as tantrums, attempts to leave the situation, destructive behavior, aggression, or even self-injury. Weeks and Gaylord-Ross (1981), for example, observed students with severe disabilities who engaged in problem behavior during difficult tasks were presented. Little or no problem behavior occurred during the pre-task condition, problem behavior occurred somewhat more often in the difficult-task condition than in the easy-task condition. These results suggested that the students' problem behavior was maintained by escape from task demands and that difficult tasks were more "aversive" than were easy tasks. However, because the consequences that followed problem behavior was termination of or providing help behavior were maintained by negative reinforcement (consequence, such as attention, which would be considered positive).

Iwata, Dorsey, Slifer, Bauman, and Richman (1994) developed a method for identifying the types of contingencies that maintain problem behavior by observing people in adult a series of conditions that differed with respect to both antecedent and consequent events. The condition involved the presentation of task demands (EO) and the removal of demands contingent on problem behavior (contingent higher rates of problem behavior under this condition relative to others indicated that problem behavior was maintained by negative reinforcement.

Smith, Iwata, Goh, and Shore (1995) extended the findings of Weeks and Gaylord-Ross (1981) and Iwata et al. (1994) by identifying some characteristics of task demands that make them aversive. After first determining that their participants' (people with severe disabilities) problem behavior was maintained by escape from task demands, Smith and colleagues examined several dimensions along which tasks might differ: task novelty, duration of the work session, and rate of demand presentation. Results of one of these analyses are shown in Figure 4, which depicts frequency distributions and cumulative records of problem behavior from the beginning to the end of sessions. These data illustrate the importance of individualized assessments in identifying most influential variables. Three participants showed positive trends (Everett and Landon) showed increasing rates of problem behavior as work sessions progressed, whereas two other participants (Miki and Sam) showed the opposite trend.

Factors That Influence the Effectiveness of Negative Reinforcement: The extent that determines whether a negative reinforcement contingency will be effective in changing behavior are similar to those that influence positive reinforcement and are related to (a) the strength of the contingency and (b) the presence of competing contingencies. In general, negative reinforcement for a given response will be more effective under the following conditions:

The stimulus change immediately follows the occurrence of the target response.
The magnitude of reinforcement is large, referring to the difference in stimulation present before and after the response occurs.
Occurrence of the target response consistently produces escape from or postponement of the EO.
Reinforcement is unavailable for competing (nontarget) responses. 

Applications of Negative Reinforcement: Negative reinforcement is a fundamental principle of learning that has been studied extensively in basic research (Hineline, 1977). Although many examples of escape and avoidance learning can be found in everyday life, research in applied behavior analysis has heavily emphasized the use of positive reinforcement over negative reinforcement, mostly for ethical reasons, which are noted in the final section of this chapter. Still, negative reinforcement has been used as one means of establishing a variety of behaviors. This section illustrates several therapeutic uses of negative reinforcement, as well as the unintended role it may play in strengthening problem behavior.

Acquisition and Maintenance of Appropriate Behavior:
Chronic Food Refusal: Pediatric feeding problems are common and are especially prevalent among children with developmental disabilities. These disorders may take a variety of forms, including selective eating, failure to consume solid foods, and complete food refusal, and may be serious enough to require tube feeding or other artificial means to ensure.

adequate nutritional intake. A large proportion of feeding problems may be attributed to a medical cause but, instead, appear to be learned responses most likely maintained by escape or avoidance.

Results from a number of studies have shown that operant teaching-based interventions can be highly effective in treating many children feeding disorders, and a study by Ahearn, Kerwin, Eicher, Shantz, and Swearingin (1996) illustrated the use of negative reinforcement as a form of intervention. Three children admitted to a hospital who had histories of chronic food refusal were first observed under a baseline (positive reinforcement) condition in which a food was presented and access to toys was provided contingent on acceptance. Food refusal, however, produced escape in that it terminated a trial. Subsequently, the experimenters compared the effects of two procedures. One treatment condition (nonremoval of the spoon) involved presenting food and keeping the spoon positioned at the child's lower lip until the bite was accepted. The other treatment (physical guidance) involved presenting food and, if the child did not accept, opening the child's mouth so that the food could be delivered. Results showed that both procedures were effective contingencies because food acceptance terminated the trial by producing removal of the spoon or avoidance of the physical guidance.

Figure 5 shows the results obtained for the three children. All children exhibited low rates of acceptance during baseline in spite of teaching-available positive reinforcement. The two interventions were implemented in a multiple baseline across subjects design. As can be seen in the second phase of the study, both interventions produced substantial increases in food acceptance across sessions. These results showed that positive reinforcement for appropriate behavior may have limited effects if other behaviors (refusal) produce negative reinforcement, and that negative reinforcement that maintains problem behavior can be used to establish alternative behavior.

Error Correction Strategies: Positive reinforcement is a basic instructional component of effective instruction. Teachers commonly deliver praise, points, or other forms of positive reinforcement contingent on correct performance. Another common procedure, but one that has received less attention than positive reinforcement, involves the correction of student errors by repeating a learning trial, having the student practice correct performance, or giving the student additional work. To the extent that correct performance avoids these remedial procedures, improvements may be just as much a function of negative reinforcement as positive reinforcement.

McClelland and collectings (2005) examined the relative contribution of these contingencies during behavioral acquisition. The learning task involved reading words presented on flashcards, and both positive reinforcement was the correct repetition of misread words. As noted by the authors, the procedure provided additional practice of correct responses but also represented an avoidance contingency. To separate these effects (in Study 3), the authors implemented two error correction conditions. In the "relevant" condition, which combined the effects of practice and negative reinforcement, students were prompted to pronounce the misread word correctly five times contingent on an error. In the "irrelevant" condition, students were prompted to repeat an unrelated, nontarget word five times contingent on an error. The irrelevant condition contained only the negative reinforcement contingency because repetition of irrelevant words provided no practice in correctly reading misread words. Figure 6 shows the results of Study 3, expressed as the cumulative number of words mastered by the 9 participants. All participants' performance improved during both error-correction conditions relative to baseline, when no error-correction procedure was in effect. Performance by 3 participants (Tess, Ariel, and Ernie) was better during relevant error correction. However, Mia's performance was clearly superior during irrelevant error correction, and performance of the remaining 5 participants (Hayley, Becky, Kara, Maisy, and Seth) was similar in both conditions. Thus, all participants showed improvement in reading performance even when they practiced irrelevant words, and most participants (6 of 9) did just as well or better practicing irrelevant rather than relevant words. These results suggest that success of many remedial (error-correction) procedures may be due at least in part to negative reinforcement.

Acquisition and Maintenance of Problem Behavior
Well-designed instructional procedures maintain a high degree of on-task behavior and lead to improved learning. Occasionally, however, the presentation of task demands may function as an EO for escape behavior due to the difficult or repetitive nature of the work requirements. Initial forms of escape may include lack of attention or mild forms of disruption. To the extent that positive reinforcement for compliance is less than optimal, attempts to escape may persist and may even escalate to more severe forms of problem behavior. In fact, research on the assessment and treatment of problem behaviors has shown that escape from task demands is a common source of negative reinforcement for property destruction, aggression, and even self-injury. O'Reilly (1995) conducted an assessment of a person's episodic aggressive behavior. The participant was an adult with severe mental retardation who attended a vocational day program. To determine whether aggressive behavior was maintained by positive versus negative reinforcement, O'Reilly observed the participant under two conditions, which were alternated in a multielement design. In one condition (attention), a therapist ignored the participant (EO) except to deliver reprimands following aggression (positive reinforcement). In the second condition (demand), a therapist presented difficult tasks to the participant (EO) and briefly terminated the trial following aggression (negative reinforcement). As Figure 7 shows, aggressive behavior was maintained by negative reinforcement. Because anecdotal reports suggested that the participant also was more likely to be aggressive following nights when he had not slept well, the data for both conditions were further divided based on whether the participant slept for more or less than 5 hours the previous night. The highest rates of aggression occurred following sleep deprivation. These data are particularly interesting in that they illustrate the influence of two antecedent events on behavior maintained by negative reinforcement: Work tasks functioned as EOs for escape but even more so in the absence of sleep.

Behavioral Replacement Strategies
Problem behaviors maintained by negative reinforcement can be treated in a number of ways. One strategy is to strengthen a more socially appropriate replacement behavior using negative reinforcement, as illustrated in a study by Durand and Carr (1987). After determining that the "stereotypic" behaviors of four special education students were maintained by escape from task demands, the authors taught the students an alternative response ("Help me"), which was followed by assistance with the task at hand. As can be seen in Figure 8, all students engaged in moderate-to-high levels of stereotypy during baseline. After being taught to use the phrase "Help me," the students began to exhibit that behavior, and their stereotypy decreased. Results of the Durand and Carr (1987) study showed that an undesirable behavior could be replaced with a desirable one; however, the replacement behavior might be considered less than ideal because it did not necessarily facilitate better task performance. This was shown In a subsequent study by Marcus and Vollmer (1995), after collecting baseline data on a young girl’s compliant and disruptive behavior, the authors compared the effects of two treatments in a reversal design. In one condition, which was called DNR (differential negative reinforcement) communication, the girl was given a brief break from the task when she said “Finished.” In the second condition, called DNR compliance, the girl was given a break after complying with an instruction (the criterion for a break was later increased to compliance with three instructions). The results of this comparison (see Figure 9) showed that both treatments produced marked reductions in disruptive behavior. However, only the DNR compliance condition produced an increase in task performance.

Ethical Considerations in the Use of Negative Reinforcement
Ethical concerns about the use of positive and negative reinforcement are similar and arise from the severity of the antecedent event (EO) that occasions behavior. Most EOs for behavior maintained by positive reinforcement can be characterized as deprivation states, which, if severe, can constitute undue restriction of rights. By contrast, most EOs for behavior maintained by negative reinforcement can be viewed as aversive events. Extremely noxious events, when presented as antecedent stimuli, cannot be justified as part of a typical behavior change program. Another concern with negative reinforcement is that the presence of aversive stimuli can itself generate behaviors that compete with the acquisition of desired behavior (Hutchinson, 1977; Myer, 1971). For example, a socially withdrawn child, when placed in the midst of others, may simply scream and run away instead of playing with the peers, and running away is incompatible with social interaction. Finally, undesirable side effects typically associated with punishment might also be observed when implementing behavior change programs based on negative reinforcement.

Summary
Definition of Negative Reinforcement

Negative reinforcement involves the termination, reduction, or postponement of a stimulus contingent on the occurrence of a response, which leads to an increase in the future occurrence of that response.
A negative reinforcement contingency involves (a) an establishing operation (EO) in whose presence escape is reinforcing, (b) a discriminative stimulus (S^D^) in whose presence a response is more likely to be reinforced, (c) the response that produces reinforcement, and (d) termination of the event that served as the EO.
Positive and negative reinforcement are similar in that both lead to an increase in responding; they differ in that positive reinforcement involves contingent stimulus presentation, whereas negative reinforcement involves contingent stimulus termination.
Negative reinforcement and punishment differ in that (a) negative reinforcement involves contingent stimulus termination, whereas punishment involves contingent stimulation, and (b) negative reinforcement leads to an increase in responding, whereas punishment leads to a decrease in responding.
Escape and Avoidance Contingencies
5. An escape contingency is one in which responding terminates an ongoing stimulus. An avoidance contingency is one in which responding delays or prevents the presentation of a stimulus.
6. In discriminated avoidance, responding in the presence of a signal prevents stimulus presentation; in free-operant avoidance, responding at any time prevents stimulus presentation.

Characteristics of Negative Reinforcement
7. Any response that successfully terminates aversive stimulation will be strengthened; as a result, a wide range of behaviors may be acquired and maintained by negative reinforcement.
8. Negative reinforcement may play an important role in the development of academic skills, but it also can account for the development of disruptive or dangerous behavior.
9. Unconditioned negative reinforcers are stimuli whose removal strengthens behavior in the absence of prior learning. Conditioned negative reinforcers are stimuli whose removal strengthens behavior as a result of previous pairing with other negative reinforcers.
10. Social negative reinforcement involves stimulus termination through the action of another person. Automatic negative reinforcement involves stimulus termination as a direct result of a response.
11. Identification of negative reinforcers requires the specification of the stimulus conditions in effect prior to and following responding.
12. In general, negative reinforcement for a given response will be more effective when (a) the stimulus change immediately follows the occurrence of the target response, (b) the magnitude of reinforcement is large, (c) the target response consistently produces escape from or postponement of the EO, and (d) reinforcement is unavailable for competing responses.

Applications of Negative Reinforcement
13. Although negative reinforcement is a fundamental principle of learning that has been studied extensively in basic research, applied behavior analysis has heavily focused on emphasized the use of positive reinforcement over neg-cially appropriate replacement behavior through nega- ative reinforcement

Applied researchers have explored the therapeutic uses of negative reinforcement in treating pediatric feeding problems. Improvements in student performance as a result of error correction that involve repeating a learning trial, having the student practice correct performance, or giving the student additional work may be a function of negative reinforcement. The presentation of task demands during instruction may function as an EO for escape; initial forms of escape may include lack of attention or mild forms of disruption. To the extent that positive reinforcement for compliance is less than optimal, escape behaviors may persist and may even escalate. One strategy for treating problem behaviors maintained by negative reinforcement is to strengthen a more socially appropriate replacement behavior through negative reinforcement.

Ethical Considerations in the Use of Negative Reinforcement
Ethical concerns about the use of positive and negative reinforcement are similar and arise from the severity of the antecedent event (EO) that occasions behavior. Most EOs for behavior maintained by negative reinforcement can be viewed as aversive events. Extremely noxious events, when presented as antecedent stimuli, cannot be justified as part of a typical behavior change program. Another concern with negative reinforcement is that the presence of aversive stimuli can itself generate behaviors that compete with the acquisition of desired behavior.



























